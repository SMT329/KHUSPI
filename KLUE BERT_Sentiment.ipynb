{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1yjBp0jQdynp8_ddQu1JxmJyk1P4Ay6cM","authorship_tag":"ABX9TyMzCf+ULJaYjJv+fzfYWknQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"17a1c797da6a4f84bd18b3fcfec2ebb1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_60227edd8eb346ff98e4e1fa2d6f4b2f","IPY_MODEL_9f7d8ac5061a477bbe0097e4464402c9","IPY_MODEL_cd3bed3327934cec8d6bbc71087dad5c"],"layout":"IPY_MODEL_467885d3fa82433d894ab0d8d66701df"}},"60227edd8eb346ff98e4e1fa2d6f4b2f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a32d945bb77d4a798be443ad5ccbf9ef","placeholder":"​","style":"IPY_MODEL_60cb718fd1bd46589c9c4a7d2bab442e","value":"config.json: 100%"}},"9f7d8ac5061a477bbe0097e4464402c9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd9536164ece490f83b0e3dead791922","max":546,"min":0,"orientation":"horizontal","style":"IPY_MODEL_569dbc13eca64ee29d9a33db1cf226a0","value":546}},"cd3bed3327934cec8d6bbc71087dad5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45f1855670e246ed97ae4662cbf6b216","placeholder":"​","style":"IPY_MODEL_34319d539a414a3bb6a24f86cf4bf340","value":" 546/546 [00:00&lt;00:00, 24.6kB/s]"}},"467885d3fa82433d894ab0d8d66701df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a32d945bb77d4a798be443ad5ccbf9ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60cb718fd1bd46589c9c4a7d2bab442e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd9536164ece490f83b0e3dead791922":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"569dbc13eca64ee29d9a33db1cf226a0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"45f1855670e246ed97ae4662cbf6b216":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34319d539a414a3bb6a24f86cf4bf340":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1cbe5920ac54157890c77efaddea681":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_22d4628cdfa547beb4d3aea1e5f041d6","IPY_MODEL_eb730a50f95c434e868ff1561a355f78","IPY_MODEL_081ae2fbc26f4716901e18ee295a30c5"],"layout":"IPY_MODEL_24fcb911720540ae9bfc60206566c124"}},"22d4628cdfa547beb4d3aea1e5f041d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3903b11df6a84182a3aee35ba8b2acc1","placeholder":"​","style":"IPY_MODEL_6cdeb826e4d84cf9a45e14dd423a8b82","value":"model.safetensors: 100%"}},"eb730a50f95c434e868ff1561a355f78":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ea05d1dbf9048e28f0dfbf34420a8b3","max":442635012,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc466820ae5d4b2cab3d062d7dd4ed51","value":442635012}},"081ae2fbc26f4716901e18ee295a30c5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5d963f301564d9eb439ea09a6a98b41","placeholder":"​","style":"IPY_MODEL_88973c9f5f54416ba95adf2461a49668","value":" 443M/443M [00:04&lt;00:00, 95.8MB/s]"}},"24fcb911720540ae9bfc60206566c124":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3903b11df6a84182a3aee35ba8b2acc1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6cdeb826e4d84cf9a45e14dd423a8b82":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ea05d1dbf9048e28f0dfbf34420a8b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc466820ae5d4b2cab3d062d7dd4ed51":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d5d963f301564d9eb439ea09a6a98b41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88973c9f5f54416ba95adf2461a49668":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da9e42075b8d4509af3863f27d0318aa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_29f0a89e5b9d4ac69e0e4da97c2de45d","IPY_MODEL_63cded615c174082baf81ca089813fea","IPY_MODEL_398459a18b8f45d4b75b8bfe5a27a13e"],"layout":"IPY_MODEL_04f144aa7d394b7db1bd7accff140e06"}},"29f0a89e5b9d4ac69e0e4da97c2de45d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28d3365ff45b4f12ac401c9db355b3d1","placeholder":"​","style":"IPY_MODEL_a18bde13957d474581721b45d463cdf0","value":"100%"}},"63cded615c174082baf81ca089813fea":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d26719a9b3dc4080be973619c3dba6f8","max":121,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d769afa297334c6e9510c47d6f66df6c","value":121}},"398459a18b8f45d4b75b8bfe5a27a13e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_90d9f3c910ed4303babc73c22387710b","placeholder":"​","style":"IPY_MODEL_cc66ad64a16e4ce9bcc49b85d8687fcc","value":" 121/121 [01:15&lt;00:00,  1.76it/s]"}},"04f144aa7d394b7db1bd7accff140e06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28d3365ff45b4f12ac401c9db355b3d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a18bde13957d474581721b45d463cdf0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d26719a9b3dc4080be973619c3dba6f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d769afa297334c6e9510c47d6f66df6c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"90d9f3c910ed4303babc73c22387710b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc66ad64a16e4ce9bcc49b85d8687fcc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2f8d435f795402d933dee8fc455cf81":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_afca902d07804727ad5d33082b878edd","IPY_MODEL_ba6fc22320d94c54b9c10a208189b226","IPY_MODEL_a8892f23c6f841a894702c363dfa9d87"],"layout":"IPY_MODEL_652600245f5340c9a9f7217745a852b7"}},"afca902d07804727ad5d33082b878edd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b460e2a69a4643ca941d9d0ef9fd8313","placeholder":"​","style":"IPY_MODEL_187bd67c91514fe59b32ffea3642115c","value":"100%"}},"ba6fc22320d94c54b9c10a208189b226":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2a6b76ccc764fe1a27712f66601f3cf","max":121,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4d53bf4691644931bd60f4769c2563a0","value":121}},"a8892f23c6f841a894702c363dfa9d87":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f2c3c5a86374181bf19a6263999057c","placeholder":"​","style":"IPY_MODEL_287b1260fdd740379533ed7b35181a82","value":" 121/121 [01:15&lt;00:00,  1.72it/s]"}},"652600245f5340c9a9f7217745a852b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b460e2a69a4643ca941d9d0ef9fd8313":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"187bd67c91514fe59b32ffea3642115c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c2a6b76ccc764fe1a27712f66601f3cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d53bf4691644931bd60f4769c2563a0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1f2c3c5a86374181bf19a6263999057c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"287b1260fdd740379533ed7b35181a82":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"04d0a7a6e11c496692b3192c535d3cf0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7dadd35bd4f244238d981714174b5766","IPY_MODEL_85d0f2f5e0d34d099d14b5c0d5a8ff97","IPY_MODEL_ead346ca6ca44c2f945d238cab19781f"],"layout":"IPY_MODEL_b5ee066205094ef3a7b17768b684bb7c"}},"7dadd35bd4f244238d981714174b5766":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6e65235ccb84ac79644c1bec4f6f84b","placeholder":"​","style":"IPY_MODEL_385a114083f24ebf8f7e8eff4594653f","value":"100%"}},"85d0f2f5e0d34d099d14b5c0d5a8ff97":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbd983f40fb740cbb1f3e1b10ca11a9a","max":121,"min":0,"orientation":"horizontal","style":"IPY_MODEL_24480d5247bc4b0b8e44bbe75c3f94dc","value":121}},"ead346ca6ca44c2f945d238cab19781f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1fd44506cd141df9fa48f12a4f43f95","placeholder":"​","style":"IPY_MODEL_2d8cf50f340e448f9297e31e836455ae","value":" 121/121 [01:17&lt;00:00,  1.70it/s]"}},"b5ee066205094ef3a7b17768b684bb7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6e65235ccb84ac79644c1bec4f6f84b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"385a114083f24ebf8f7e8eff4594653f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dbd983f40fb740cbb1f3e1b10ca11a9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24480d5247bc4b0b8e44bbe75c3f94dc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f1fd44506cd141df9fa48f12a4f43f95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d8cf50f340e448f9297e31e836455ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2684c68039d34e12b03638e0476c746c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_34a82862024a417baa6c15437596a5af","IPY_MODEL_b3332a0798f74fa1aae27521821c7302","IPY_MODEL_7f99595d81cc4ca9a3c3c2017c234ee0"],"layout":"IPY_MODEL_472efee92ba243d3b7b348c7ebc07714"}},"34a82862024a417baa6c15437596a5af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5bf605e479d49ea8dda981efe5f0d96","placeholder":"​","style":"IPY_MODEL_afd787d059384df6a0d0900f142cdc66","value":"100%"}},"b3332a0798f74fa1aae27521821c7302":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e99bb8af22bd490b98c6fa71466f5013","max":31,"min":0,"orientation":"horizontal","style":"IPY_MODEL_841e12077abb4de5b3a68dfcb5592957","value":31}},"7f99595d81cc4ca9a3c3c2017c234ee0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3b859d922904f25a6ed1011156b19ae","placeholder":"​","style":"IPY_MODEL_f10c3848693045fd8107b27bc398d373","value":" 31/31 [00:06&lt;00:00,  4.99it/s]"}},"472efee92ba243d3b7b348c7ebc07714":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5bf605e479d49ea8dda981efe5f0d96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afd787d059384df6a0d0900f142cdc66":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e99bb8af22bd490b98c6fa71466f5013":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"841e12077abb4de5b3a68dfcb5592957":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d3b859d922904f25a6ed1011156b19ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f10c3848693045fd8107b27bc398d373":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce8d71432f9a41d9be2e171f77038262":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_edb6aba81f1f450abbbc9e28cd6e89e8","IPY_MODEL_72171b2434904f79af7ffd59598caf6a","IPY_MODEL_8d8aa29f98c247dba927301b00fd8a13"],"layout":"IPY_MODEL_79279cc9d2ec4df6a2f3bcb0c464ce99"}},"edb6aba81f1f450abbbc9e28cd6e89e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_adf183da464c4beaa7444b4c8759f091","placeholder":"​","style":"IPY_MODEL_e6e9f7ff096642089395ffb6ada4ea8f","value":"100%"}},"72171b2434904f79af7ffd59598caf6a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_478992846a964dfa84284a5cd95d7ea5","max":31,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c44b189d90b84e0da8bc5fa1f6129371","value":31}},"8d8aa29f98c247dba927301b00fd8a13":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a6bb807d9294c46a99124dae29ba124","placeholder":"​","style":"IPY_MODEL_d02d490cdf474dc58028d7f30842696f","value":" 31/31 [00:05&lt;00:00,  5.19it/s]"}},"79279cc9d2ec4df6a2f3bcb0c464ce99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"adf183da464c4beaa7444b4c8759f091":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6e9f7ff096642089395ffb6ada4ea8f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"478992846a964dfa84284a5cd95d7ea5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c44b189d90b84e0da8bc5fa1f6129371":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0a6bb807d9294c46a99124dae29ba124":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d02d490cdf474dc58028d7f30842696f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"734eb82841134bdfa9244fa0eedca60b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4486f75f441e4872970d8f3d869f9f27","IPY_MODEL_6f2a88dd024d4a99bf69511351d5d50d","IPY_MODEL_6e11cd45d8694d538949a65fccfcc976"],"layout":"IPY_MODEL_b98351841ca04b19b2b0c5a5fed9dda0"}},"4486f75f441e4872970d8f3d869f9f27":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_217f61b562e24d679eccac07593b0bc9","placeholder":"​","style":"IPY_MODEL_8afb42ae78254049a79a9fd6f6c5a0df","value":"100%"}},"6f2a88dd024d4a99bf69511351d5d50d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c61f314dcbc44bc853d2e664684d2b2","max":93,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a7f14f794d40458a8dead66504aa1e8e","value":93}},"6e11cd45d8694d538949a65fccfcc976":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac9529257f8240628906a3df948e964c","placeholder":"​","style":"IPY_MODEL_3c75f82b465f4cc5b9ee5a7cfcfa714e","value":" 93/93 [00:05&lt;00:00, 18.66it/s]"}},"b98351841ca04b19b2b0c5a5fed9dda0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"217f61b562e24d679eccac07593b0bc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8afb42ae78254049a79a9fd6f6c5a0df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c61f314dcbc44bc853d2e664684d2b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7f14f794d40458a8dead66504aa1e8e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ac9529257f8240628906a3df948e964c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c75f82b465f4cc5b9ee5a7cfcfa714e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sJcRtPz2zebk","executionInfo":{"status":"ok","timestamp":1731900312116,"user_tz":-540,"elapsed":6975,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"3c3c61fb-c70b-43c1-ce74-8c824f3cc3ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","source":["!pip install adabelief-pytorch==0.2.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H7QdM7g1zrUQ","executionInfo":{"status":"ok","timestamp":1731900315039,"user_tz":-540,"elapsed":2927,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"82643ff4-223b-4498-99f4-0b8ac330448b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting adabelief-pytorch==0.2.0\n","  Downloading adabelief_pytorch-0.2.0-py3-none-any.whl.metadata (616 bytes)\n","Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch==0.2.0) (2.5.1+cu121)\n","Collecting colorama>=0.4.0 (from adabelief-pytorch==0.2.0)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.10/dist-packages (from adabelief-pytorch==0.2.0) (0.9.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch==0.2.0) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch==0.2.0) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch==0.2.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch==0.2.0) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch==0.2.0) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.0->adabelief-pytorch==0.2.0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=0.4.0->adabelief-pytorch==0.2.0) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.0->adabelief-pytorch==0.2.0) (3.0.2)\n","Downloading adabelief_pytorch-0.2.0-py3-none-any.whl (5.7 kB)\n","Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Installing collected packages: colorama, adabelief-pytorch\n","Successfully installed adabelief-pytorch-0.2.0 colorama-0.4.6\n"]}]},{"cell_type":"code","source":["!pip install -U nbformat # 기설치시, update"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p2NX-jFqztOR","executionInfo":{"status":"ok","timestamp":1731900317668,"user_tz":-540,"elapsed":2632,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"c73b686f-86e8-4460-a411-5f3fbf9a693a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (5.10.4)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat) (2.20.0)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat) (4.23.0)\n","Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from nbformat) (5.7.2)\n","Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbformat) (5.7.1)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (24.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat) (0.21.0)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.6)\n"]}]},{"cell_type":"code","source":["import os\n","import re\n","import numpy as np\n","from tqdm import tqdm\n","from glob import glob\n","import json\n","import requests\n","import tensorflow as tf\n","from transformers import BertModel, TFBertModel, TFRobertaModel, RobertaTokenizer\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoModelForSequenceClassification\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split,KFold,StratifiedKFold\n","from adabelief_pytorch import AdaBelief\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","from tqdm import tqdm, tqdm_notebook\n","import shutil\n","import gc\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import urllib.request\n","\n","from sklearn.metrics import precision_score, recall_score, f1_score, classification_report"],"metadata":{"id":"JOpRDpYCzu8a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cjhd8qn8zyqv","executionInfo":{"status":"ok","timestamp":1731900341458,"user_tz":-540,"elapsed":13064,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"91e28a29-c2e6-4ed7-c995-5e3244e3b823"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# 초기에 소스 파일이 위치하는 Colab 노트북 기본 파일 디렉토리로 이동\n","%cd /content/drive/MyDrive/KHUDA 금융/심화트랙"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YfCjtYEKz2ln","executionInfo":{"status":"ok","timestamp":1731900341458,"user_tz":-540,"elapsed":3,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"cbd37c59-8ae7-4e6c-be78-818c83421fba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/KHUDA 금융/심화트랙\n"]}]},{"cell_type":"code","source":["#random seed 고정\n","tf.random.set_seed(1234)\n","np.random.seed(1234)\n","BATCH_SIZE = 32\n","NUM_EPOCHS = 3\n","\n","L_RATE = 1e-5\n","MAX_LEN = 128\n","max_grad_norm=1\n","log_interval=200\n","NUM_CORES = os.cpu_count()\n","device = torch.device(\"cuda:0\")\n","\n","DATA_IN_PATH = './데이터'\n","DATA_OUT_PATH = \"./데이터\""],"metadata":{"id":"gTF1vZGSz07C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATASET_URL = \"https://raw.githubusercontent.com/ukairia777/finance_sentiment_corpus/main/finance_data.csv\"\n","DATASET_NAME = \"finance_data.csv\""],"metadata":{"id":"YtusUhWT0Nv5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["urllib.request.urlretrieve(DATASET_URL,\n","                           filename = DATASET_NAME\n","                           )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sMfdYggc0PiC","executionInfo":{"status":"ok","timestamp":1731900355524,"user_tz":-540,"elapsed":1357,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"7d6bf45a-5f9e-4471-ea3a-c17a386a4b6a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('finance_data.csv', <http.client.HTTPMessage at 0x794be59a7670>)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["dataset = pd.read_csv(DATASET_NAME)\n","dataset.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":289},"id":"5z_5NqzW0Q2Q","executionInfo":{"status":"ok","timestamp":1731900358017,"user_tz":-540,"elapsed":424,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"1fd65a57-83f9-46a1-97c2-9e4b1be2dc01"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     labels                                           sentence  \\\n","0   neutral  According to Gran, the company has no plans to...   \n","1   neutral  Technopolis plans to develop in stages an area...   \n","2  negative  The international electronic industry company ...   \n","3  positive  With the new production plant the company woul...   \n","4  positive  According to the company's updated strategy fo...   \n","\n","                                        kor_sentence  \n","0  Gran에 따르면, 그 회사는 회사가 성장하고 있는 곳이지만, 모든 생산을 러시아로...  \n","1  테크노폴리스는 컴퓨터 기술과 통신 분야에서 일하는 회사들을 유치하기 위해 10만 평...  \n","2  국제 전자산업 회사인 엘코텍은 탈린 공장에서 수십 명의 직원을 해고했으며, 이전의 ...  \n","3  새로운 생산공장으로 인해 회사는 예상되는 수요 증가를 충족시킬 수 있는 능력을 증가...  \n","4  2009-2012년 회사의 업데이트된 전략에 따르면, Basware는 20% - 4...  "],"text/html":["\n","  <div id=\"df-3f0d7d08-952e-439d-9dc9-5bf2b502a1e3\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>labels</th>\n","      <th>sentence</th>\n","      <th>kor_sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>neutral</td>\n","      <td>According to Gran, the company has no plans to...</td>\n","      <td>Gran에 따르면, 그 회사는 회사가 성장하고 있는 곳이지만, 모든 생산을 러시아로...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>neutral</td>\n","      <td>Technopolis plans to develop in stages an area...</td>\n","      <td>테크노폴리스는 컴퓨터 기술과 통신 분야에서 일하는 회사들을 유치하기 위해 10만 평...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>negative</td>\n","      <td>The international electronic industry company ...</td>\n","      <td>국제 전자산업 회사인 엘코텍은 탈린 공장에서 수십 명의 직원을 해고했으며, 이전의 ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>positive</td>\n","      <td>With the new production plant the company woul...</td>\n","      <td>새로운 생산공장으로 인해 회사는 예상되는 수요 증가를 충족시킬 수 있는 능력을 증가...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>positive</td>\n","      <td>According to the company's updated strategy fo...</td>\n","      <td>2009-2012년 회사의 업데이트된 전략에 따르면, Basware는 20% - 4...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f0d7d08-952e-439d-9dc9-5bf2b502a1e3')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-3f0d7d08-952e-439d-9dc9-5bf2b502a1e3 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-3f0d7d08-952e-439d-9dc9-5bf2b502a1e3');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-31f0fe06-3760-4a5f-b957-662280dfe182\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-31f0fe06-3760-4a5f-b957-662280dfe182')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-31f0fe06-3760-4a5f-b957-662280dfe182 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"dataset","summary":"{\n  \"name\": \"dataset\",\n  \"rows\": 4846,\n  \"fields\": [\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"neutral\",\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4838,\n        \"samples\": [\n          \"The Company serves approximately 3,000 customers in over 100 countries.\",\n          \"On Dec. 1, Grimaldi acquired 1.5 million shares and a 50.1-percent stake in Finnlines.\",\n          \"The extracted filtrates are very high in clarity while the dried filter cakes meet required transport moisture limits (TMLs)for their ore grades.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"kor_sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4827,\n        \"samples\": [\n          \"YIT\\uc5d0 \\ub300\\ud55c \\uace0\\uac1d\\ub4e4\\uc758 \\uc2e0\\ub8b0 \\uc99d\\uac00\\ub294 \\uc544\\ud30c\\ud2b8 \\ub9e4\\ub9e4\\uac00 \\uac00\\uc18d\\ud654\\ub41c \\uac83\\uc73c\\ub85c \\ubcfc \\uc218 \\uc788\\uc2b5\\ub2c8\\ub2e4.\",\n          \"\\uc0dd\\uc0b0\\uc740 2010\\ub144 \\ub3d9\\uc548 \\uba55\\uc2dc\\ucf54\\uc640 \\ud5dd\\uac00\\ub9ac\\ub97c \\ud3ec\\ud568\\ud55c \\uc5d8\\ucf54\\ud14d\\uc758 \\ub2e4\\ub978 \\uc9c0\\uc5ed\\uc73c\\ub85c \\ud655\\ub300\\ub420 \\uac83\\uc774\\ub2e4.\",\n          \"\\ubc14\\uc774\\uc0b4\\ub77c\\ub294 \\ub610\\ud55c 2009\\ub144\\uc758 2\\uc5b55220\\ub9cc \\uc720\\ub85c\\uc5d0 \\ube44\\ud574 2010\\ub144\\uc5d0\\ub294 2\\uc5b55320\\ub9cc \\uc720\\ub85c\\uac00 \\uc21c\\ub9e4\\ucd9c\\ub420 \\uac83\\uc73c\\ub85c \\uc608\\uc0c1\\ud55c\\ub2e4\\uace0 \\ubc1d\\ud614\\ub2e4.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# 영어 원문 문장 제거\n","del dataset['sentence']\n","# 데이터셋 라벨데이터를 숫자로 치환, 라벨인코딩 사용해도 됨.\n","dataset['labels'] = dataset['labels'].replace(['neutral', 'positive', 'negative'],[0, 1, 2])\n","\n","# 중복 데이터 제거\n","DATASET_PREP_FILE = './데이터/dataset_prep.csv'\n","dataset.drop_duplicates(subset = ['kor_sentence'], inplace = True)\n","dataset.to_csv(DATASET_PREP_FILE) # 구글 드라이브 내 data 폴더에 저장"],"metadata":{"id":"g3mnjVDu0Sa-","executionInfo":{"status":"ok","timestamp":1731900361696,"user_tz":-540,"elapsed":803,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"890e9d00-0a13-4314-a9cc-45982b96aa5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-12-5fe54cf057e2>:4: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  dataset['labels'] = dataset['labels'].replace(['neutral', 'positive', 'negative'],[0, 1, 2])\n"]}]},{"cell_type":"code","source":["#두가지 임베딩\n","#\"klue/roberta-large\"\n","#\"monologg/kobigbird-bert-base\"\n","#tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobigbird-bert-base\", cache_dir='bert_ckpt', do_lower_case=False)\n","tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\", cache_dir='bert_ckpt', do_lower_case=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kl3gaNZn0YnB","executionInfo":{"status":"ok","timestamp":1731900372086,"user_tz":-540,"elapsed":6345,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"0f53cdb0-9993-4f82-864f-0fd0817b8a8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["dataset = pd.read_csv('./데이터/dataset_prep.csv')\n","dataset.drop('Unnamed: 0',axis=1,inplace=True)"],"metadata":{"id":"g1ONhtX20dqm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 입출력 데이터 분리\n","X_data = dataset['kor_sentence']\n","y_data = dataset['labels']"],"metadata":{"id":"MX_Bl-hq0fNG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["TEST_SIZE = 0.2 # Train: Test = 8 :2 분리\n","RANDOM_STATE = 42\n","# strtify = True 일 경우, 데이터 분리 이전의 라벨별 분포 고려\n","X_train, X_test, y_train, y_test = train_test_split(X_data, y_data,\n","                                                    test_size = TEST_SIZE,\n","                                                    random_state = RANDOM_STATE,\n","                                                    stratify = y_data)"],"metadata":{"id":"0Hth-_yF0j2N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"훈련 입력 데이터 개수: {len(X_train)}\")\n","print(f\"테스트 입력 데이터 개수: {len(X_test)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k9K3ZJZN0icH","executionInfo":{"status":"ok","timestamp":1731900422384,"user_tz":-540,"elapsed":454,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"6b0443ee-66a3-4379-a9ed-f61ab7049b42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련 입력 데이터 개수: 3861\n","테스트 입력 데이터 개수: 966\n"]}]},{"cell_type":"code","source":["# 훈련 데이터 라벨별 비율\n","y_train.value_counts(normalize = True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":207},"id":"U3C-ry8o0loq","executionInfo":{"status":"ok","timestamp":1731900425562,"user_tz":-540,"elapsed":578,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"c66b0dfe-91b7-4491-f1c7-e2fab298e5e4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["labels\n","0    0.592852\n","1    0.282051\n","2    0.125097\n","Name: proportion, dtype: float64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>proportion</th>\n","    </tr>\n","    <tr>\n","      <th>labels</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.592852</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.282051</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.125097</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> float64</label>"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# 테스트 데이터 라벨별 비율\n","y_test.value_counts(normalize = True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":207},"id":"bU4ohfdS0p71","executionInfo":{"status":"ok","timestamp":1731900425934,"user_tz":-540,"elapsed":2,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"f7bee0f3-416f-4a1c-dc3c-be5f2c0a15d8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["labels\n","0    0.592133\n","1    0.282609\n","2    0.125259\n","Name: proportion, dtype: float64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>proportion</th>\n","    </tr>\n","    <tr>\n","      <th>labels</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.592133</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.282609</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.125259</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> float64</label>"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["# 데이터셋으로 변환해주기 위해 형식을 맞추도록 하겠습니다.\n","X=pd.concat([X_train,y_train],axis=1)\n","X.reset_index(drop=True,inplace=True)\n","X_test=X_test.reset_index()"],"metadata":{"id":"a3oYT6zM0raN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TrainDataset(Dataset):\n","    def __init__(self, df):\n","        self.df_data = df\n","    def __getitem__(self, index):\n","        # get the sentence from the dataframe\n","        sentence = self.df_data.loc[index, 'kor_sentence']\n","        encoded_dict = tokenizer(\n","          text = sentence,\n","          add_special_tokens = True,\n","          max_length = MAX_LEN,\n","          pad_to_max_length = True,\n","          truncation=True,           # Pad & truncate all sentences.\n","          return_tensors=\"pt\")\n","\n","        padded_token_list = encoded_dict['input_ids'][0]\n","        token_type_id = encoded_dict['token_type_ids'][0]\n","        att_mask = encoded_dict['attention_mask'][0]\n","        target = torch.tensor(self.df_data.loc[index, \"labels\"])\n","        sample = (padded_token_list, token_type_id , att_mask, target)\n","        return sample\n","    def __len__(self):\n","        return len(self.df_data)"],"metadata":{"id":"h86S_MRi0ujq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TestDataset(Dataset):\n","    def __init__(self, df):\n","        self.df_data = df\n","    def __getitem__(self, index):\n","        # get the sentence from the dataframe\n","        sentence = self.df_data.loc[index, 'kor_sentence']\n","        encoded_dict = tokenizer(\n","          text = sentence,\n","          add_special_tokens = True,\n","          max_length = MAX_LEN,\n","          pad_to_max_length = True,\n","          truncation=True,           # Pad & truncate all sentences.\n","          return_tensors=\"pt\")\n","\n","        padded_token_list = encoded_dict['input_ids'][0]\n","        token_type_id = encoded_dict['token_type_ids'][0]\n","        att_mask = encoded_dict['attention_mask'][0]\n","        sample = (padded_token_list, token_type_id , att_mask)\n","        return sample\n","    def __len__(self):\n","        return len(self.df_data)"],"metadata":{"id":"F0qIWJBP0w2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc"],"metadata":{"id":"oXJe0RRZ0yhH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#AUto\n","train_data = TrainDataset(X)\n","\n","test_data = TestDataset(X_test)\n","\n","train_dataloader = torch.utils.data.DataLoader(train_data,\n","                                        batch_size=BATCH_SIZE,\n","                                        shuffle=True,\n","                                      num_workers=0)\n","test_dataloader = torch.utils.data.DataLoader(test_data,\n","                                        batch_size=BATCH_SIZE,\n","                                        shuffle=False,\n","                                      num_workers=0)\n","#두가지 모델\n","#\"klue/roberta-base\"\n","#kobigbird-bert-base\n","model = AutoModelForSequenceClassification.from_pretrained(\"klue/roberta-base\",num_labels=3)\n","\n","\n","\n","####미세조정\n","n=0\n","for name, child in model.named_children():\n","    if n==0:\n","      h=0\n","      for param in child.parameters():\n","        if h<=0: #이부분 숫자 조절로 fine-tuning => Roberta229: h=229\n","          param.requires_grad = False\n","        h+=1\n","    n+=1\n","#####\n","    # print(param)\n","model.to(device)\n","optimizer = AdaBelief(model.parameters(), lr=1e-5, eps=1e-16, betas=(0.9,0.999), weight_decouple = True, rectify = False)\n","\n","warmup_ratio = 0.1\n","t_total = len(train_dataloader) * NUM_EPOCHS\n","warmup_step = int(t_total * warmup_ratio)\n","scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n","\n","for e in range(NUM_EPOCHS):\n","    train_acc = 0.0\n","    test_acc = 0.0\n","    best_acc =0.0\n","    model.train()\n","    torch.set_grad_enabled(True)\n","    for batch_id, (input_id,token_type_id,attention_mask,label) in enumerate(tqdm_notebook(train_dataloader)):\n","        optimizer.zero_grad()\n","        input_id = input_id.long().to(device)\n","        token_type_id = token_type_id.long().to(device)\n","        attention_mask = attention_mask.long().to(device)\n","        label = label.to(device)\n","        outputs = model(input_ids=input_id, token_type_ids=token_type_id, attention_mask=attention_mask, labels=label)\n","        loss = outputs[0]\n","        out = outputs[1]\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","        optimizer.step()\n","        scheduler.step()  # Update learning rate schedule\n","        _, preds = torch.max(out, dim=1)\n","        train_acc += f1_score(preds.cpu(), label.cpu(),average='weighted')\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","\n","preds = []\n","model.eval()\n","torch.set_grad_enabled(False)\n","for batch_id, (input_id,token_type_id,attention_mask) in enumerate(tqdm_notebook(test_dataloader)):\n","    input_id = input_id.long().to(device)\n","    token_type_id = token_type_id.long().to(device)\n","    attention_mask = attention_mask.long().to(device)\n","    outputs = model(input_ids=input_id, token_type_ids=token_type_id, attention_mask=attention_mask)\n","    out = outputs[0]\n","    for inp in out:\n","      preds.append(inp.detach().cpu().numpy())\n","Preds = np.array(preds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["17a1c797da6a4f84bd18b3fcfec2ebb1","60227edd8eb346ff98e4e1fa2d6f4b2f","9f7d8ac5061a477bbe0097e4464402c9","cd3bed3327934cec8d6bbc71087dad5c","467885d3fa82433d894ab0d8d66701df","a32d945bb77d4a798be443ad5ccbf9ef","60cb718fd1bd46589c9c4a7d2bab442e","dd9536164ece490f83b0e3dead791922","569dbc13eca64ee29d9a33db1cf226a0","45f1855670e246ed97ae4662cbf6b216","34319d539a414a3bb6a24f86cf4bf340","b1cbe5920ac54157890c77efaddea681","22d4628cdfa547beb4d3aea1e5f041d6","eb730a50f95c434e868ff1561a355f78","081ae2fbc26f4716901e18ee295a30c5","24fcb911720540ae9bfc60206566c124","3903b11df6a84182a3aee35ba8b2acc1","6cdeb826e4d84cf9a45e14dd423a8b82","6ea05d1dbf9048e28f0dfbf34420a8b3","cc466820ae5d4b2cab3d062d7dd4ed51","d5d963f301564d9eb439ea09a6a98b41","88973c9f5f54416ba95adf2461a49668","da9e42075b8d4509af3863f27d0318aa","29f0a89e5b9d4ac69e0e4da97c2de45d","63cded615c174082baf81ca089813fea","398459a18b8f45d4b75b8bfe5a27a13e","04f144aa7d394b7db1bd7accff140e06","28d3365ff45b4f12ac401c9db355b3d1","a18bde13957d474581721b45d463cdf0","d26719a9b3dc4080be973619c3dba6f8","d769afa297334c6e9510c47d6f66df6c","90d9f3c910ed4303babc73c22387710b","cc66ad64a16e4ce9bcc49b85d8687fcc","e2f8d435f795402d933dee8fc455cf81","afca902d07804727ad5d33082b878edd","ba6fc22320d94c54b9c10a208189b226","a8892f23c6f841a894702c363dfa9d87","652600245f5340c9a9f7217745a852b7","b460e2a69a4643ca941d9d0ef9fd8313","187bd67c91514fe59b32ffea3642115c","c2a6b76ccc764fe1a27712f66601f3cf","4d53bf4691644931bd60f4769c2563a0","1f2c3c5a86374181bf19a6263999057c","287b1260fdd740379533ed7b35181a82","04d0a7a6e11c496692b3192c535d3cf0","7dadd35bd4f244238d981714174b5766","85d0f2f5e0d34d099d14b5c0d5a8ff97","ead346ca6ca44c2f945d238cab19781f","b5ee066205094ef3a7b17768b684bb7c","e6e65235ccb84ac79644c1bec4f6f84b","385a114083f24ebf8f7e8eff4594653f","dbd983f40fb740cbb1f3e1b10ca11a9a","24480d5247bc4b0b8e44bbe75c3f94dc","f1fd44506cd141df9fa48f12a4f43f95","2d8cf50f340e448f9297e31e836455ae","2684c68039d34e12b03638e0476c746c","34a82862024a417baa6c15437596a5af","b3332a0798f74fa1aae27521821c7302","7f99595d81cc4ca9a3c3c2017c234ee0","472efee92ba243d3b7b348c7ebc07714","c5bf605e479d49ea8dda981efe5f0d96","afd787d059384df6a0d0900f142cdc66","e99bb8af22bd490b98c6fa71466f5013","841e12077abb4de5b3a68dfcb5592957","d3b859d922904f25a6ed1011156b19ae","f10c3848693045fd8107b27bc398d373"]},"id":"Xygnj9A60z6T","executionInfo":{"status":"ok","timestamp":1731900684917,"user_tz":-540,"elapsed":241275,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"cf8dae61-a17f-49e6-b7ec-7062c6e31947"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/546 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17a1c797da6a4f84bd18b3fcfec2ebb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1cbe5920ac54157890c77efaddea681"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n","\u001b[31mModifications to default arguments:\n","\u001b[31m                           eps  weight_decouple    rectify\n","-----------------------  -----  -----------------  ---------\n","adabelief-pytorch=0.0.5  1e-08  False              False\n",">=0.1.0 (Current 0.2.0)  1e-16  True               True\n","\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n","----------------------------------------------------------  ----------------------------------------------\n","Recommended eps = 1e-8                                      Recommended eps = 1e-16\n","\u001b[34mFor a complete table of recommended hyperparameters, see\n","\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n","\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n","\u001b[0m\n","Weight decoupling enabled in AdaBelief\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-24-70fd2a362e84>:47: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for batch_id, (input_id,token_type_id,attention_mask,label) in enumerate(tqdm_notebook(train_dataloader)):\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/121 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da9e42075b8d4509af3863f27d0318aa"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["epoch 1 batch id 1 loss 1.048166036605835 train acc 0.6464811783960721\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["epoch 1 train acc 0.7735183861468781\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-24-70fd2a362e84>:47: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for batch_id, (input_id,token_type_id,attention_mask,label) in enumerate(tqdm_notebook(train_dataloader)):\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/121 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2f8d435f795402d933dee8fc455cf81"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["epoch 2 batch id 1 loss 0.2178483009338379 train acc 0.9375\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["epoch 2 train acc 0.8635036038221797\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-24-70fd2a362e84>:47: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for batch_id, (input_id,token_type_id,attention_mask,label) in enumerate(tqdm_notebook(train_dataloader)):\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/121 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04d0a7a6e11c496692b3192c535d3cf0"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["epoch 3 batch id 1 loss 0.2545599639415741 train acc 0.9042151162790697\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["epoch 3 train acc 0.8971422573765364\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-24-70fd2a362e84>:69: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for batch_id, (input_id,token_type_id,attention_mask) in enumerate(tqdm_notebook(test_dataloader)):\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/31 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2684c68039d34e12b03638e0476c746c"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["preds = []\n","model.eval()\n","torch.set_grad_enabled(False)\n","for batch_id, (input_id,token_type_id,attention_mask) in enumerate(tqdm_notebook(test_dataloader)):\n","    input_id = input_id.long().to(device)\n","    token_type_id = token_type_id.long().to(device)\n","    attention_mask = attention_mask.long().to(device)\n","    outputs = model(input_ids=input_id, token_type_ids=token_type_id, attention_mask=attention_mask)\n","    out = outputs[0]\n","    for inp in out:\n","      preds.append(inp.detach().cpu().numpy())\n","Preds = np.array(preds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":120,"referenced_widgets":["ce8d71432f9a41d9be2e171f77038262","edb6aba81f1f450abbbc9e28cd6e89e8","72171b2434904f79af7ffd59598caf6a","8d8aa29f98c247dba927301b00fd8a13","79279cc9d2ec4df6a2f3bcb0c464ce99","adf183da464c4beaa7444b4c8759f091","e6e9f7ff096642089395ffb6ada4ea8f","478992846a964dfa84284a5cd95d7ea5","c44b189d90b84e0da8bc5fa1f6129371","0a6bb807d9294c46a99124dae29ba124","d02d490cdf474dc58028d7f30842696f"]},"id":"8nyZKmzR021B","executionInfo":{"status":"ok","timestamp":1731900760808,"user_tz":-540,"elapsed":6461,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"5d297b7c-5b77-485b-eeed-2c1e2a874db8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-25-2835772bb3d5>:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for batch_id, (input_id,token_type_id,attention_mask) in enumerate(tqdm_notebook(test_dataloader)):\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/31 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce8d71432f9a41d9be2e171f77038262"}},"metadata":{}}]},{"cell_type":"code","source":["# 모델이 예측한 라벨 도출\n","predicted_label = np.argmax(Preds, axis=1)\n"],"metadata":{"id":"5OcolFL12QN4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predicted_label"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PjywLdSQ2SXb","executionInfo":{"status":"ok","timestamp":1731900767425,"user_tz":-540,"elapsed":579,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"ab75b61c-83e9-4741-ca30-6210c392a5ee"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1,\n","       1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1,\n","       2, 0, 1, 2, 0, 2, 2, 0, 1, 1, 1, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0,\n","       1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 2, 1, 0, 0,\n","       0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n","       1, 1, 2, 0, 0, 1, 0, 0, 2, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n","       0, 1, 0, 2, 2, 2, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0,\n","       2, 1, 0, 0, 2, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 2, 1, 1, 1, 0, 1, 2,\n","       0, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 2,\n","       0, 2, 1, 0, 0, 0, 1, 0, 1, 2, 0, 0, 2, 1, 2, 1, 0, 2, 2, 0, 0, 1,\n","       0, 0, 2, 0, 1, 0, 1, 0, 1, 0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 1, 2, 0,\n","       0, 0, 0, 1, 1, 0, 2, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n","       0, 0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n","       0, 2, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, 0, 1, 1, 0, 0, 2, 0, 0, 1,\n","       0, 2, 1, 0, 2, 1, 0, 2, 1, 1, 1, 1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0,\n","       0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 2, 1, 0, 0, 1, 2, 2, 0, 1, 0, 0,\n","       1, 0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 1, 2, 2, 1, 1, 0, 2, 1, 2, 0,\n","       0, 1, 0, 1, 2, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2,\n","       0, 0, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n","       0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 2, 0, 1, 0, 2, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n","       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 0, 1, 1,\n","       1, 2, 1, 0, 0, 2, 1, 0, 1, 1, 2, 0, 2, 0, 2, 0, 0, 0, 2, 1, 0, 2,\n","       0, 0, 0, 1, 2, 1, 0, 1, 0, 1, 0, 0, 2, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n","       0, 1, 1, 0, 1, 2, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 2,\n","       2, 1, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0,\n","       0, 1, 0, 0, 2, 2, 1, 0, 1, 0, 2, 0, 0, 0, 0, 1, 1, 0, 2, 2, 0, 2,\n","       0, 1, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2,\n","       1, 1, 2, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 1, 0,\n","       0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 1, 0,\n","       0, 0, 1, 1, 1, 1, 2, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n","       0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n","       2, 0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n","       1, 0, 1, 0, 2, 0, 0, 1, 1, 1, 0, 1, 0, 1, 2, 1, 0, 1, 0, 0, 0, 0,\n","       1, 0, 0, 1, 0, 1, 0, 1, 0, 2, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n","       1, 0, 0, 2, 1, 0, 1, 2, 0, 0, 1, 1, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n","       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 1,\n","       0, 1, 0, 1, 0, 0, 1, 2, 1, 2, 1, 0, 0, 1, 0, 0, 0, 0, 2, 1, 0, 1,\n","       0, 2, 0, 0, 2, 0, 1, 2, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 2, 1, 0,\n","       0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n","       0, 1, 0, 0, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0, 1, 0,\n","       0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 2, 1, 1, 1, 0, 0, 1, 0, 1,\n","       1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 1])"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, \\\n","                            roc_auc_score, confusion_matrix, classification_report, \\\n","                            matthews_corrcoef, cohen_kappa_score, log_loss\n","\n","# Classification Report 저장\n","path2data='./데이터'\n","if not os.path.exists(path2data):\n","    os.mkdir(path2data)\n","CL_REPORT_FILE = \"./데이터/cl_report.csv\"\n","\n","cl_report = classification_report(y_test, predicted_label, output_dict = True)\n","cl_report_df = pd.DataFrame(cl_report).transpose()\n","cl_report_df = cl_report_df.round(3)\n","cl_report_df.to_csv(CL_REPORT_FILE)\n","print(cl_report_df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HrOHPAC_2VCB","executionInfo":{"status":"ok","timestamp":1731900776736,"user_tz":-540,"elapsed":8,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"60fe4d61-c0b2-452c-c829-21bf0e44beb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision  recall  f1-score  support\n","0                 0.895   0.879     0.887  572.000\n","1                 0.799   0.817     0.808  273.000\n","2                 0.824   0.851     0.837  121.000\n","accuracy          0.858   0.858     0.858    0.858\n","macro avg         0.839   0.849     0.844  966.000\n","weighted avg      0.859   0.858     0.859  966.000\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(),'./klue_base_fold3_s.pth')"],"metadata":{"id":"5bNtcvXz9Xwm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#여기서부터 데이터 적용. 학습은 위에서 끝"],"metadata":{"id":"GJkTf4IjXQJy"}},{"cell_type":"code","source":["\n","import torch.cuda.amp as amp\n","from transformers import XLMPreTrainedModel, XLMRobertaModel, XLMRobertaConfig, XLMRobertaTokenizer\n","from transformers import XLMRobertaForSequenceClassification, BertForSequenceClassification\n","from transformers import AutoTokenizer\n","from transformers import BertForSequenceClassification, DistilBertForSequenceClassification, XLNetForSequenceClassification,\\\n","XLMRobertaForSequenceClassification, XLMForSequenceClassification, RobertaForSequenceClassification\n","from transformers import AdamW\n","from transformers import get_linear_schedule_with_warmup"],"metadata":{"id":"QKoXYUskXM2V","executionInfo":{"status":"ok","timestamp":1731941324993,"user_tz":-540,"elapsed":13988,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["tf.random.set_seed(1234)\n","np.random.seed(1234)\n","\n","class args:\n","    # ---- factor ---- #\n","    debug=False\n","    amp = True\n","    gpu = '0'\n","\n","    epochs=5\n","    batch_size=32\n","    weight_decay=1e-6\n","    n_fold=5\n","    fold=3 # [0, 1, 2, 3, 4] # 원래는 3\n","\n","    exp_name = 'model_f'\n","    dir_ = f'./saved_models/'\n","    pt = 'mz_model'\n","    max_len = 128\n","\n","    start_lr = 1e-5#1e-3,5e-5\n","    min_lr=1e-6\n","    # ---- Dataset ---- #\n","\n","    # ---- Else ---- #\n","    num_workers=8\n","    seed=2021\n","    scheduler = None#'get_linear_schedule_with_warmup'\n"],"metadata":{"id":"lgep8lk_bQC_","colab":{"base_uri":"https://localhost:8080/","height":207},"executionInfo":{"status":"error","timestamp":1731941324993,"user_tz":-540,"elapsed":7,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"71648ece-9ac8-433a-9e45-499420183505"},"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'tf' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-185582aa2a58>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1234\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1234\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# ---- factor ---- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"]}]},{"cell_type":"code","source":["os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n","device = torch.device(f'cuda' if torch.cuda.is_available() else 'cpu')\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\", cache_dir = 'bert_ckpt', do_lower_case = False)"],"metadata":{"id":"abi0wr-RbSWA","executionInfo":{"status":"aborted","timestamp":1731941324994,"user_tz":-540,"elapsed":5,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TestDataset(Dataset):\n","    def __init__(self, df):\n","        self.df_data = df\n","    def __getitem__(self, index):\n","        # get the sentence from the dataframe\n","        sentence = self.df_data.loc[index, 'kor_sentence']\n","        encoded_dict = tokenizer(\n","          text = sentence,\n","          add_special_tokens = True,\n","          max_length = MAX_LEN,\n","          pad_to_max_length = True,\n","          truncation=True,           # Pad & truncate all sentences.\n","          return_tensors=\"pt\")\n","\n","        padded_token_list = encoded_dict['input_ids'][0]\n","        token_type_id = encoded_dict['token_type_ids'][0]\n","        att_mask = encoded_dict['attention_mask'][0]\n","        sample = (padded_token_list, token_type_id , att_mask)\n","        return sample\n","    def __len__(self):\n","        return len(self.df_data)\n","\n"],"metadata":{"id":"zcDM7oN7bToA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 불러오기 및 전처리\n","df = pd.read_csv(\"\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"id":"QPqLcRmm46a0","executionInfo":{"status":"error","timestamp":1731910156762,"user_tz":-540,"elapsed":497,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"cfea65b5-839a-405b-ee9a-07b41b8484de"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'pd' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-186c70b8a655>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/KHUDA 금융/심화트랙/데이터/현대차_daily_news_titles .csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}]},{"cell_type":"code","source":["test_data = TestDataset(df)\n","\n","test_dataloader = torch.utils.data.DataLoader(test_data,\n","                                        batch_size=args.batch_size,\n","                                        shuffle=False,\n","                                      num_workers=0)"],"metadata":{"id":"H0CdeSF546Sh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def do_predict(net, valid_loader):\n","\n","    val_loss = 0\n","    pred_lst = []\n","    logit=[]\n","    net.eval()\n","    for batch_id, (input_id,token_type_id,attention_mask) in enumerate(tqdm_notebook(valid_loader)):\n","        input_id = input_id.long().to(device)\n","        token_type_id = token_type_id.long().to(device)\n","        attention_mask = attention_mask.long().to(device)\n","\n","        with torch.no_grad():\n","            if args.amp:\n","                with amp.autocast():\n","                    # output\n","                    output = net(input_ids=input_id, token_type_ids=token_type_id, attention_mask=attention_mask)[0]\n","\n","            else:\n","                output = net(outputs = model(input_ids=input_id, token_type_ids=token_type_id, attention_mask=attention_mask))\n","\n","            pred_lst.extend(output.argmax(dim=1).tolist())\n","            logit.extend(output.tolist())\n","\n","    return pred_lst,logit\n","\n","def run_predict(model_path,test_dataloader):\n","\n","\n","    print('set testloader')\n","    ## net ----------------------------------------\n","    scaler = amp.GradScaler()\n","\n","    net = RobertaForSequenceClassification.from_pretrained('klue/roberta-base', num_labels = 3)\n","\n","\n","    net.to(device)\n","\n","    if len(args.gpu)>1:\n","        net = nn.DataParallel(net)\n","\n","    f = torch.load(model_path)\n","    net.load_state_dict(f, strict=True)  # True\n","    print('load saved models')\n","    # ------------------------\n","    # validation\n","    preds, logit = do_predict(net, test_dataloader) #outputs\n","\n","    print('complete predict')\n","\n","    return preds, np.array(logit)"],"metadata":{"id":"dKc9tH-0bat6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds1, logit1 = run_predict('./klue_base_fold3_s.pth',test_dataloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":341,"referenced_widgets":["734eb82841134bdfa9244fa0eedca60b","4486f75f441e4872970d8f3d869f9f27","6f2a88dd024d4a99bf69511351d5d50d","6e11cd45d8694d538949a65fccfcc976","b98351841ca04b19b2b0c5a5fed9dda0","217f61b562e24d679eccac07593b0bc9","8afb42ae78254049a79a9fd6f6c5a0df","3c61f314dcbc44bc853d2e664684d2b2","a7f14f794d40458a8dead66504aa1e8e","ac9529257f8240628906a3df948e964c","3c75f82b465f4cc5b9ee5a7cfcfa714e"]},"id":"Cg-nzVG3bczu","executionInfo":{"status":"ok","timestamp":1731904408437,"user_tz":-540,"elapsed":7103,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"ab45bc00-4b73-4ec5-a7e1-08566da5652f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["set testloader\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-63-08a68ad77adc>:31: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = amp.GradScaler()\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","<ipython-input-63-08a68ad77adc>:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  f = torch.load(model_path)\n"]},{"output_type":"stream","name":"stdout","text":["load saved models\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-63-08a68ad77adc>:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","  for batch_id, (input_id,token_type_id,attention_mask) in enumerate(tqdm_notebook(valid_loader)):\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/93 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"734eb82841134bdfa9244fa0eedca60b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2834: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","<ipython-input-63-08a68ad77adc>:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with amp.autocast():\n"]},{"output_type":"stream","name":"stdout","text":["complete predict\n"]}]},{"cell_type":"code","source":["print(type(logit1))  # 타입 확인\n","print(logit1.dtype)  # 데이터 타입\n","print(logit1.shape)  # 배열의 차원 (행, 열)\n","print(logit1.size)   # 총 원소 수\n","print(logit1.ndim)   # 차원 수"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yZWVADm2beDo","executionInfo":{"status":"ok","timestamp":1731904586151,"user_tz":-540,"elapsed":5,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"a24e503c-304c-49eb-8974-2bb11557502c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'numpy.ndarray'>\n","float64\n","(2967, 3)\n","8901\n","2\n"]}]},{"cell_type":"code","source":["print(logit1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GCsHmwfMkvon","executionInfo":{"status":"ok","timestamp":1731904867609,"user_tz":-540,"elapsed":4,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"7c98f223-8b32-463b-fb53-7f0923b3ef8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.57861328  2.734375   -1.51660156]\n"," [-0.32470703  2.39257812 -1.59082031]\n"," [ 2.17578125 -0.28491211 -1.96386719]\n"," ...\n"," [ 1.43945312  1.29492188 -2.41015625]\n"," [ 2.61523438 -0.74414062 -2.00195312]\n"," [-0.64257812 -1.79296875  2.21679688]]\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","# logit1: 각 클래스(긍정, 중립, 부정)에 대한 정규화되지 않은 점수 리스트\n","# 예제 logit1 (실제 데이터로 교체하세요)\n","\n","# 소프트맥스 함수 정의\n","def softmax(logits):\n","    exp_values = np.exp(logits - np.max(logits, axis=-1, keepdims=True))  # 안정성을 위해 max 값 빼기\n","    probabilities = exp_values / np.sum(exp_values, axis=-1, keepdims=True)\n","    return probabilities\n","\n","# logit1을 소프트맥스 변환\n","new_logit1 = [softmax(logit) for logit in logit1]\n","\n","# 결과 확인\n","print(\"Softmax 변환 후 확률:\")\n","for i, probs in enumerate(new_logit1):\n","    print(f\"Logit {i + 1}: {probs}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0a8YsnTrnYxi","executionInfo":{"status":"ok","timestamp":1731905575276,"user_tz":-540,"elapsed":2812,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"e00e0960-5bb2-41ee-efc1-b6cce48da4f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Softmax 변환 후 확률:\n","Logit 1: [0.03465184 0.95178493 0.01356323]\n","Logit 2: [0.06089729 0.92193422 0.01716849]\n","Logit 3: [0.90801435 0.07752239 0.01446326]\n","Logit 4: [0.01957411 0.96738672 0.01303917]\n","Logit 5: [0.90801435 0.07752239 0.01446326]\n","Logit 6: [0.02266249 0.96504617 0.01229134]\n","Logit 7: [0.02213297 0.964377   0.01349003]\n","Logit 8: [0.05552068 0.92789423 0.01658508]\n","Logit 9: [0.21469549 0.77428153 0.01102298]\n","Logit 10: [0.45602827 0.53159139 0.01238034]\n","Logit 11: [0.08083315 0.0161716  0.90299525]\n","Logit 12: [0.06554225 0.92431903 0.01013872]\n","Logit 13: [0.18046593 0.80681999 0.01271408]\n","Logit 14: [0.02259963 0.96566438 0.01173599]\n","Logit 15: [0.45476933 0.53480366 0.01042701]\n","Logit 16: [0.87501935 0.1144907  0.01048995]\n","Logit 17: [0.25189618 0.72995426 0.01814955]\n","Logit 18: [0.15802543 0.01914435 0.82283022]\n","Logit 19: [0.02507126 0.96263218 0.01229656]\n","Logit 20: [0.31064741 0.67752314 0.01182945]\n","Logit 21: [0.74418191 0.24588492 0.00993317]\n","Logit 22: [0.76987319 0.21832219 0.01180463]\n","Logit 23: [0.48374045 0.49521203 0.02104752]\n","Logit 24: [0.06289809 0.92676618 0.01033574]\n","Logit 25: [0.86241959 0.09878975 0.03879066]\n","Logit 26: [0.13839942 0.85070192 0.01089866]\n","Logit 27: [0.80015426 0.17803798 0.02180775]\n","Logit 28: [0.89227322 0.0941253  0.01360148]\n","Logit 29: [0.85152031 0.13887119 0.0096085 ]\n","Logit 30: [0.05181725 0.93203618 0.01614657]\n","Logit 31: [0.91668698 0.07372783 0.00958519]\n","Logit 32: [0.06554894 0.91721958 0.01723147]\n","Logit 33: [0.89817606 0.09128524 0.01053869]\n","Logit 34: [0.85424879 0.13384827 0.01190293]\n","Logit 35: [0.88768328 0.10190115 0.01041558]\n","Logit 36: [0.39942207 0.08921019 0.51136774]\n","Logit 37: [0.05815767 0.93131291 0.01052942]\n","Logit 38: [0.03938782 0.01560631 0.94500586]\n","Logit 39: [0.53970916 0.01914733 0.44114351]\n","Logit 40: [0.89368801 0.08842231 0.01788968]\n","Logit 41: [0.45845267 0.52408093 0.0174664 ]\n","Logit 42: [0.52364787 0.02538006 0.45097206]\n","Logit 43: [0.7491065  0.03237154 0.21852196]\n","Logit 44: [0.26295075 0.72709459 0.00995466]\n","Logit 45: [0.18907971 0.80151575 0.00940454]\n","Logit 46: [0.14230875 0.84947636 0.00821489]\n","Logit 47: [0.05074233 0.93708703 0.01217064]\n","Logit 48: [0.45361953 0.50235628 0.04402419]\n","Logit 49: [0.83249424 0.03047203 0.13703373]\n","Logit 50: [0.03786932 0.95031884 0.01181184]\n","Logit 51: [0.94021806 0.04975679 0.01002514]\n","Logit 52: [0.16451675 0.25704223 0.57844102]\n","Logit 53: [0.08584761 0.01653701 0.89761538]\n","Logit 54: [0.29305667 0.69617451 0.01076882]\n","Logit 55: [0.03016009 0.94931265 0.02052726]\n","Logit 56: [0.40855828 0.02642613 0.56501559]\n","Logit 57: [0.03969859 0.94898021 0.0113212 ]\n","Logit 58: [0.45330162 0.51895213 0.02774625]\n","Logit 59: [0.36105968 0.62568557 0.01325475]\n","Logit 60: [0.80807425 0.18296637 0.00895938]\n","Logit 61: [0.92231918 0.06889979 0.00878104]\n","Logit 62: [0.08037484 0.90979092 0.00983425]\n","Logit 63: [0.14625094 0.84243832 0.01131075]\n","Logit 64: [0.91913042 0.07153633 0.00933326]\n","Logit 65: [0.23376491 0.65793491 0.10830018]\n","Logit 66: [0.35643727 0.6078023  0.03576043]\n","Logit 67: [0.05966235 0.92488361 0.01545405]\n","Logit 68: [0.56341643 0.03490985 0.40167372]\n","Logit 69: [0.69289061 0.28159828 0.02551112]\n","Logit 70: [0.08169965 0.01779867 0.90050168]\n","Logit 71: [0.08218207 0.0201445  0.89767343]\n","Logit 72: [0.62902378 0.04789321 0.32308301]\n","Logit 73: [0.13492876 0.01598271 0.84908853]\n","Logit 74: [0.14947971 0.02201435 0.82850595]\n","Logit 75: [0.10652988 0.01597793 0.87749219]\n","Logit 76: [0.96387004 0.02216458 0.01396538]\n","Logit 77: [0.32466171 0.02666938 0.64866891]\n","Logit 78: [0.04768579 0.02126402 0.93105019]\n","Logit 79: [0.5447455  0.44201081 0.0132437 ]\n","Logit 80: [0.92843618 0.04206699 0.02949683]\n","Logit 81: [0.48807264 0.02954093 0.48238643]\n","Logit 82: [0.56258443 0.03279434 0.40462122]\n","Logit 83: [0.69238733 0.29417902 0.01343365]\n","Logit 84: [0.92428583 0.02420219 0.05151198]\n","Logit 85: [0.69142343 0.29621814 0.01235843]\n","Logit 86: [0.84196957 0.14839892 0.00963152]\n","Logit 87: [0.15014187 0.03163321 0.81822492]\n","Logit 88: [0.17603272 0.02506141 0.79890587]\n","Logit 89: [0.17603272 0.02506141 0.79890587]\n","Logit 90: [0.78708187 0.20184346 0.01107467]\n","Logit 91: [0.91630478 0.07624794 0.00744728]\n","Logit 92: [0.79986592 0.18896712 0.01116696]\n","Logit 93: [0.78565037 0.18217432 0.03217531]\n","Logit 94: [0.04983803 0.93922737 0.01093461]\n","Logit 95: [0.26963869 0.71877734 0.01158397]\n","Logit 96: [0.94179193 0.04589242 0.01231564]\n","Logit 97: [0.73992513 0.24905701 0.01101786]\n","Logit 98: [0.15019483 0.82897106 0.02083411]\n","Logit 99: [0.03179682 0.95686568 0.0113375 ]\n","Logit 100: [0.82824811 0.16146736 0.01028453]\n","Logit 101: [0.16493722 0.82423169 0.01083109]\n","Logit 102: [0.82447038 0.16611668 0.00941294]\n","Logit 103: [0.08200657 0.87521986 0.04277357]\n","Logit 104: [0.93956423 0.04892742 0.01150835]\n","Logit 105: [0.61817747 0.37129985 0.01052268]\n","Logit 106: [0.71904224 0.27052947 0.01042828]\n","Logit 107: [0.10940997 0.0255905  0.86499953]\n","Logit 108: [0.03910254 0.9524698  0.00842766]\n","Logit 109: [0.03398549 0.02152878 0.94448574]\n","Logit 110: [0.61679455 0.07660011 0.30660534]\n","Logit 111: [0.06512389 0.91004901 0.0248271 ]\n","Logit 112: [0.23349124 0.02899035 0.73751841]\n","Logit 113: [0.84542882 0.14239345 0.01217774]\n","Logit 114: [0.94855167 0.04264395 0.00880438]\n","Logit 115: [0.02661382 0.96323527 0.01015091]\n","Logit 116: [0.10871581 0.02174719 0.869537  ]\n","Logit 117: [0.06475897 0.05856201 0.87667902]\n","Logit 118: [0.90241512 0.08279791 0.01478697]\n","Logit 119: [0.84229181 0.14870964 0.00899856]\n","Logit 120: [0.84229181 0.14870964 0.00899856]\n","Logit 121: [0.88553308 0.10627952 0.0081874 ]\n","Logit 122: [0.81478684 0.16278868 0.02242448]\n","Logit 123: [0.88553308 0.10627952 0.0081874 ]\n","Logit 124: [0.85463005 0.11988793 0.02548203]\n","Logit 125: [0.75690546 0.22659939 0.01649515]\n","Logit 126: [0.20075099 0.77957955 0.01966946]\n","Logit 127: [0.6430954 0.3472629 0.0096417]\n","Logit 128: [0.91105003 0.02832925 0.06062073]\n","Logit 129: [0.17102895 0.81573432 0.01323672]\n","Logit 130: [0.79269228 0.1969317  0.01037602]\n","Logit 131: [0.90470086 0.08860418 0.00669496]\n","Logit 132: [0.6532738  0.28925223 0.05747398]\n","Logit 133: [0.6532738  0.28925223 0.05747398]\n","Logit 134: [0.09744978 0.89056987 0.01198035]\n","Logit 135: [0.39837744 0.58190439 0.01971817]\n","Logit 136: [0.27328856 0.68889495 0.03781648]\n","Logit 137: [0.8650193  0.12069533 0.01428537]\n","Logit 138: [0.05836656 0.92174126 0.01989219]\n","Logit 139: [0.74942833 0.23284323 0.01772844]\n","Logit 140: [0.06250729 0.92540319 0.01208952]\n","Logit 141: [0.03910254 0.9524698  0.00842766]\n","Logit 142: [0.05837697 0.92415916 0.01746387]\n","Logit 143: [0.07806193 0.911993   0.00994508]\n","Logit 144: [0.15901362 0.04984078 0.7911456 ]\n","Logit 145: [0.82363608 0.10973889 0.06662503]\n","Logit 146: [0.14014655 0.84601766 0.01383579]\n","Logit 147: [0.6768895  0.08563767 0.23747283]\n","Logit 148: [0.69679591 0.28877077 0.01443332]\n","Logit 149: [0.05477891 0.93423784 0.01098325]\n","Logit 150: [0.87503626 0.11344255 0.01152118]\n","Logit 151: [0.68202498 0.30561676 0.01235825]\n","Logit 152: [0.1127382  0.87130333 0.01595848]\n","Logit 153: [0.77561363 0.21287087 0.0115155 ]\n","Logit 154: [0.03910254 0.9524698  0.00842766]\n","Logit 155: [0.11517675 0.87314592 0.01167733]\n","Logit 156: [0.89140693 0.02222893 0.08636414]\n","Logit 157: [0.92872031 0.04251357 0.02876612]\n","Logit 158: [0.642293   0.34852727 0.00917973]\n","Logit 159: [0.85037782 0.13688521 0.01273697]\n","Logit 160: [0.87087046 0.10318863 0.02594091]\n","Logit 161: [0.44271896 0.54882461 0.00845643]\n","Logit 162: [0.82840387 0.16404094 0.00755519]\n","Logit 163: [0.74719012 0.24376447 0.00904541]\n","Logit 164: [0.21360035 0.77296946 0.01343019]\n","Logit 165: [0.05410513 0.93590595 0.00998891]\n","Logit 166: [0.14840824 0.83363608 0.01795568]\n","Logit 167: [0.04442424 0.93237069 0.02320508]\n","Logit 168: [0.05456862 0.9336099  0.01182148]\n","Logit 169: [0.05855156 0.01881499 0.92263345]\n","Logit 170: [0.67821487 0.31066105 0.01112409]\n","Logit 171: [0.55560292 0.03591964 0.40847744]\n","Logit 172: [0.38196251 0.60208618 0.01595131]\n","Logit 173: [0.06301256 0.91875688 0.01823057]\n","Logit 174: [0.04151924 0.9445123  0.01396845]\n","Logit 175: [0.95516811 0.03580888 0.00902301]\n","Logit 176: [0.03291628 0.95680174 0.01028198]\n","Logit 177: [0.34787671 0.63269765 0.01942565]\n","Logit 178: [0.63950005 0.03323673 0.32726323]\n","Logit 179: [0.31280446 0.66577338 0.02142216]\n","Logit 180: [0.04089928 0.94321673 0.01588399]\n","Logit 181: [0.0295814  0.95784402 0.01257458]\n","Logit 182: [0.0292204  0.9587112  0.01206839]\n","Logit 183: [0.02457181 0.96065505 0.01477314]\n","Logit 184: [0.04101789 0.94525959 0.01372251]\n","Logit 185: [0.09636103 0.88445797 0.019181  ]\n","Logit 186: [0.02285396 0.96562625 0.01151978]\n","Logit 187: [0.51828938 0.02118446 0.46052616]\n","Logit 188: [0.92568628 0.05929284 0.01502089]\n","Logit 189: [0.2368974  0.75102258 0.01208002]\n","Logit 190: [0.5455746  0.43412128 0.02030412]\n","Logit 191: [0.77097213 0.22088721 0.00814066]\n","Logit 192: [0.78369364 0.19699115 0.01931522]\n","Logit 193: [0.75549307 0.23029989 0.01420704]\n","Logit 194: [0.90426456 0.08848581 0.00724963]\n","Logit 195: [0.88719595 0.0973892  0.01541485]\n","Logit 196: [0.69672693 0.28874218 0.01453088]\n","Logit 197: [0.84876771 0.1373786  0.01385368]\n","Logit 198: [0.80709308 0.1798232  0.01308372]\n","Logit 199: [0.74073907 0.24860158 0.01065935]\n","Logit 200: [0.30314225 0.68615029 0.01070746]\n","Logit 201: [0.05292693 0.93586464 0.01120843]\n","Logit 202: [0.20162485 0.78584562 0.01252953]\n","Logit 203: [0.03907103 0.94429616 0.01663281]\n","Logit 204: [0.96814769 0.01771494 0.01413738]\n","Logit 205: [0.06546256 0.92094411 0.01359333]\n","Logit 206: [0.02020531 0.96785847 0.01193623]\n","Logit 207: [0.0195133  0.96768332 0.01280338]\n","Logit 208: [0.76831541 0.22293843 0.00874616]\n","Logit 209: [0.19413173 0.79063297 0.0152353 ]\n","Logit 210: [0.03896983 0.94553661 0.01549356]\n","Logit 211: [0.60815676 0.38318527 0.00865796]\n","Logit 212: [0.11228142 0.87265958 0.01505899]\n","Logit 213: [0.90505809 0.07429169 0.02065021]\n","Logit 214: [0.63964758 0.35152631 0.00882611]\n","Logit 215: [0.12064489 0.87080139 0.00855373]\n","Logit 216: [0.72294861 0.26440414 0.01264725]\n","Logit 217: [0.03454188 0.95527246 0.01018566]\n","Logit 218: [0.90224796 0.08302545 0.01472659]\n","Logit 219: [0.23738247 0.02892459 0.73369294]\n","Logit 220: [0.23738247 0.02892459 0.73369294]\n","Logit 221: [0.52367978 0.45720789 0.01911233]\n","Logit 222: [0.16304329 0.82356677 0.01338994]\n","Logit 223: [0.05947147 0.9253071  0.01522142]\n","Logit 224: [0.97049182 0.01937029 0.01013789]\n","Logit 225: [0.9013273  0.01939473 0.07927797]\n","Logit 226: [0.28653204 0.6793467  0.03412126]\n","Logit 227: [0.05443302 0.93413629 0.01143069]\n","Logit 228: [0.23065334 0.75776174 0.01158493]\n","Logit 229: [0.14098174 0.84299454 0.01602372]\n","Logit 230: [0.12057843 0.86968448 0.0097371 ]\n","Logit 231: [0.12065904 0.86159821 0.01774274]\n","Logit 232: [0.93607493 0.03433041 0.02959466]\n","Logit 233: [0.03828359 0.94720686 0.01450954]\n","Logit 234: [0.89288253 0.02029299 0.08682448]\n","Logit 235: [0.02144117 0.96671789 0.01184094]\n","Logit 236: [0.08629194 0.01999691 0.89371115]\n","Logit 237: [0.02588929 0.96204753 0.01206318]\n","Logit 238: [0.04306561 0.94630832 0.01062607]\n","Logit 239: [0.91812204 0.07022987 0.0116481 ]\n","Logit 240: [0.80550286 0.18453404 0.0099631 ]\n","Logit 241: [0.68843679 0.29710674 0.01445647]\n","Logit 242: [0.05410513 0.93590595 0.00998891]\n","Logit 243: [0.60793282 0.34402985 0.04803733]\n","Logit 244: [0.05873298 0.93104455 0.01022247]\n","Logit 245: [0.52114862 0.46624308 0.0126083 ]\n","Logit 246: [0.07414981 0.90233778 0.0235124 ]\n","Logit 247: [0.02185359 0.96531188 0.01283452]\n","Logit 248: [0.81856723 0.1680981  0.01333468]\n","Logit 249: [0.07450006 0.83611709 0.08938285]\n","Logit 250: [0.02860183 0.95926372 0.01213445]\n","Logit 251: [0.14529683 0.02348784 0.83121533]\n","Logit 252: [0.124395   0.02094131 0.8546637 ]\n","Logit 253: [0.13257007 0.01996933 0.84746061]\n","Logit 254: [0.14598747 0.02042489 0.83358763]\n","Logit 255: [0.15005795 0.02135985 0.8285822 ]\n","Logit 256: [0.91735679 0.07462864 0.00801458]\n","Logit 257: [0.05433578 0.86756532 0.0780989 ]\n","Logit 258: [0.57793842 0.41303397 0.00902761]\n","Logit 259: [0.96319905 0.01675174 0.02004921]\n","Logit 260: [0.69215859 0.29929694 0.00854447]\n","Logit 261: [0.43439718 0.5561453  0.00945753]\n","Logit 262: [0.90924247 0.08172092 0.00903661]\n","Logit 263: [0.70649849 0.28545119 0.00805032]\n","Logit 264: [0.62204341 0.36890861 0.00904798]\n","Logit 265: [0.80195285 0.18964526 0.00840188]\n","Logit 266: [0.25126626 0.10040078 0.64833296]\n","Logit 267: [0.08808439 0.88269776 0.02921785]\n","Logit 268: [0.06937067 0.91713187 0.01349746]\n","Logit 269: [0.27033675 0.01915753 0.71050572]\n","Logit 270: [0.27855719 0.7095862  0.01185661]\n","Logit 271: [0.15331832 0.02135909 0.82532259]\n","Logit 272: [0.93542963 0.05234986 0.01222051]\n","Logit 273: [0.95599832 0.03293685 0.01106483]\n","Logit 274: [0.93275537 0.0267199  0.04052473]\n","Logit 275: [0.84647077 0.14484943 0.00867979]\n","Logit 276: [0.13903496 0.84754385 0.01342119]\n","Logit 277: [0.82628825 0.160066   0.01364575]\n","Logit 278: [0.23870979 0.74649228 0.01479792]\n","Logit 279: [0.70660696 0.27942676 0.01396629]\n","Logit 280: [0.52932213 0.46078212 0.00989575]\n","Logit 281: [0.85568252 0.12393665 0.02038083]\n","Logit 282: [0.52381541 0.46091317 0.01527142]\n","Logit 283: [0.69847999 0.29031799 0.01120202]\n","Logit 284: [0.9646105  0.02450489 0.0108846 ]\n","Logit 285: [0.0270388  0.94897104 0.02399016]\n","Logit 286: [0.49678406 0.48860475 0.0146112 ]\n","Logit 287: [0.96954766 0.02074081 0.00971153]\n","Logit 288: [0.87426345 0.11143882 0.01429773]\n","Logit 289: [0.81374788 0.1677215  0.01853061]\n","Logit 290: [0.53562086 0.45368855 0.01069059]\n","Logit 291: [0.37228489 0.03916198 0.58855313]\n","Logit 292: [0.26884061 0.72156565 0.00959374]\n","Logit 293: [0.80405571 0.18276903 0.01317525]\n","Logit 294: [0.59515012 0.39413926 0.01071062]\n","Logit 295: [0.5440679  0.44492342 0.01100867]\n","Logit 296: [0.5807463  0.40741072 0.01184299]\n","Logit 297: [0.34405189 0.64465943 0.01128868]\n","Logit 298: [0.61310633 0.37368799 0.01320569]\n","Logit 299: [0.49809571 0.47901397 0.02289032]\n","Logit 300: [0.54279365 0.04768421 0.40952214]\n","Logit 301: [0.75628766 0.23371569 0.00999665]\n","Logit 302: [0.86251499 0.12581388 0.01167113]\n","Logit 303: [0.83890971 0.14703167 0.01405862]\n","Logit 304: [0.36491681 0.62439226 0.01069093]\n","Logit 305: [0.65071031 0.33725455 0.01203513]\n","Logit 306: [0.41021016 0.57848905 0.01130079]\n","Logit 307: [0.60449562 0.38311674 0.01238764]\n","Logit 308: [0.8787749  0.10777695 0.01344815]\n","Logit 309: [0.94629634 0.04619079 0.00751288]\n","Logit 310: [0.32115758 0.66414095 0.01470147]\n","Logit 311: [0.51907928 0.47032494 0.01059577]\n","Logit 312: [0.78419534 0.1847681  0.03103656]\n","Logit 313: [0.82982197 0.14859738 0.02158065]\n","Logit 314: [0.6497574  0.33923623 0.01100637]\n","Logit 315: [0.81141151 0.17711561 0.01147288]\n","Logit 316: [0.87707642 0.11437526 0.00854832]\n","Logit 317: [0.21829663 0.77204158 0.00966179]\n","Logit 318: [0.77273653 0.21689868 0.01036479]\n","Logit 319: [0.54603964 0.43875517 0.01520519]\n","Logit 320: [0.73149327 0.25778329 0.01072344]\n","Logit 321: [0.81004562 0.18012888 0.0098255 ]\n","Logit 322: [0.81810575 0.17173566 0.01015859]\n","Logit 323: [0.92606117 0.05998676 0.01395207]\n","Logit 324: [0.81481424 0.17335695 0.01182881]\n","Logit 325: [0.8810214  0.02840348 0.09057512]\n","Logit 326: [0.83691508 0.15178229 0.01130263]\n","Logit 327: [0.81165598 0.17816662 0.01017741]\n","Logit 328: [0.28376968 0.69347668 0.02275365]\n","Logit 329: [0.59502886 0.37933306 0.02563808]\n","Logit 330: [0.64815977 0.33790677 0.01393346]\n","Logit 331: [0.8085163  0.18009633 0.01138738]\n","Logit 332: [0.91267945 0.07772119 0.00959937]\n","Logit 333: [0.07796481 0.89794172 0.02409348]\n","Logit 334: [0.9140413  0.07613623 0.00982247]\n","Logit 335: [0.74178553 0.22789486 0.03031961]\n","Logit 336: [0.84547749 0.01527519 0.13924732]\n","Logit 337: [0.6603268  0.05126821 0.28840499]\n","Logit 338: [0.68752965 0.04610662 0.26636373]\n","Logit 339: [0.24248179 0.74799139 0.00952681]\n","Logit 340: [0.31361442 0.67668593 0.00969965]\n","Logit 341: [0.87017367 0.11916792 0.01065841]\n","Logit 342: [0.93652828 0.0539432  0.00952852]\n","Logit 343: [0.924736   0.06426431 0.0109997 ]\n","Logit 344: [0.94273932 0.047189   0.01007168]\n","Logit 345: [0.05246544 0.01437831 0.93315625]\n","Logit 346: [0.74668694 0.23994077 0.0133723 ]\n","Logit 347: [0.66612324 0.32195891 0.01191785]\n","Logit 348: [0.24008342 0.74603725 0.01387933]\n","Logit 349: [0.225734   0.76029977 0.01396623]\n","Logit 350: [0.6343325  0.03025328 0.33541422]\n","Logit 351: [0.67156924 0.0182497  0.31018106]\n","Logit 352: [0.22570938 0.76543143 0.00885919]\n","Logit 353: [0.40289112 0.58620312 0.01090576]\n","Logit 354: [0.964611   0.02679536 0.00859363]\n","Logit 355: [0.61820915 0.37131888 0.01047197]\n","Logit 356: [0.28099565 0.70849562 0.01050873]\n","Logit 357: [0.06666847 0.92291739 0.01041414]\n","Logit 358: [0.8817294  0.10960952 0.00866107]\n","Logit 359: [0.85556769 0.13659884 0.00783347]\n","Logit 360: [0.94843785 0.04377806 0.00778409]\n","Logit 361: [0.22230862 0.76464997 0.0130414 ]\n","Logit 362: [0.81607689 0.17110078 0.01282233]\n","Logit 363: [0.88085593 0.11153805 0.00760603]\n","Logit 364: [0.84098412 0.15114871 0.00786717]\n","Logit 365: [0.16219392 0.82529903 0.01250705]\n","Logit 366: [0.85682398 0.13234798 0.01082804]\n","Logit 367: [0.52894502 0.44672321 0.02433177]\n","Logit 368: [0.6348952  0.30321626 0.06188854]\n","Logit 369: [0.95050566 0.01855002 0.03094432]\n","Logit 370: [0.43680595 0.53675607 0.02643798]\n","Logit 371: [0.64775661 0.01929494 0.33294845]\n","Logit 372: [0.72517718 0.039191   0.23563182]\n","Logit 373: [0.6211422  0.36461617 0.01424163]\n","Logit 374: [0.12822097 0.01855787 0.85322116]\n","Logit 375: [0.56681438 0.38861552 0.0445701 ]\n","Logit 376: [0.74587881 0.23839719 0.015724  ]\n","Logit 377: [0.63721224 0.31321746 0.0495703 ]\n","Logit 378: [0.21364568 0.02557871 0.76077561]\n","Logit 379: [0.92417759 0.06663737 0.00918504]\n","Logit 380: [0.52894502 0.44672321 0.02433177]\n","Logit 381: [0.04389358 0.94215916 0.01394727]\n","Logit 382: [0.08738549 0.89539416 0.01722036]\n","Logit 383: [0.06254395 0.91975    0.01770605]\n","Logit 384: [0.10606838 0.88065074 0.01328088]\n","Logit 385: [0.55087125 0.43833589 0.01079285]\n","Logit 386: [0.84530706 0.1457848  0.00890814]\n","Logit 387: [0.87853871 0.10244583 0.01901546]\n","Logit 388: [0.37978495 0.5847859  0.03542915]\n","Logit 389: [0.96211258 0.02546476 0.01242266]\n","Logit 390: [0.52894502 0.44672321 0.02433177]\n","Logit 391: [0.92276751 0.05678609 0.0204464 ]\n","Logit 392: [0.03666599 0.01724384 0.94609017]\n","Logit 393: [0.71962758 0.26785682 0.0125156 ]\n","Logit 394: [0.58341909 0.40294037 0.01364054]\n","Logit 395: [0.29153992 0.70039313 0.00806694]\n","Logit 396: [0.10673766 0.8854506  0.00781174]\n","Logit 397: [0.93595879 0.04864457 0.01539664]\n","Logit 398: [0.22914354 0.75390516 0.0169513 ]\n","Logit 399: [0.78776331 0.20251202 0.00972467]\n","Logit 400: [0.47033778 0.51960063 0.01006158]\n","Logit 401: [0.29318234 0.01610193 0.69071572]\n","Logit 402: [0.07047877 0.90919766 0.02032357]\n","Logit 403: [0.91646419 0.07265578 0.01088003]\n","Logit 404: [0.69176445 0.01575281 0.29248274]\n","Logit 405: [0.16001532 0.0224347  0.81754998]\n","Logit 406: [0.03435543 0.94687441 0.01877015]\n","Logit 407: [0.03710698 0.01750244 0.94539058]\n","Logit 408: [0.04739991 0.01667961 0.93592048]\n","Logit 409: [0.77832091 0.03463394 0.18704515]\n","Logit 410: [0.50915241 0.47597464 0.01487295]\n","Logit 411: [0.3319567  0.65407839 0.01396491]\n","Logit 412: [0.20668026 0.78343843 0.00988131]\n","Logit 413: [0.07230433 0.01755531 0.91014036]\n","Logit 414: [0.28868796 0.0756589  0.63565314]\n","Logit 415: [0.55506285 0.03425817 0.41067897]\n","Logit 416: [0.12135998 0.02380403 0.85483599]\n","Logit 417: [0.91957911 0.06396837 0.01645252]\n","Logit 418: [0.37803879 0.04986713 0.57209408]\n","Logit 419: [0.22887309 0.75191311 0.0192138 ]\n","Logit 420: [0.65066761 0.32717622 0.02215617]\n","Logit 421: [0.05979206 0.92746037 0.01274757]\n","Logit 422: [0.11748242 0.86861418 0.0139034 ]\n","Logit 423: [0.15829074 0.82353281 0.01817645]\n","Logit 424: [0.5695728  0.02987831 0.40054889]\n","Logit 425: [0.78830923 0.19999821 0.01169256]\n","Logit 426: [0.84417818 0.06680263 0.08901918]\n","Logit 427: [0.10151918 0.08342589 0.81505494]\n","Logit 428: [0.21093766 0.78029191 0.00877044]\n","Logit 429: [0.57811901 0.02493382 0.39694717]\n","Logit 430: [0.21243222 0.0248079  0.76275988]\n","Logit 431: [0.32438946 0.04819265 0.62741789]\n","Logit 432: [0.93585935 0.05023241 0.01390823]\n","Logit 433: [0.70975796 0.03274506 0.25749698]\n","Logit 434: [0.48633068 0.50373234 0.00993699]\n","Logit 435: [0.09785358 0.89306016 0.00908626]\n","Logit 436: [0.17932044 0.02580204 0.79487752]\n","Logit 437: [0.39335802 0.59339068 0.01325129]\n","Logit 438: [0.63446872 0.35175828 0.013773  ]\n","Logit 439: [0.06995361 0.92135973 0.00868666]\n","Logit 440: [0.20931435 0.02467162 0.76601403]\n","Logit 441: [0.04700355 0.9397223  0.01327415]\n","Logit 442: [0.52733122 0.45636713 0.01630165]\n","Logit 443: [0.26693868 0.02822457 0.70483676]\n","Logit 444: [0.71200493 0.27652025 0.01147482]\n","Logit 445: [0.63811925 0.09019574 0.27168501]\n","Logit 446: [0.40411817 0.03564057 0.56024125]\n","Logit 447: [0.26263465 0.04406244 0.69330292]\n","Logit 448: [0.49860039 0.4851515  0.01624811]\n","Logit 449: [0.82431877 0.0767485  0.09893273]\n","Logit 450: [0.82431877 0.0767485  0.09893273]\n","Logit 451: [0.94156367 0.03986236 0.01857396]\n","Logit 452: [0.49606583 0.48932984 0.01460433]\n","Logit 453: [0.926608   0.05986121 0.01353079]\n","Logit 454: [0.94800275 0.03731868 0.01467857]\n","Logit 455: [0.87867284 0.0298617  0.09146546]\n","Logit 456: [0.86990512 0.11825458 0.0118403 ]\n","Logit 457: [0.95628247 0.031102   0.01261553]\n","Logit 458: [0.89773932 0.09272578 0.00953489]\n","Logit 459: [0.08563247 0.90198225 0.01238528]\n","Logit 460: [0.65944508 0.17169087 0.16886406]\n","Logit 461: [0.92835037 0.05924612 0.01240351]\n","Logit 462: [0.61973404 0.36485692 0.01540904]\n","Logit 463: [0.93501547 0.05622063 0.0087639 ]\n","Logit 464: [0.8613509  0.13080875 0.00784035]\n","Logit 465: [0.89306616 0.09917706 0.00775678]\n","Logit 466: [0.90858286 0.06975517 0.02166197]\n","Logit 467: [0.14657087 0.84263383 0.0107953 ]\n","Logit 468: [0.38192527 0.60261569 0.01545904]\n","Logit 469: [0.24162998 0.73920229 0.01916772]\n","Logit 470: [0.96578407 0.0246548  0.00956113]\n","Logit 471: [0.62902953 0.36122127 0.00974921]\n","Logit 472: [0.06053302 0.92767468 0.0117923 ]\n","Logit 473: [0.09512664 0.89128691 0.01358645]\n","Logit 474: [0.62250218 0.36918069 0.00831713]\n","Logit 475: [0.54082829 0.45011724 0.00905447]\n","Logit 476: [0.12753349 0.01779028 0.85467623]\n","Logit 477: [0.9673251  0.01915684 0.01351806]\n","Logit 478: [0.93368979 0.05719232 0.00911789]\n","Logit 479: [0.85808198 0.12773337 0.01418465]\n","Logit 480: [0.079885   0.91106033 0.00905467]\n","Logit 481: [0.68433036 0.30515615 0.01051349]\n","Logit 482: [0.29969436 0.68466867 0.01563697]\n","Logit 483: [0.86108036 0.12461574 0.01430389]\n","Logit 484: [0.75711144 0.23397027 0.00891829]\n","Logit 485: [0.24895559 0.14099177 0.61005265]\n","Logit 486: [0.50531053 0.030764   0.46392548]\n","Logit 487: [0.46735937 0.02705786 0.50558277]\n","Logit 488: [0.39258203 0.02551706 0.58190092]\n","Logit 489: [0.1024012  0.01611346 0.88148534]\n","Logit 490: [0.81680878 0.17280821 0.01038301]\n","Logit 491: [0.1148171  0.87361286 0.01157004]\n","Logit 492: [0.26722565 0.02993069 0.70284365]\n","Logit 493: [0.90962546 0.08181525 0.00855929]\n","Logit 494: [0.83690941 0.15327077 0.00981983]\n","Logit 495: [0.78854439 0.20390407 0.00755154]\n","Logit 496: [0.72530513 0.2659144  0.00878047]\n","Logit 497: [0.27861118 0.7083389  0.01304992]\n","Logit 498: [0.49149268 0.49680084 0.01170648]\n","Logit 499: [0.20636305 0.78300031 0.01063664]\n","Logit 500: [0.02982135 0.95763139 0.01254726]\n","Logit 501: [0.03401207 0.95450036 0.01148758]\n","Logit 502: [0.75933837 0.23158512 0.00907651]\n","Logit 503: [0.02550635 0.96227348 0.01222017]\n","Logit 504: [0.02434723 0.96309539 0.01255738]\n","Logit 505: [0.15052987 0.0170256  0.83244453]\n","Logit 506: [0.43514046 0.55438333 0.0104762 ]\n","Logit 507: [0.96697909 0.02247623 0.01054469]\n","Logit 508: [0.35665376 0.63024044 0.0131058 ]\n","Logit 509: [0.92362907 0.03063247 0.04573846]\n","Logit 510: [0.57178152 0.41750805 0.01071043]\n","Logit 511: [0.41600457 0.5730705  0.01092494]\n","Logit 512: [0.46069221 0.52921884 0.01008895]\n","Logit 513: [0.03830955 0.94507637 0.01661407]\n","Logit 514: [0.03090746 0.95308925 0.01600329]\n","Logit 515: [0.02152608 0.96534737 0.01312654]\n","Logit 516: [0.0217743  0.96510249 0.01312321]\n","Logit 517: [0.54198795 0.43635097 0.02166108]\n","Logit 518: [0.44332313 0.54370155 0.01297532]\n","Logit 519: [0.74732397 0.24143879 0.01123724]\n","Logit 520: [0.30169283 0.68053955 0.01776763]\n","Logit 521: [0.92218969 0.07013746 0.00767285]\n","Logit 522: [0.75483752 0.23326756 0.01189493]\n","Logit 523: [0.59512047 0.38763063 0.0172489 ]\n","Logit 524: [0.07291012 0.90464082 0.02244906]\n","Logit 525: [0.29493533 0.69179881 0.01326586]\n","Logit 526: [0.75749108 0.22555995 0.01694897]\n","Logit 527: [0.55346925 0.43569758 0.01083317]\n","Logit 528: [0.69979993 0.29200503 0.00819504]\n","Logit 529: [0.87179214 0.11608419 0.01212368]\n","Logit 530: [0.87179214 0.11608419 0.01212368]\n","Logit 531: [0.89968715 0.08977702 0.01053583]\n","Logit 532: [0.84537545 0.14593905 0.00868551]\n","Logit 533: [0.45644953 0.5305259  0.01302457]\n","Logit 534: [0.81257363 0.17624696 0.01117941]\n","Logit 535: [0.90960073 0.08177308 0.00862619]\n","Logit 536: [0.37668946 0.5952257  0.02808483]\n","Logit 537: [0.17342385 0.02528398 0.80129217]\n","Logit 538: [0.05152948 0.01798287 0.93048765]\n","Logit 539: [0.82803239 0.02875181 0.1432158 ]\n","Logit 540: [0.90488839 0.03898903 0.05612258]\n","Logit 541: [0.40250224 0.02514732 0.57235043]\n","Logit 542: [0.07645765 0.91249577 0.01104658]\n","Logit 543: [0.15364785 0.83528966 0.01106249]\n","Logit 544: [0.1556848  0.83507452 0.00924067]\n","Logit 545: [0.33702009 0.0412864  0.62169351]\n","Logit 546: [0.9533071  0.03809986 0.00859304]\n","Logit 547: [0.13678089 0.02768376 0.83553535]\n","Logit 548: [0.10987972 0.01619518 0.87392511]\n","Logit 549: [0.2630991 0.0217234 0.7151775]\n","Logit 550: [0.09056803 0.01744206 0.89198991]\n","Logit 551: [0.8609256 0.1290318 0.0100426]\n","Logit 552: [0.86214303 0.1280523  0.00980467]\n","Logit 553: [0.88810327 0.10484587 0.00705087]\n","Logit 554: [0.96260584 0.02545295 0.01194121]\n","Logit 555: [0.16011363 0.02420233 0.81568404]\n","Logit 556: [0.72000861 0.26996873 0.01002267]\n","Logit 557: [0.89673089 0.09363266 0.00963645]\n","Logit 558: [0.89132101 0.09902556 0.00965344]\n","Logit 559: [0.58899906 0.40009653 0.0109044 ]\n","Logit 560: [0.90396231 0.08411239 0.01192531]\n","Logit 561: [0.11719522 0.87318011 0.00962467]\n","Logit 562: [0.88723597 0.10157354 0.01119049]\n","Logit 563: [0.92305989 0.06771227 0.00922784]\n","Logit 564: [0.96378917 0.0245319  0.01167893]\n","Logit 565: [0.33611777 0.65488116 0.00900108]\n","Logit 566: [0.64903262 0.34101562 0.00995175]\n","Logit 567: [0.3803286  0.38219022 0.23748118]\n","Logit 568: [0.40484867 0.02478048 0.57037085]\n","Logit 569: [0.87707041 0.10926951 0.01366008]\n","Logit 570: [0.95400896 0.02091278 0.02507826]\n","Logit 571: [0.92835522 0.06280628 0.0088385 ]\n","Logit 572: [0.94020547 0.05262998 0.00716455]\n","Logit 573: [0.60892028 0.37993784 0.01114188]\n","Logit 574: [0.4788019  0.50968197 0.01151612]\n","Logit 575: [0.3400746  0.64660987 0.01331552]\n","Logit 576: [0.50698768 0.47860207 0.01441025]\n","Logit 577: [0.08465755 0.90561058 0.00973187]\n","Logit 578: [0.05287907 0.91291289 0.03420804]\n","Logit 579: [0.08452482 0.89485735 0.02061784]\n","Logit 580: [0.11881524 0.84788852 0.03329624]\n","Logit 581: [0.20662695 0.74481923 0.04855382]\n","Logit 582: [0.17450687 0.76932623 0.0561669 ]\n","Logit 583: [0.15951172 0.7842821  0.05620617]\n","Logit 584: [0.95631127 0.0330443  0.01064443]\n","Logit 585: [0.29321306 0.02935991 0.67742704]\n","Logit 586: [0.86251332 0.12996595 0.00752073]\n","Logit 587: [0.09293124 0.88063136 0.0264374 ]\n","Logit 588: [0.97102181 0.01692081 0.01205738]\n","Logit 589: [0.55878356 0.43095194 0.01026451]\n","Logit 590: [0.96817219 0.02301514 0.00881268]\n","Logit 591: [0.46461338 0.51831368 0.01707294]\n","Logit 592: [0.03195215 0.94941032 0.01863752]\n","Logit 593: [0.85813112 0.06965305 0.07221583]\n","Logit 594: [0.96796281 0.02022723 0.01180996]\n","Logit 595: [0.03299967 0.95362161 0.01337872]\n","Logit 596: [0.25460907 0.03027538 0.71511555]\n","Logit 597: [0.96194766 0.0236161  0.01443623]\n","Logit 598: [0.07220144 0.9163089  0.01148966]\n","Logit 599: [0.96548264 0.02363355 0.01088381]\n","Logit 600: [0.91888913 0.0707016  0.01040927]\n","Logit 601: [0.12827153 0.81466919 0.05705928]\n","Logit 602: [0.10221257 0.86526951 0.03251791]\n","Logit 603: [0.66343425 0.18896662 0.14759914]\n","Logit 604: [0.88997796 0.10213962 0.00788241]\n","Logit 605: [0.95781163 0.03319325 0.00899512]\n","Logit 606: [0.04401191 0.93232645 0.02366163]\n","Logit 607: [0.19523632 0.75733184 0.04743184]\n","Logit 608: [0.14367903 0.8032381  0.05308287]\n","Logit 609: [0.5083459  0.47661498 0.01503911]\n","Logit 610: [0.78777759 0.20271356 0.00950886]\n","Logit 611: [0.90033209 0.09205896 0.00760896]\n","Logit 612: [0.92234163 0.06983289 0.00782548]\n","Logit 613: [0.93415964 0.05387249 0.01196787]\n","Logit 614: [0.93684078 0.05315056 0.01000866]\n","Logit 615: [0.46726841 0.51823016 0.01450143]\n","Logit 616: [0.85251593 0.14002142 0.00746265]\n","Logit 617: [0.9712457  0.01563752 0.01311678]\n","Logit 618: [0.17714698 0.81195159 0.01090143]\n","Logit 619: [0.03038242 0.95351309 0.0161045 ]\n","Logit 620: [0.94774153 0.03579165 0.01646682]\n","Logit 621: [0.84140588 0.12815471 0.03043941]\n","Logit 622: [0.11064873 0.87543417 0.0139171 ]\n","Logit 623: [0.90348877 0.08713756 0.00937367]\n","Logit 624: [0.84443248 0.14175765 0.01380987]\n","Logit 625: [0.16644512 0.81827211 0.01528277]\n","Logit 626: [0.61309767 0.0373992  0.34950313]\n","Logit 627: [0.46721796 0.51918725 0.01359479]\n","Logit 628: [0.91935493 0.03078994 0.04985513]\n","Logit 629: [0.93674531 0.01677602 0.04647867]\n","Logit 630: [0.68338744 0.25439871 0.06221385]\n","Logit 631: [0.79222866 0.03386921 0.17390213]\n","Logit 632: [0.59494842 0.340069   0.06498258]\n","Logit 633: [0.94366445 0.04617491 0.01016063]\n","Logit 634: [0.89896532 0.0900451  0.01098958]\n","Logit 635: [0.43197706 0.02646682 0.54155612]\n","Logit 636: [0.94719271 0.04423595 0.00857134]\n","Logit 637: [0.06081852 0.91001468 0.0291668 ]\n","Logit 638: [0.77243272 0.19241458 0.03515269]\n","Logit 639: [0.91784812 0.07270335 0.00944854]\n","Logit 640: [0.73846254 0.25112658 0.01041088]\n","Logit 641: [0.71984251 0.27030211 0.00985537]\n","Logit 642: [0.51452061 0.44615101 0.03932839]\n","Logit 643: [0.92049401 0.06963348 0.00987251]\n","Logit 644: [0.10988858 0.87891707 0.01119435]\n","Logit 645: [0.10988858 0.87891707 0.01119435]\n","Logit 646: [0.92647079 0.06509612 0.00843309]\n","Logit 647: [0.37366852 0.5971206  0.02921087]\n","Logit 648: [0.17191617 0.80508857 0.02299526]\n","Logit 649: [0.95786441 0.03366847 0.00846712]\n","Logit 650: [0.84844073 0.14025658 0.01130269]\n","Logit 651: [0.3908777  0.07158496 0.53753734]\n","Logit 652: [0.51452061 0.44615101 0.03932839]\n","Logit 653: [0.24829246 0.71565707 0.03605047]\n","Logit 654: [0.78433624 0.20540714 0.01025662]\n","Logit 655: [0.57499757 0.41133373 0.01366869]\n","Logit 656: [0.07530906 0.91120195 0.01348899]\n","Logit 657: [0.51642827 0.46746619 0.01610553]\n","Logit 658: [0.6074463  0.38236404 0.01018966]\n","Logit 659: [0.4024931  0.55527046 0.04223644]\n","Logit 660: [0.1129728  0.87610583 0.01092137]\n","Logit 661: [0.0330415  0.94693739 0.02002112]\n","Logit 662: [0.41551606 0.56849802 0.01598593]\n","Logit 663: [0.09390584 0.88078888 0.02530529]\n","Logit 664: [0.05520976 0.89365471 0.05113553]\n","Logit 665: [0.03726823 0.94810879 0.01462298]\n","Logit 666: [0.11395862 0.86310763 0.02293375]\n","Logit 667: [0.14793007 0.82811479 0.02395514]\n","Logit 668: [0.95468597 0.03161614 0.01369789]\n","Logit 669: [0.93073754 0.05589513 0.01336732]\n","Logit 670: [0.90083992 0.08608241 0.01307766]\n","Logit 671: [0.3087591  0.01520054 0.67604036]\n","Logit 672: [0.80327046 0.18728599 0.00944355]\n","Logit 673: [0.10189387 0.02515373 0.8729524 ]\n","Logit 674: [0.96659855 0.02063737 0.01276408]\n","Logit 675: [0.0244372  0.96382625 0.01173655]\n","Logit 676: [0.05621505 0.93629319 0.00749176]\n","Logit 677: [0.70334588 0.27570332 0.0209508 ]\n","Logit 678: [0.90555799 0.08526451 0.0091775 ]\n","Logit 679: [0.9389806  0.03062888 0.03039052]\n","Logit 680: [0.7766504  0.21378081 0.00956878]\n","Logit 681: [0.54853087 0.44032672 0.01114242]\n","Logit 682: [0.4271471  0.56202286 0.01083004]\n","Logit 683: [0.92696771 0.06363781 0.00939448]\n","Logit 684: [0.42178468 0.56757333 0.01064199]\n","Logit 685: [0.93384592 0.03728564 0.02886844]\n","Logit 686: [0.36640256 0.62388071 0.00971674]\n","Logit 687: [0.04974608 0.93795249 0.01230142]\n","Logit 688: [0.09794495 0.01976075 0.8822943 ]\n","Logit 689: [0.96278156 0.0259217  0.01129674]\n","Logit 690: [0.76281968 0.22659278 0.01058754]\n","Logit 691: [0.90974918 0.07982361 0.01042721]\n","Logit 692: [0.64620054 0.32683952 0.02695994]\n","Logit 693: [0.89142573 0.09046656 0.01810771]\n","Logit 694: [0.96682854 0.02264898 0.01052247]\n","Logit 695: [0.95410667 0.03538691 0.01050642]\n","Logit 696: [0.9472623  0.04075506 0.01198264]\n","Logit 697: [0.89533534 0.09532099 0.00934366]\n","Logit 698: [0.7828616  0.20683084 0.01030756]\n","Logit 699: [0.73543689 0.25540424 0.00915887]\n","Logit 700: [0.9612404  0.03120225 0.00755736]\n","Logit 701: [0.05942173 0.02009911 0.92047916]\n","Logit 702: [0.05232334 0.0165934  0.93108327]\n","Logit 703: [0.84287256 0.1455425  0.01158494]\n","Logit 704: [0.94245508 0.04472965 0.01281526]\n","Logit 705: [0.38033471 0.60658725 0.01307804]\n","Logit 706: [0.31673394 0.0244239  0.65884216]\n","Logit 707: [0.62972629 0.35324444 0.01702927]\n","Logit 708: [0.03306876 0.95096364 0.0159676 ]\n","Logit 709: [0.88712729 0.10354209 0.00933062]\n","Logit 710: [0.03530583 0.95424759 0.01044658]\n","Logit 711: [0.93863839 0.03849709 0.02286452]\n","Logit 712: [0.81840862 0.17179924 0.00979214]\n","Logit 713: [0.90947277 0.07364036 0.01688688]\n","Logit 714: [0.35332659 0.01833656 0.62833685]\n","Logit 715: [0.94446347 0.04717154 0.00836498]\n","Logit 716: [0.96204326 0.02609223 0.01186451]\n","Logit 717: [0.95845381 0.02932689 0.0122193 ]\n","Logit 718: [0.95569796 0.03563682 0.00866522]\n","Logit 719: [0.42016032 0.55364034 0.02619933]\n","Logit 720: [0.68738725 0.29134245 0.0212703 ]\n","Logit 721: [0.96483239 0.02289099 0.01227662]\n","Logit 722: [0.05621505 0.93629319 0.00749176]\n","Logit 723: [0.94497338 0.04788177 0.00714485]\n","Logit 724: [0.79571716 0.18876882 0.01551401]\n","Logit 725: [0.5408574  0.44489716 0.01424544]\n","Logit 726: [0.91151449 0.01955656 0.06892895]\n","Logit 727: [0.9250599  0.02901882 0.04592129]\n","Logit 728: [0.67831593 0.30934492 0.01233915]\n","Logit 729: [0.73860687 0.24837062 0.01302252]\n","Logit 730: [0.88653217 0.03846013 0.07500771]\n","Logit 731: [0.11633303 0.86823882 0.01542814]\n","Logit 732: [0.95903393 0.02618903 0.01477704]\n","Logit 733: [0.06148683 0.92384086 0.01467231]\n","Logit 734: [0.82474568 0.16062762 0.0146267 ]\n","Logit 735: [0.06093431 0.92531417 0.01375151]\n","Logit 736: [0.04119444 0.94678212 0.01202344]\n","Logit 737: [0.11363073 0.87030407 0.0160652 ]\n","Logit 738: [0.06474749 0.92016004 0.01509247]\n","Logit 739: [0.33453395 0.64357286 0.0218932 ]\n","Logit 740: [0.17306774 0.02000472 0.80692754]\n","Logit 741: [0.15931564 0.01877146 0.82191291]\n","Logit 742: [0.15300119 0.02008409 0.82691472]\n","Logit 743: [0.83731081 0.15488693 0.00780227]\n","Logit 744: [0.95506858 0.03244216 0.01248926]\n","Logit 745: [0.59740716 0.09424702 0.30834582]\n","Logit 746: [0.91966834 0.06831706 0.0120146 ]\n","Logit 747: [0.91773909 0.07200583 0.01025507]\n","Logit 748: [0.05998512 0.92130032 0.01871456]\n","Logit 749: [0.51997685 0.46292884 0.0170943 ]\n","Logit 750: [0.83386654 0.15440051 0.01173295]\n","Logit 751: [0.4367405  0.55101447 0.01224503]\n","Logit 752: [0.22672601 0.73636124 0.03691274]\n","Logit 753: [0.96339194 0.02814183 0.00846623]\n","Logit 754: [0.9576734  0.01720114 0.02512546]\n","Logit 755: [0.05569519 0.9303563  0.0139485 ]\n","Logit 756: [0.11290679 0.86335523 0.02373798]\n","Logit 757: [0.29844225 0.68481098 0.01674677]\n","Logit 758: [0.93225852 0.05853025 0.00921122]\n","Logit 759: [0.44490582 0.53665885 0.01843533]\n","Logit 760: [0.29874436 0.68383264 0.017423  ]\n","Logit 761: [0.70645553 0.02904522 0.26449924]\n","Logit 762: [0.55075319 0.02073846 0.42850835]\n","Logit 763: [0.03737059 0.93095982 0.03166958]\n","Logit 764: [0.06721451 0.9229255  0.00986   ]\n","Logit 765: [0.0595497  0.93071839 0.00973191]\n","Logit 766: [0.1032248  0.26936749 0.62740772]\n","Logit 767: [0.04001995 0.02342335 0.9365567 ]\n","Logit 768: [0.06616797 0.92148414 0.01234789]\n","Logit 769: [0.01966251 0.96655039 0.0137871 ]\n","Logit 770: [0.0709802  0.91477252 0.01424728]\n","Logit 771: [0.0781694  0.90592042 0.01591018]\n","Logit 772: [0.96680493 0.02368904 0.00950603]\n","Logit 773: [0.84434411 0.14636723 0.00928866]\n","Logit 774: [0.33063013 0.65753652 0.01183335]\n","Logit 775: [0.62936178 0.04635401 0.32428421]\n","Logit 776: [0.18367686 0.01788267 0.79844047]\n","Logit 777: [0.79483843 0.19487867 0.0102829 ]\n","Logit 778: [0.66902391 0.31990588 0.01107021]\n","Logit 779: [0.95649633 0.03446812 0.00903556]\n","Logit 780: [0.9541928  0.02651891 0.0192883 ]\n","Logit 781: [0.58496626 0.40322062 0.01181312]\n","Logit 782: [0.36800694 0.61120109 0.02079197]\n","Logit 783: [0.20894255 0.77121525 0.01984221]\n","Logit 784: [0.06599722 0.91933064 0.01467214]\n","Logit 785: [0.11967438 0.86580215 0.01452347]\n","Logit 786: [0.9153354  0.07475108 0.00991352]\n","Logit 787: [0.95948254 0.02921537 0.01130209]\n","Logit 788: [0.70519196 0.27374061 0.02106742]\n","Logit 789: [0.94965186 0.04196999 0.00837815]\n","Logit 790: [0.83428796 0.14176613 0.02394591]\n","Logit 791: [0.03073586 0.95663167 0.01263246]\n","Logit 792: [0.31221803 0.67597947 0.0118025 ]\n","Logit 793: [0.77563106 0.21287566 0.01149328]\n","Logit 794: [0.94768733 0.04412957 0.0081831 ]\n","Logit 795: [0.0609231  0.91693647 0.02214043]\n","Logit 796: [0.91162443 0.07296352 0.01541205]\n","Logit 797: [0.72842099 0.26074304 0.01083597]\n","Logit 798: [0.90598487 0.08265493 0.0113602 ]\n","Logit 799: [0.8950844  0.09738048 0.00753512]\n","Logit 800: [0.85557398 0.11866551 0.02576051]\n","Logit 801: [0.94876558 0.04154361 0.0096908 ]\n","Logit 802: [0.84923436 0.14112661 0.00963902]\n","Logit 803: [0.88682325 0.10581255 0.0073642 ]\n","Logit 804: [0.8195989  0.17188117 0.00851994]\n","Logit 805: [0.90257259 0.08881758 0.00860984]\n","Logit 806: [0.8304187  0.16055226 0.00902904]\n","Logit 807: [0.94305222 0.03955669 0.01739108]\n","Logit 808: [0.81468376 0.17541516 0.00990108]\n","Logit 809: [0.78585096 0.20480137 0.00934766]\n","Logit 810: [0.95920819 0.01689547 0.02389634]\n","Logit 811: [0.03060923 0.95455293 0.01483784]\n","Logit 812: [0.23072126 0.75098531 0.01829344]\n","Logit 813: [0.81450268 0.1688224  0.01667492]\n","Logit 814: [0.9333202 0.0534574 0.0132224]\n","Logit 815: [0.63896688 0.03228172 0.3287514 ]\n","Logit 816: [0.8310499  0.16024337 0.00870673]\n","Logit 817: [0.63786648 0.3526075  0.00952602]\n","Logit 818: [0.25908141 0.73195319 0.0089654 ]\n","Logit 819: [0.0373247  0.94861869 0.01405661]\n","Logit 820: [0.37100877 0.61139145 0.01759977]\n","Logit 821: [0.18540446 0.79987364 0.01472191]\n","Logit 822: [0.49983261 0.06844236 0.43172503]\n","Logit 823: [0.25220099 0.72869955 0.01909946]\n","Logit 824: [0.94907241 0.04123365 0.00969394]\n","Logit 825: [0.51634391 0.04114029 0.4425158 ]\n","Logit 826: [0.9351467  0.05603666 0.00881664]\n","Logit 827: [0.7606854  0.22629004 0.01302456]\n","Logit 828: [0.90347652 0.08654539 0.00997808]\n","Logit 829: [0.35512708 0.62999878 0.01487414]\n","Logit 830: [0.64537466 0.34108636 0.01353898]\n","Logit 831: [0.94522078 0.04590234 0.00887688]\n","Logit 832: [0.04216329 0.01814306 0.93969364]\n","Logit 833: [0.36288308 0.30795608 0.32916084]\n","Logit 834: [0.04049639 0.94793796 0.01156564]\n","Logit 835: [0.04328126 0.01675172 0.93996702]\n","Logit 836: [0.03832233 0.02135043 0.94032725]\n","Logit 837: [0.89513505 0.09312509 0.01173986]\n","Logit 838: [0.88171681 0.10943416 0.00884904]\n","Logit 839: [0.71436412 0.27514308 0.0104928 ]\n","Logit 840: [0.17370624 0.81307441 0.01321935]\n","Logit 841: [0.50018127 0.4857401  0.01407863]\n","Logit 842: [0.75296636 0.23820738 0.00882626]\n","Logit 843: [0.14071114 0.84158194 0.01770692]\n","Logit 844: [0.63499431 0.34982206 0.01518363]\n","Logit 845: [0.49304439 0.49112219 0.01583342]\n","Logit 846: [0.05790862 0.01928364 0.92280775]\n","Logit 847: [0.03675699 0.95308072 0.01016229]\n","Logit 848: [0.70108964 0.14167202 0.15723834]\n","Logit 849: [0.83554008 0.14818538 0.01627454]\n","Logit 850: [0.7603217  0.11295656 0.12672173]\n","Logit 851: [0.50918132 0.41863667 0.07218201]\n","Logit 852: [0.83374338 0.15688538 0.00937123]\n","Logit 853: [0.21448756 0.77466557 0.01084687]\n","Logit 854: [0.54947228 0.44194476 0.00858296]\n","Logit 855: [0.0968093  0.72270204 0.18048865]\n","Logit 856: [0.21893693 0.06197962 0.71908345]\n","Logit 857: [0.40020714 0.59089049 0.00890238]\n","Logit 858: [0.12294941 0.86814851 0.00890208]\n","Logit 859: [0.27983741 0.07976476 0.64039784]\n","Logit 860: [0.92851051 0.05454953 0.01693996]\n","Logit 861: [0.04538131 0.93837813 0.01624057]\n","Logit 862: [0.86695718 0.1234118  0.00963102]\n","Logit 863: [0.17828038 0.02028033 0.80143929]\n","Logit 864: [0.04638308 0.9221251  0.03149182]\n","Logit 865: [0.49412353 0.49702729 0.00884918]\n","Logit 866: [0.91489819 0.07591976 0.00918205]\n","Logit 867: [0.94669932 0.04027597 0.01302472]\n","Logit 868: [0.96431202 0.01501884 0.02066913]\n","Logit 869: [0.95224973 0.03348746 0.01426281]\n","Logit 870: [0.50161204 0.4847568  0.01363116]\n","Logit 871: [0.91999547 0.07239465 0.00760988]\n","Logit 872: [0.26837467 0.71680648 0.01481884]\n","Logit 873: [0.47983182 0.50928408 0.0108841 ]\n","Logit 874: [0.94165984 0.02480203 0.03353814]\n","Logit 875: [0.9354754  0.05650981 0.0080148 ]\n","Logit 876: [0.92992178 0.0440057  0.02607252]\n","Logit 877: [0.9588857  0.02820233 0.01291197]\n","Logit 878: [0.91448846 0.07471839 0.01079315]\n","Logit 879: [0.94821691 0.04338486 0.00839823]\n","Logit 880: [0.93878016 0.01635897 0.04486086]\n","Logit 881: [0.3224539  0.66649616 0.01104994]\n","Logit 882: [0.71655507 0.27038469 0.01306024]\n","Logit 883: [0.92540945 0.05694867 0.01764188]\n","Logit 884: [0.09359638 0.89014769 0.01625593]\n","Logit 885: [0.73683466 0.25290845 0.01025689]\n","Logit 886: [0.05206376 0.93373057 0.01420567]\n","Logit 887: [0.8000284  0.18928257 0.01068903]\n","Logit 888: [0.06564039 0.91458331 0.0197763 ]\n","Logit 889: [0.44345062 0.54651995 0.01002943]\n","Logit 890: [0.09424239 0.89305611 0.0127015 ]\n","Logit 891: [0.17337332 0.02103257 0.80559411]\n","Logit 892: [0.83980388 0.15223185 0.00796427]\n","Logit 893: [0.7287414  0.04449694 0.22676166]\n","Logit 894: [0.26469916 0.01997756 0.71532328]\n","Logit 895: [0.79334351 0.19690111 0.00975538]\n","Logit 896: [0.45632635 0.0343562  0.50931746]\n","Logit 897: [0.02442468 0.96098356 0.01459176]\n","Logit 898: [0.9197989  0.07056457 0.00963653]\n","Logit 899: [0.6712808  0.31462298 0.01409621]\n","Logit 900: [0.29400433 0.0300491  0.67594657]\n","Logit 901: [0.81255164 0.15146732 0.03598103]\n","Logit 902: [0.92859438 0.06156226 0.00984336]\n","Logit 903: [0.84845166 0.14251932 0.00902902]\n","Logit 904: [0.92444273 0.06617007 0.0093872 ]\n","Logit 905: [0.0850693  0.90216219 0.01276851]\n","Logit 906: [0.56767288 0.42392356 0.00840355]\n","Logit 907: [0.59137717 0.39858579 0.01003704]\n","Logit 908: [0.65710075 0.33365348 0.00924576]\n","Logit 909: [0.21567151 0.77552608 0.00880241]\n","Logit 910: [0.69989008 0.28140375 0.01870616]\n","Logit 911: [0.6671479  0.32324236 0.00960974]\n","Logit 912: [0.55803843 0.43290642 0.00905515]\n","Logit 913: [0.06564039 0.91458331 0.0197763 ]\n","Logit 914: [0.46222151 0.52838929 0.0093892 ]\n","Logit 915: [0.60622132 0.38383543 0.00994325]\n","Logit 916: [0.08367885 0.90702047 0.00930067]\n","Logit 917: [0.23253032 0.75909435 0.00837533]\n","Logit 918: [0.09362724 0.89713742 0.00923534]\n","Logit 919: [0.27468493 0.71631558 0.00899949]\n","Logit 920: [0.13950909 0.85043413 0.01005678]\n","Logit 921: [0.95370901 0.03653065 0.00976034]\n","Logit 922: [0.5168667  0.47384059 0.00929271]\n","Logit 923: [0.95796368 0.03300456 0.00903176]\n","Logit 924: [0.85255591 0.13863333 0.00881076]\n","Logit 925: [0.93114368 0.05753987 0.01131645]\n","Logit 926: [0.6482217  0.33465494 0.01712336]\n","Logit 927: [0.88816677 0.10471266 0.00712057]\n","Logit 928: [0.84150438 0.14637441 0.01212121]\n","Logit 929: [0.72798806 0.25492471 0.01708723]\n","Logit 930: [0.9627271  0.02759195 0.00968096]\n","Logit 931: [0.86043982 0.1258948  0.01366538]\n","Logit 932: [0.1734228  0.81691678 0.00966042]\n","Logit 933: [0.32680796 0.66210715 0.0110849 ]\n","Logit 934: [0.78743547 0.02970863 0.1828559 ]\n","Logit 935: [0.79380902 0.19453127 0.01165972]\n","Logit 936: [0.58560224 0.4005177  0.01388006]\n","Logit 937: [0.89668964 0.09483977 0.00847059]\n","Logit 938: [0.05059464 0.93801651 0.01138885]\n","Logit 939: [0.79648268 0.02709491 0.17642241]\n","Logit 940: [0.8646849  0.02340595 0.11190915]\n","Logit 941: [0.8795935  0.10460778 0.01579872]\n","Logit 942: [0.06937789 0.91946935 0.01115276]\n","Logit 943: [0.01835664 0.97045499 0.01118837]\n","Logit 944: [0.84724048 0.14014368 0.01261584]\n","Logit 945: [0.19307876 0.79406134 0.0128599 ]\n","Logit 946: [0.04262156 0.94551103 0.01186741]\n","Logit 947: [0.09607254 0.89344429 0.01048317]\n","Logit 948: [0.32710072 0.65979442 0.01310486]\n","Logit 949: [0.51453901 0.47633574 0.00912525]\n","Logit 950: [0.89606774 0.09492161 0.00901064]\n","Logit 951: [0.03262825 0.95446895 0.0129028 ]\n","Logit 952: [0.03595106 0.95056902 0.01347992]\n","Logit 953: [0.02060526 0.96792618 0.01146856]\n","Logit 954: [0.0624902  0.91582395 0.02168585]\n","Logit 955: [0.02571093 0.9624428  0.01184628]\n","Logit 956: [0.70324241 0.28206215 0.01469545]\n","Logit 957: [0.26070523 0.72902694 0.01026783]\n","Logit 958: [0.45562263 0.02834133 0.51603604]\n","Logit 959: [0.27761515 0.02582223 0.69656262]\n","Logit 960: [0.14759906 0.07544146 0.77695948]\n","Logit 961: [0.09534652 0.89342882 0.01122466]\n","Logit 962: [0.96410249 0.02276226 0.01313525]\n","Logit 963: [0.87196171 0.11213002 0.01590827]\n","Logit 964: [0.89314038 0.06295373 0.04390589]\n","Logit 965: [0.73406258 0.21850356 0.04743387]\n","Logit 966: [0.79978701 0.18830376 0.01190924]\n","Logit 967: [0.08279432 0.02119596 0.89600972]\n","Logit 968: [0.07216571 0.02032548 0.90750881]\n","Logit 969: [0.24792039 0.56195473 0.19012487]\n","Logit 970: [0.0395441  0.94875527 0.01170063]\n","Logit 971: [0.3270238  0.04431192 0.62866429]\n","Logit 972: [0.95867679 0.03134779 0.00997542]\n","Logit 973: [0.36539976 0.02207292 0.61252732]\n","Logit 974: [0.74372394 0.2459737  0.01030236]\n","Logit 975: [0.92244692 0.06845659 0.0090965 ]\n","Logit 976: [0.05330085 0.93035976 0.0163394 ]\n","Logit 977: [0.10882119 0.88117723 0.01000158]\n","Logit 978: [0.19528296 0.02076944 0.78394759]\n","Logit 979: [0.02767708 0.95537946 0.01694346]\n","Logit 980: [0.5451767  0.44626576 0.00855754]\n","Logit 981: [0.64800379 0.32871319 0.02328303]\n","Logit 982: [0.74049137 0.23598289 0.02352574]\n","Logit 983: [0.62102833 0.34296442 0.03600726]\n","Logit 984: [0.12958631 0.85392657 0.01648712]\n","Logit 985: [0.51157691 0.46625118 0.02217191]\n","Logit 986: [0.1445249  0.838411   0.01706411]\n","Logit 987: [0.12749533 0.86028126 0.01222341]\n","Logit 988: [0.08950751 0.89258986 0.01790263]\n","Logit 989: [0.65671785 0.33313358 0.01014857]\n","Logit 990: [0.83869748 0.12260876 0.03869375]\n","Logit 991: [0.93235162 0.06018816 0.00746022]\n","Logit 992: [0.72437573 0.26726479 0.00835948]\n","Logit 993: [0.72437573 0.26726479 0.00835948]\n","Logit 994: [0.95322526 0.02895407 0.01782067]\n","Logit 995: [0.26691746 0.70564158 0.02744097]\n","Logit 996: [0.87347858 0.11905239 0.00746903]\n","Logit 997: [0.16985806 0.80777967 0.02236227]\n","Logit 998: [0.62761993 0.36217593 0.01020415]\n","Logit 999: [0.93763438 0.0522304  0.01013522]\n","Logit 1000: [0.27157317 0.01606423 0.71236261]\n","Logit 1001: [0.07743415 0.91194883 0.01061703]\n","Logit 1002: [0.03349291 0.95380082 0.01270627]\n","Logit 1003: [0.02186513 0.96534988 0.01278499]\n","Logit 1004: [0.55181198 0.42536749 0.02282053]\n","Logit 1005: [0.02386296 0.96443881 0.01169822]\n","Logit 1006: [0.02879946 0.9593118  0.01188873]\n","Logit 1007: [0.02660949 0.96166879 0.01172172]\n","Logit 1008: [0.93851046 0.0485396  0.01294994]\n","Logit 1009: [0.79852595 0.19397483 0.00749922]\n","Logit 1010: [0.94061038 0.03871005 0.02067957]\n","Logit 1011: [0.90109071 0.09102143 0.00788786]\n","Logit 1012: [0.77522822 0.2099785  0.01479328]\n","Logit 1013: [0.83420821 0.04508341 0.12070838]\n","Logit 1014: [0.09892311 0.88828189 0.012795  ]\n","Logit 1015: [0.95181295 0.03826369 0.00992336]\n","Logit 1016: [0.10898239 0.8779697  0.01304791]\n","Logit 1017: [0.10247643 0.88407338 0.01345019]\n","Logit 1018: [0.03246885 0.95492127 0.01260988]\n","Logit 1019: [0.78823433 0.20056594 0.01119972]\n","Logit 1020: [0.88803065 0.10351477 0.00845459]\n","Logit 1021: [0.88171661 0.10767178 0.01061161]\n","Logit 1022: [0.38245023 0.03920352 0.57834625]\n","Logit 1023: [0.90199471 0.08312382 0.01488146]\n","Logit 1024: [0.90199471 0.08312382 0.01488146]\n","Logit 1025: [0.02406825 0.96187229 0.01405946]\n","Logit 1026: [0.12221397 0.01861444 0.8591716 ]\n","Logit 1027: [0.86730443 0.12412613 0.00856944]\n","Logit 1028: [0.85817154 0.13231363 0.00951482]\n","Logit 1029: [0.13309832 0.02661809 0.84028359]\n","Logit 1030: [0.46275189 0.03835775 0.49889036]\n","Logit 1031: [0.11715027 0.86130835 0.02154139]\n","Logit 1032: [0.66724114 0.32328754 0.00947132]\n","Logit 1033: [0.1429656 0.8324081 0.0246263]\n","Logit 1034: [0.9513801  0.03978927 0.00883063]\n","Logit 1035: [0.96609788 0.02121919 0.01268293]\n","Logit 1036: [0.19100079 0.79710668 0.01189253]\n","Logit 1037: [0.94513637 0.02598655 0.02887707]\n","Logit 1038: [0.5497166  0.03330448 0.41697892]\n","Logit 1039: [0.37307698 0.02489715 0.60202586]\n","Logit 1040: [0.88280437 0.10826619 0.00892944]\n","Logit 1041: [0.14090252 0.8468515  0.01224597]\n","Logit 1042: [0.72677271 0.01879029 0.254437  ]\n","Logit 1043: [0.1200081  0.87002005 0.00997185]\n","Logit 1044: [0.56974361 0.41887362 0.01138277]\n","Logit 1045: [0.58089468 0.39266935 0.02643597]\n","Logit 1046: [0.50118712 0.03551684 0.46329604]\n","Logit 1047: [0.95857312 0.03275256 0.00867433]\n","Logit 1048: [0.27756662 0.71156533 0.01086804]\n","Logit 1049: [0.84880597 0.14219645 0.00899758]\n","Logit 1050: [0.57799056 0.41025713 0.01175231]\n","Logit 1051: [0.87775449 0.10474328 0.01750222]\n","Logit 1052: [0.60265573 0.3838202  0.01352407]\n","Logit 1053: [0.0687671  0.01450261 0.91673029]\n","Logit 1054: [0.33276959 0.65856783 0.00866258]\n","Logit 1055: [0.65009223 0.34123896 0.00866881]\n","Logit 1056: [0.69933303 0.29124083 0.00942614]\n","Logit 1057: [0.0406311  0.01683039 0.94253851]\n","Logit 1058: [0.73083818 0.25343567 0.01572615]\n","Logit 1059: [0.6775771  0.30720268 0.01522022]\n","Logit 1060: [0.70311401 0.27872513 0.01816087]\n","Logit 1061: [0.94973614 0.03985651 0.01040735]\n","Logit 1062: [0.918837   0.06390114 0.01726186]\n","Logit 1063: [0.80733602 0.17470526 0.01795872]\n","Logit 1064: [0.69163977 0.2897296  0.01863064]\n","Logit 1065: [0.92887722 0.05258309 0.01853969]\n","Logit 1066: [0.93668398 0.05573279 0.00758323]\n","Logit 1067: [0.53235313 0.39023931 0.07740756]\n","Logit 1068: [0.20767083 0.73787654 0.05445263]\n","Logit 1069: [0.8323262  0.15990282 0.00777098]\n","Logit 1070: [0.60689743 0.3823918  0.01071078]\n","Logit 1071: [0.09356099 0.01658723 0.88985178]\n","Logit 1072: [0.49939606 0.02570287 0.47490107]\n","Logit 1073: [0.62119433 0.368765   0.01004067]\n","Logit 1074: [0.75822474 0.2283283  0.01344696]\n","Logit 1075: [0.74365952 0.23885128 0.0174892 ]\n","Logit 1076: [0.67223419 0.31692136 0.01084445]\n","Logit 1077: [0.5526132  0.43800767 0.00937912]\n","Logit 1078: [0.76302128 0.13800984 0.09896888]\n","Logit 1079: [0.206583   0.78383486 0.00958215]\n","Logit 1080: [0.45266632 0.51595245 0.03138123]\n","Logit 1081: [0.51254869 0.47542084 0.01203047]\n","Logit 1082: [0.5231853  0.46760832 0.00920638]\n","Logit 1083: [0.80897289 0.18281244 0.00821467]\n","Logit 1084: [0.39659213 0.58956958 0.01383829]\n","Logit 1085: [0.90073768 0.08931448 0.00994783]\n","Logit 1086: [0.9150156  0.07477971 0.01020469]\n","Logit 1087: [0.95322229 0.03741438 0.00936333]\n","Logit 1088: [0.17558393 0.81109941 0.01331666]\n","Logit 1089: [0.76505337 0.22166696 0.01327967]\n","Logit 1090: [0.941173   0.04871297 0.01011403]\n","Logit 1091: [0.7804431  0.21005341 0.00950349]\n","Logit 1092: [0.8097463  0.17874832 0.01150538]\n","Logit 1093: [0.07044949 0.90782198 0.02172854]\n","Logit 1094: [0.75819807 0.23123735 0.01056458]\n","Logit 1095: [0.02919841 0.95565365 0.01514794]\n","Logit 1096: [0.21267445 0.77489796 0.01242759]\n","Logit 1097: [0.30225908 0.60923999 0.08850093]\n","Logit 1098: [0.04597534 0.88719622 0.06682844]\n","Logit 1099: [0.07473648 0.90947697 0.01578656]\n","Logit 1100: [0.26730288 0.71673812 0.015959  ]\n","Logit 1101: [0.9108913  0.04272795 0.04638075]\n","Logit 1102: [0.57126526 0.39281625 0.03591849]\n","Logit 1103: [0.89963454 0.03400807 0.06635738]\n","Logit 1104: [0.29486477 0.69467939 0.01045585]\n","Logit 1105: [0.90193721 0.08999606 0.00806673]\n","Logit 1106: [0.35481798 0.61426907 0.03091295]\n","Logit 1107: [0.90636695 0.06246707 0.03116598]\n","Logit 1108: [0.53721225 0.43039938 0.03238837]\n","Logit 1109: [0.49086524 0.49230543 0.01682932]\n","Logit 1110: [0.69530804 0.29614208 0.00854988]\n","Logit 1111: [0.78063127 0.20048269 0.01888604]\n","Logit 1112: [0.87479498 0.07956137 0.04564365]\n","Logit 1113: [0.89941648 0.04177969 0.05880384]\n","Logit 1114: [0.66510298 0.24120801 0.09368902]\n","Logit 1115: [0.83633702 0.03236503 0.13129795]\n","Logit 1116: [0.13381318 0.84913905 0.01704778]\n","Logit 1117: [0.17761799 0.72994209 0.09243992]\n","Logit 1118: [0.95970744 0.03059473 0.00969783]\n","Logit 1119: [0.95543447 0.03113517 0.01343036]\n","Logit 1120: [0.95307609 0.03548703 0.01143688]\n","Logit 1121: [0.54484672 0.44382324 0.01133004]\n","Logit 1122: [0.95586233 0.02174559 0.02239209]\n","Logit 1123: [0.67492566 0.04855837 0.27651597]\n","Logit 1124: [0.84974432 0.14042065 0.00983504]\n","Logit 1125: [0.95249922 0.01959551 0.02790528]\n","Logit 1126: [0.66696844 0.19739631 0.13563525]\n","Logit 1127: [0.16291377 0.82070556 0.01638067]\n","Logit 1128: [0.90253641 0.08601322 0.01145036]\n","Logit 1129: [0.91182585 0.07992285 0.0082513 ]\n","Logit 1130: [0.92869243 0.06256887 0.0087387 ]\n","Logit 1131: [0.88729991 0.10290066 0.00979942]\n","Logit 1132: [0.95197485 0.03985304 0.0081721 ]\n","Logit 1133: [0.87019853 0.12158112 0.00822034]\n","Logit 1134: [0.69966427 0.16793864 0.13239709]\n","Logit 1135: [0.27208911 0.69446469 0.0334462 ]\n","Logit 1136: [0.95262464 0.02603942 0.02133594]\n","Logit 1137: [0.7928742 0.1985218 0.008604 ]\n","Logit 1138: [0.15014918 0.0231777  0.82667312]\n","Logit 1139: [0.51408376 0.46625372 0.01966252]\n","Logit 1140: [0.86640979 0.12342424 0.01016597]\n","Logit 1141: [0.46945524 0.51811943 0.01242533]\n","Logit 1142: [0.55122638 0.03268611 0.4160875 ]\n","Logit 1143: [0.73469311 0.25070014 0.01460676]\n","Logit 1144: [0.56873567 0.41650245 0.01476189]\n","Logit 1145: [0.18902016 0.7938641  0.01711574]\n","Logit 1146: [0.06017577 0.92513143 0.0146928 ]\n","Logit 1147: [0.05245626 0.93641598 0.01112776]\n","Logit 1148: [0.06406078 0.9263193  0.00961992]\n","Logit 1149: [0.04757952 0.93420636 0.01821412]\n","Logit 1150: [0.25312723 0.71216958 0.03470319]\n","Logit 1151: [0.91070217 0.07730706 0.01199078]\n","Logit 1152: [0.84877449 0.14105019 0.01017532]\n","Logit 1153: [0.94302675 0.04699641 0.00997684]\n","Logit 1154: [0.56085795 0.42918563 0.00995642]\n","Logit 1155: [0.85026626 0.02176951 0.12796422]\n","Logit 1156: [0.68360929 0.30647631 0.0099144 ]\n","Logit 1157: [0.40875118 0.58265707 0.00859175]\n","Logit 1158: [0.89590497 0.09718758 0.00690745]\n","Logit 1159: [0.7715403  0.21826144 0.01019826]\n","Logit 1160: [0.88821733 0.10424672 0.00753595]\n","Logit 1161: [0.19956146 0.7846697  0.01576885]\n","Logit 1162: [0.67603286 0.31163344 0.0123337 ]\n","Logit 1163: [0.85295075 0.13721559 0.00983366]\n","Logit 1164: [0.87926562 0.10709349 0.01364088]\n","Logit 1165: [0.80256985 0.18494259 0.01248756]\n","Logit 1166: [0.96160275 0.01506466 0.0233326 ]\n","Logit 1167: [0.18652126 0.79415266 0.01932608]\n","Logit 1168: [0.94048632 0.0487131  0.01080058]\n","Logit 1169: [0.95985427 0.02053344 0.01961229]\n","Logit 1170: [0.63624982 0.07412026 0.28962992]\n","Logit 1171: [0.13770459 0.85223698 0.01005843]\n","Logit 1172: [0.89631674 0.09561969 0.00806357]\n","Logit 1173: [0.90514054 0.08614039 0.00871907]\n","Logit 1174: [0.18683879 0.80057024 0.01259098]\n","Logit 1175: [0.04006853 0.94921018 0.01072129]\n","Logit 1176: [0.28466853 0.69941987 0.0159116 ]\n","Logit 1177: [0.96942913 0.018427   0.01214387]\n","Logit 1178: [0.48458097 0.50635087 0.00906816]\n","Logit 1179: [0.34201428 0.64713038 0.01085534]\n","Logit 1180: [0.72901304 0.25640795 0.014579  ]\n","Logit 1181: [0.79745369 0.19409308 0.00845323]\n","Logit 1182: [0.96915133 0.02194051 0.00890816]\n","Logit 1183: [0.36075072 0.61186245 0.02738683]\n","Logit 1184: [0.0405382  0.94660271 0.0128591 ]\n","Logit 1185: [0.10021595 0.36720172 0.53258233]\n","Logit 1186: [0.39782248 0.58308332 0.0190942 ]\n","Logit 1187: [0.29235036 0.69348006 0.01416958]\n","Logit 1188: [0.31024839 0.65009553 0.03965608]\n","Logit 1189: [0.12799963 0.01902951 0.85297086]\n","Logit 1190: [0.9319095  0.01660812 0.05148238]\n","Logit 1191: [0.74118272 0.24778069 0.01103659]\n","Logit 1192: [0.75435331 0.23471696 0.01092972]\n","Logit 1193: [0.96548375 0.02540469 0.00911155]\n","Logit 1194: [0.83548984 0.14687984 0.01763031]\n","Logit 1195: [0.10192394 0.87408983 0.02398623]\n","Logit 1196: [0.10840725 0.01366016 0.87793258]\n","Logit 1197: [0.95156031 0.03174428 0.01669542]\n","Logit 1198: [0.96806712 0.01740481 0.01452807]\n","Logit 1199: [0.17382742 0.80691571 0.01925687]\n","Logit 1200: [0.45286838 0.53152902 0.0156026 ]\n","Logit 1201: [0.29531402 0.68361488 0.0210711 ]\n","Logit 1202: [0.89115521 0.09892559 0.00991919]\n","Logit 1203: [0.61390686 0.37198988 0.01410326]\n","Logit 1204: [0.20649175 0.77663269 0.01687555]\n","Logit 1205: [0.11380218 0.87673933 0.00945849]\n","Logit 1206: [0.952495   0.03956451 0.00794049]\n","Logit 1207: [0.9379188  0.04961088 0.01247031]\n","Logit 1208: [0.4672659  0.51974785 0.01298624]\n","Logit 1209: [0.80292538 0.18697715 0.01009747]\n","Logit 1210: [0.97316741 0.01323288 0.01359971]\n","Logit 1211: [0.90771531 0.08461333 0.00767136]\n","Logit 1212: [0.97130229 0.01246805 0.01622967]\n","Logit 1213: [0.9299392  0.05992992 0.01013088]\n","Logit 1214: [0.22300776 0.75295348 0.02403876]\n","Logit 1215: [0.68928663 0.3008331  0.00988027]\n","Logit 1216: [0.94463862 0.04867796 0.00668342]\n","Logit 1217: [0.31373713 0.67102699 0.01523588]\n","Logit 1218: [0.26460291 0.61020902 0.12518807]\n","Logit 1219: [0.17573164 0.81575519 0.00851317]\n","Logit 1220: [0.04756566 0.94332964 0.0091047 ]\n","Logit 1221: [0.82246347 0.16579326 0.01174327]\n","Logit 1222: [0.08347095 0.89915178 0.01737727]\n","Logit 1223: [0.63868576 0.02045048 0.34086376]\n","Logit 1224: [0.31267107 0.67038156 0.01694737]\n","Logit 1225: [0.23203203 0.75157303 0.01639495]\n","Logit 1226: [0.8967228  0.05766245 0.04561475]\n","Logit 1227: [0.92953807 0.03939169 0.03107023]\n","Logit 1228: [0.14950879 0.83003387 0.02045734]\n","Logit 1229: [0.93473624 0.0301352  0.03512856]\n","Logit 1230: [0.52845703 0.45823556 0.01330741]\n","Logit 1231: [0.92540142 0.0622103  0.01238828]\n","Logit 1232: [0.03107671 0.95550505 0.01341824]\n","Logit 1233: [0.03005179 0.95892511 0.0110231 ]\n","Logit 1234: [0.08347095 0.89915178 0.01737727]\n","Logit 1235: [0.93644273 0.05523087 0.00832641]\n","Logit 1236: [0.84713794 0.06703685 0.08582521]\n","Logit 1237: [0.09479927 0.89636147 0.00883926]\n","Logit 1238: [0.02910112 0.9585345  0.01236438]\n","Logit 1239: [0.88159963 0.10892652 0.00947385]\n","Logit 1240: [0.02678735 0.96103206 0.01218059]\n","Logit 1241: [0.23553227 0.75144964 0.01301809]\n","Logit 1242: [0.96576059 0.0234793  0.01076011]\n","Logit 1243: [0.94655268 0.04481459 0.00863273]\n","Logit 1244: [0.24566087 0.73952107 0.01481806]\n","Logit 1245: [0.362404   0.56542848 0.07216752]\n","Logit 1246: [0.06449395 0.9249867  0.01051935]\n","Logit 1247: [0.39669491 0.59260892 0.01069618]\n","Logit 1248: [0.02629116 0.96089555 0.01281329]\n","Logit 1249: [0.0924857  0.89642658 0.01108772]\n","Logit 1250: [0.12057317 0.86657342 0.01285341]\n","Logit 1251: [0.0924857  0.89642658 0.01108772]\n","Logit 1252: [0.07462643 0.91478573 0.01058783]\n","Logit 1253: [0.05585995 0.93390617 0.01023388]\n","Logit 1254: [0.06861672 0.92055037 0.01083291]\n","Logit 1255: [0.96780696 0.01979411 0.01239894]\n","Logit 1256: [0.92930941 0.05632954 0.01436105]\n","Logit 1257: [0.88957065 0.03174487 0.07868448]\n","Logit 1258: [0.67386481 0.31337624 0.01275895]\n","Logit 1259: [0.49865159 0.48662488 0.01472353]\n","Logit 1260: [0.83532244 0.15440598 0.01027157]\n","Logit 1261: [0.24809726 0.73887549 0.01302725]\n","Logit 1262: [0.10225049 0.88282435 0.01492516]\n","Logit 1263: [0.40988728 0.57803372 0.012079  ]\n","Logit 1264: [0.63149619 0.0405678  0.327936  ]\n","Logit 1265: [0.96397978 0.02610648 0.00991374]\n","Logit 1266: [0.06269364 0.92589875 0.01140761]\n","Logit 1267: [0.93980584 0.03841345 0.02178071]\n","Logit 1268: [0.21529627 0.76965384 0.01504989]\n","Logit 1269: [0.18517624 0.80279942 0.01202434]\n","Logit 1270: [0.64084234 0.34134738 0.01781028]\n","Logit 1271: [0.87952281 0.10707007 0.01340712]\n","Logit 1272: [0.74243289 0.24375484 0.01381227]\n","Logit 1273: [0.10315853 0.88594738 0.01089409]\n","Logit 1274: [0.84288823 0.13897454 0.01813724]\n","Logit 1275: [0.58211349 0.02381645 0.39407006]\n","Logit 1276: [0.15900998 0.82949946 0.01149056]\n","Logit 1277: [0.12183926 0.86599944 0.01216131]\n","Logit 1278: [0.77967534 0.05500975 0.16531491]\n","Logit 1279: [0.08903744 0.89849196 0.0124706 ]\n","Logit 1280: [0.89947329 0.09106624 0.00946047]\n","Logit 1281: [0.03116637 0.93241424 0.03641939]\n","Logit 1282: [0.2889477  0.70029356 0.01075874]\n","Logit 1283: [0.49938901 0.48877438 0.01183662]\n","Logit 1284: [0.88056746 0.11124321 0.00818933]\n","Logit 1285: [0.94427855 0.04724298 0.00847847]\n","Logit 1286: [0.92432629 0.06396978 0.01170393]\n","Logit 1287: [0.94941482 0.03456555 0.01601963]\n","Logit 1288: [0.08596956 0.90224348 0.01178696]\n","Logit 1289: [0.54327004 0.44731837 0.0094116 ]\n","Logit 1290: [0.94921913 0.04117966 0.00960122]\n","Logit 1291: [0.03270583 0.95673818 0.01055599]\n","Logit 1292: [0.87336993 0.11829881 0.00833127]\n","Logit 1293: [0.79359171 0.19099604 0.01541225]\n","Logit 1294: [0.76225411 0.21647843 0.02126746]\n","Logit 1295: [0.03744749 0.95243668 0.01011583]\n","Logit 1296: [0.43305991 0.54958164 0.01735845]\n","Logit 1297: [0.87216545 0.11561055 0.012224  ]\n","Logit 1298: [0.92863276 0.05953968 0.01182756]\n","Logit 1299: [0.90437038 0.08733313 0.00829649]\n","Logit 1300: [0.91691377 0.07428818 0.00879804]\n","Logit 1301: [0.63819456 0.34833841 0.01346703]\n","Logit 1302: [0.93392261 0.05722055 0.00885684]\n","Logit 1303: [0.60687199 0.38237577 0.01075225]\n","Logit 1304: [0.18609367 0.03663843 0.7772679 ]\n","Logit 1305: [0.4951536  0.0309813  0.47386511]\n","Logit 1306: [0.6097343  0.37454749 0.01571821]\n","Logit 1307: [0.80305217 0.17057316 0.02637467]\n","Logit 1308: [0.83001182 0.15930254 0.01068564]\n","Logit 1309: [0.95419356 0.033474   0.01233245]\n","Logit 1310: [0.29279229 0.69385043 0.01335727]\n","Logit 1311: [0.96622631 0.01680439 0.0169693 ]\n","Logit 1312: [0.33522631 0.6502803  0.01449339]\n","Logit 1313: [0.82865905 0.15349248 0.01784848]\n","Logit 1314: [0.21996603 0.0346257  0.74540828]\n","Logit 1315: [0.62081468 0.34875597 0.03042935]\n","Logit 1316: [0.48569642 0.49770018 0.0166034 ]\n","Logit 1317: [0.25589182 0.0339365  0.71017168]\n","Logit 1318: [0.26443649 0.72268411 0.01287939]\n","Logit 1319: [0.37695866 0.61246211 0.01057924]\n","Logit 1320: [0.95653951 0.03367118 0.00978931]\n","Logit 1321: [0.58009969 0.40815111 0.01174921]\n","Logit 1322: [0.96942072 0.01699218 0.0135871 ]\n","Logit 1323: [0.9393517  0.03418249 0.02646582]\n","Logit 1324: [0.15388736 0.83577511 0.01033753]\n","Logit 1325: [0.1019797  0.88809647 0.00992384]\n","Logit 1326: [0.87194442 0.11325667 0.01479891]\n","Logit 1327: [0.38690784 0.59517209 0.01792006]\n","Logit 1328: [0.7832739  0.18486666 0.03185945]\n","Logit 1329: [0.07945278 0.90530173 0.01524549]\n","Logit 1330: [0.09597915 0.89585053 0.00817032]\n","Logit 1331: [0.92586668 0.06586873 0.00826459]\n","Logit 1332: [0.91191321 0.08095913 0.00712766]\n","Logit 1333: [0.03827557 0.94885982 0.01286461]\n","Logit 1334: [0.82637583 0.15848857 0.0151356 ]\n","Logit 1335: [0.79057591 0.19621421 0.01320988]\n","Logit 1336: [0.02514761 0.96226897 0.01258342]\n","Logit 1337: [0.95183693 0.03518227 0.01298081]\n","Logit 1338: [0.05427409 0.91261356 0.03311235]\n","Logit 1339: [0.92589433 0.01882625 0.05527942]\n","Logit 1340: [0.53051078 0.45867067 0.01081855]\n","Logit 1341: [0.82701181 0.16091175 0.01207644]\n","Logit 1342: [0.94077179 0.0525461  0.00668212]\n","Logit 1343: [0.34346165 0.64104451 0.01549384]\n","Logit 1344: [0.54057673 0.45078746 0.00863581]\n","Logit 1345: [0.17954504 0.80055012 0.01990485]\n","Logit 1346: [0.54777024 0.44359786 0.00863191]\n","Logit 1347: [0.62394929 0.36715924 0.00889147]\n","Logit 1348: [0.05522366 0.93472182 0.01005452]\n","Logit 1349: [0.91776629 0.07341027 0.00882344]\n","Logit 1350: [0.97140777 0.01709365 0.01149857]\n","Logit 1351: [0.57885846 0.22596591 0.19517562]\n","Logit 1352: [0.93971082 0.05023023 0.01005895]\n","Logit 1353: [0.84886182 0.13731    0.01382819]\n","Logit 1354: [0.91010173 0.07883755 0.01106072]\n","Logit 1355: [0.20835473 0.77299859 0.01864668]\n","Logit 1356: [0.94842799 0.0417321  0.00983991]\n","Logit 1357: [0.94905506 0.01782955 0.03311539]\n","Logit 1358: [0.95163275 0.03500339 0.01336386]\n","Logit 1359: [0.96315681 0.02628877 0.01055442]\n","Logit 1360: [0.16361357 0.0170709  0.81931553]\n","Logit 1361: [0.61254111 0.36863373 0.01882516]\n","Logit 1362: [0.19136951 0.01600666 0.79262383]\n","Logit 1363: [0.88676857 0.10485456 0.00837687]\n","Logit 1364: [0.96459352 0.02627662 0.00912986]\n","Logit 1365: [0.96313173 0.01821812 0.01865015]\n","Logit 1366: [0.22012068 0.0155116  0.76436772]\n","Logit 1367: [0.21379911 0.01609272 0.77010817]\n","Logit 1368: [0.28637052 0.01754565 0.69608383]\n","Logit 1369: [0.80947788 0.17146643 0.01905569]\n","Logit 1370: [0.78572488 0.20708112 0.007194  ]\n","Logit 1371: [0.75715729 0.19991774 0.04292497]\n","Logit 1372: [0.69977982 0.28259858 0.01762159]\n","Logit 1373: [0.70605482 0.28332849 0.01061669]\n","Logit 1374: [0.02565709 0.96042741 0.0139155 ]\n","Logit 1375: [0.90620205 0.01589959 0.07789836]\n","Logit 1376: [0.72463247 0.25837566 0.01699187]\n","Logit 1377: [0.2931505  0.68727665 0.01957285]\n","Logit 1378: [0.02486801 0.96184708 0.01328491]\n","Logit 1379: [0.62347032 0.33930612 0.03722356]\n","Logit 1380: [0.93766612 0.05356228 0.0087716 ]\n","Logit 1381: [0.85144097 0.13478332 0.01377571]\n","Logit 1382: [0.04964702 0.93883113 0.01152185]\n","Logit 1383: [0.04940638 0.93725055 0.01334307]\n","Logit 1384: [0.95097888 0.03843605 0.01058507]\n","Logit 1385: [0.94113053 0.04855642 0.01031305]\n","Logit 1386: [0.42277157 0.54844441 0.02878402]\n","Logit 1387: [0.93999302 0.05224686 0.00776012]\n","Logit 1388: [0.92048619 0.06964139 0.00987243]\n","Logit 1389: [0.92149203 0.06970047 0.0088075 ]\n","Logit 1390: [0.80017074 0.01935877 0.1804705 ]\n","Logit 1391: [0.93558412 0.05633729 0.00807859]\n","Logit 1392: [0.30636761 0.02576971 0.66786268]\n","Logit 1393: [0.90490288 0.08574539 0.00935174]\n","Logit 1394: [0.93871377 0.05097955 0.01030668]\n","Logit 1395: [0.96239776 0.02520014 0.01240209]\n","Logit 1396: [0.22117192 0.01566962 0.76315847]\n","Logit 1397: [0.87386923 0.11851102 0.00761975]\n","Logit 1398: [0.41472926 0.57411012 0.01116062]\n","Logit 1399: [0.96649342 0.02112447 0.01238211]\n","Logit 1400: [0.79156302 0.19298888 0.0154481 ]\n","Logit 1401: [0.85636733 0.13459033 0.00904234]\n","Logit 1402: [0.08521345 0.90293285 0.0118537 ]\n","Logit 1403: [0.77828936 0.21204646 0.00966418]\n","Logit 1404: [0.27728297 0.04459608 0.67812095]\n","Logit 1405: [0.55964074 0.41225042 0.02810884]\n","Logit 1406: [0.06900264 0.91740329 0.01359406]\n","Logit 1407: [0.1243207  0.86401075 0.01166856]\n","Logit 1408: [0.11049755 0.87658906 0.01291339]\n","Logit 1409: [0.08619187 0.90374876 0.01005937]\n","Logit 1410: [0.245702   0.73532372 0.01897428]\n","Logit 1411: [0.93426506 0.05704622 0.00868872]\n","Logit 1412: [0.96127222 0.02962935 0.00909843]\n","Logit 1413: [0.33804785 0.65096826 0.01098389]\n","Logit 1414: [0.62074108 0.36491486 0.01434406]\n","Logit 1415: [0.4396556  0.54025777 0.02008663]\n","Logit 1416: [0.80528631 0.18202378 0.01268991]\n","Logit 1417: [0.87796599 0.11265434 0.00937968]\n","Logit 1418: [0.87492041 0.11137291 0.01370667]\n","Logit 1419: [0.03996242 0.94924238 0.0107952 ]\n","Logit 1420: [0.81594324 0.1722462  0.01181055]\n","Logit 1421: [0.59072505 0.39775764 0.01151731]\n","Logit 1422: [0.03848982 0.94998724 0.01152293]\n","Logit 1423: [0.07763555 0.91245315 0.0099113 ]\n","Logit 1424: [0.8708209  0.12189107 0.00728803]\n","Logit 1425: [0.06900264 0.91740329 0.01359406]\n","Logit 1426: [0.79651504 0.18960506 0.01387989]\n","Logit 1427: [0.0492689  0.93944676 0.01128434]\n","Logit 1428: [0.61113837 0.37121753 0.0176441 ]\n","Logit 1429: [0.06629979 0.91904691 0.0146533 ]\n","Logit 1430: [0.72559542 0.26266496 0.01173962]\n","Logit 1431: [0.66376762 0.13000346 0.20622892]\n","Logit 1432: [0.49829392 0.02553369 0.47617239]\n","Logit 1433: [0.81757978 0.03488481 0.14753541]\n","Logit 1434: [0.30960633 0.67855772 0.01183595]\n","Logit 1435: [0.91026115 0.07685571 0.01288313]\n","Logit 1436: [0.72299268 0.26610405 0.01090327]\n","Logit 1437: [0.91257551 0.06617134 0.02125315]\n","Logit 1438: [0.85369368 0.13760361 0.00870271]\n","Logit 1439: [0.81309883 0.17670567 0.0101955 ]\n","Logit 1440: [0.72654094 0.25918267 0.01427639]\n","Logit 1441: [0.9610514  0.03098359 0.00796502]\n","Logit 1442: [0.41815012 0.02626566 0.55558422]\n","Logit 1443: [0.8757026  0.11256298 0.01173442]\n","Logit 1444: [0.03033432 0.95900173 0.01066396]\n","Logit 1445: [0.92991937 0.06028082 0.00979982]\n","Logit 1446: [0.59593597 0.392354   0.01171003]\n","Logit 1447: [0.30252073 0.681741   0.01573827]\n","Logit 1448: [0.73785881 0.24933355 0.01280764]\n","Logit 1449: [0.08959205 0.02846109 0.88194686]\n","Logit 1450: [0.06900264 0.91740329 0.01359406]\n","Logit 1451: [0.89023451 0.09355554 0.01620995]\n","Logit 1452: [0.34942844 0.03820322 0.61236833]\n","Logit 1453: [0.07795881 0.01602108 0.90602011]\n","Logit 1454: [0.93207307 0.0223526  0.04557434]\n","Logit 1455: [0.3931716  0.59310946 0.01371894]\n","Logit 1456: [0.02415462 0.9639113  0.01193407]\n","Logit 1457: [0.03598252 0.95326094 0.01075654]\n","Logit 1458: [0.02572011 0.96419791 0.01008198]\n","Logit 1459: [0.03528732 0.95444599 0.01026669]\n","Logit 1460: [0.89735079 0.0934297  0.00921951]\n","Logit 1461: [0.96869421 0.01942905 0.01187674]\n","Logit 1462: [0.14534826 0.03630835 0.8183434 ]\n","Logit 1463: [0.81151768 0.17345833 0.01502399]\n","Logit 1464: [0.03528742 0.95328437 0.01142821]\n","Logit 1465: [0.86422897 0.12551112 0.01025991]\n","Logit 1466: [0.05257308 0.93667016 0.01075676]\n","Logit 1467: [0.42088204 0.56635868 0.01275928]\n","Logit 1468: [0.3350423  0.65470102 0.01025667]\n","Logit 1469: [0.03153255 0.95728986 0.01117759]\n","Logit 1470: [0.19394364 0.79218436 0.013872  ]\n","Logit 1471: [0.61835708 0.36959864 0.01204428]\n","Logit 1472: [0.85745473 0.13012172 0.01242354]\n","Logit 1473: [0.84674247 0.14362801 0.00962953]\n","Logit 1474: [0.69025732 0.29687598 0.0128667 ]\n","Logit 1475: [0.05708896 0.93257133 0.01033972]\n","Logit 1476: [0.06900264 0.91740329 0.01359406]\n","Logit 1477: [0.7999732  0.17534509 0.02468171]\n","Logit 1478: [0.60611013 0.38489099 0.00899889]\n","Logit 1479: [0.87117472 0.120417   0.00840829]\n","Logit 1480: [0.95086318 0.01647273 0.03266409]\n","Logit 1481: [0.04459477 0.93938368 0.01602155]\n","Logit 1482: [0.02345882 0.96538946 0.01115172]\n","Logit 1483: [0.27746122 0.02568221 0.69685657]\n","Logit 1484: [0.20327918 0.02164629 0.77507453]\n","Logit 1485: [0.91521175 0.06098692 0.02380133]\n","Logit 1486: [0.0683714  0.92289342 0.00873517]\n","Logit 1487: [0.22854914 0.7468268  0.02462406]\n","Logit 1488: [0.3786347  0.17728929 0.444076  ]\n","Logit 1489: [0.67542751 0.31548633 0.00908616]\n","Logit 1490: [0.40377749 0.58691933 0.00930318]\n","Logit 1491: [0.11054596 0.86616768 0.02328636]\n","Logit 1492: [0.66239526 0.03218332 0.30542142]\n","Logit 1493: [0.742329   0.24671407 0.01095693]\n","Logit 1494: [0.90891535 0.08215654 0.00892811]\n","Logit 1495: [0.71258241 0.27674452 0.01067307]\n","Logit 1496: [0.34511651 0.64350445 0.01137904]\n","Logit 1497: [0.58452518 0.11276343 0.30271139]\n","Logit 1498: [0.46616001 0.52411779 0.0097222 ]\n","Logit 1499: [0.04406023 0.94273952 0.01320024]\n","Logit 1500: [0.04577133 0.01858381 0.93564486]\n","Logit 1501: [0.05068499 0.01920969 0.93010531]\n","Logit 1502: [0.04524139 0.01767358 0.93708503]\n","Logit 1503: [0.21795857 0.77197597 0.01006546]\n","Logit 1504: [0.5394185  0.02693486 0.43364663]\n","Logit 1505: [0.73740444 0.20050915 0.06208641]\n","Logit 1506: [0.39140162 0.59973748 0.0088609 ]\n","Logit 1507: [0.96327986 0.02210779 0.01461236]\n","Logit 1508: [0.94935841 0.04255537 0.00808622]\n","Logit 1509: [0.63084061 0.35979333 0.00936606]\n","Logit 1510: [0.61566417 0.37378404 0.01055179]\n","Logit 1511: [0.954997   0.02605333 0.01894967]\n","Logit 1512: [0.36017913 0.62599093 0.01382994]\n","Logit 1513: [0.78333188 0.20444413 0.01222399]\n","Logit 1514: [0.86875367 0.04497569 0.08627064]\n","Logit 1515: [0.62123771 0.36951175 0.00925055]\n","Logit 1516: [0.96892061 0.01715008 0.01392931]\n","Logit 1517: [0.75606999 0.23273752 0.01119249]\n","Logit 1518: [0.71480477 0.27531279 0.00988244]\n","Logit 1519: [0.72298572 0.26325827 0.013756  ]\n","Logit 1520: [0.96272497 0.02575617 0.01151887]\n","Logit 1521: [0.91388119 0.07845545 0.00766336]\n","Logit 1522: [0.93167971 0.05909679 0.0092235 ]\n","Logit 1523: [0.9614253  0.02103423 0.01754047]\n","Logit 1524: [0.8898101  0.10150051 0.00868938]\n","Logit 1525: [0.89016952 0.10075592 0.00907456]\n","Logit 1526: [0.92351987 0.06313852 0.01334161]\n","Logit 1527: [0.9491351  0.04123637 0.00962853]\n","Logit 1528: [0.82609133 0.16547087 0.00843779]\n","Logit 1529: [0.38093223 0.60576291 0.01330486]\n","Logit 1530: [0.03683573 0.953957   0.00920727]\n","Logit 1531: [0.81991118 0.17002642 0.0100624 ]\n","Logit 1532: [0.7766751  0.21191685 0.01140805]\n","Logit 1533: [0.5489216 0.4419332 0.0091452]\n","Logit 1534: [0.93041217 0.05496905 0.01461879]\n","Logit 1535: [0.9385359  0.05017968 0.01128442]\n","Logit 1536: [0.08045747 0.91061517 0.00892736]\n","Logit 1537: [0.82016835 0.16912739 0.01070427]\n","Logit 1538: [0.90212582 0.07811802 0.01975615]\n","Logit 1539: [0.76210075 0.22815474 0.00974451]\n","Logit 1540: [0.90337588 0.08862304 0.00800108]\n","Logit 1541: [0.21695339 0.02209835 0.76094826]\n","Logit 1542: [0.76951729 0.05466547 0.17581724]\n","Logit 1543: [0.94268921 0.04923419 0.0080766 ]\n","Logit 1544: [0.0648263 0.9212801 0.0138936]\n","Logit 1545: [0.61627423 0.37305987 0.0106659 ]\n","Logit 1546: [0.89200263 0.09848633 0.00951104]\n","Logit 1547: [0.83453873 0.15298593 0.01247534]\n","Logit 1548: [0.83388574 0.15410279 0.01201147]\n","Logit 1549: [0.02929668 0.95699903 0.01370429]\n","Logit 1550: [0.0395797  0.95169816 0.00872214]\n","Logit 1551: [0.26617923 0.72355016 0.01027062]\n","Logit 1552: [0.33577501 0.64912221 0.01510278]\n","Logit 1553: [0.92887021 0.06355083 0.00757895]\n","Logit 1554: [0.25765443 0.72756637 0.0147792 ]\n","Logit 1555: [0.70300156 0.03462642 0.26237202]\n","Logit 1556: [0.72818839 0.26142456 0.01038705]\n","Logit 1557: [0.2618547  0.72831888 0.00982642]\n","Logit 1558: [0.8033944  0.18841565 0.00818995]\n","Logit 1559: [0.05023988 0.02104561 0.92871451]\n","Logit 1560: [0.35729559 0.03611005 0.60659436]\n","Logit 1561: [0.87475251 0.11412792 0.01111957]\n","Logit 1562: [0.91780373 0.0739394  0.00825687]\n","Logit 1563: [0.93320854 0.05997932 0.00681214]\n","Logit 1564: [0.79996632 0.18624259 0.01379109]\n","Logit 1565: [0.12463251 0.03778409 0.8375834 ]\n","Logit 1566: [0.8567895  0.13248813 0.01072238]\n","Logit 1567: [0.85375471 0.13541381 0.01083148]\n","Logit 1568: [0.89337555 0.09483881 0.01178564]\n","Logit 1569: [0.62051482 0.36836162 0.01112356]\n","Logit 1570: [0.94524353 0.04329108 0.01146539]\n","Logit 1571: [0.09356476 0.89588232 0.01055292]\n","Logit 1572: [0.8945871  0.09720616 0.00820674]\n","Logit 1573: [0.90689385 0.08462242 0.00848373]\n","Logit 1574: [0.90564603 0.08566405 0.00868992]\n","Logit 1575: [0.80076912 0.18697648 0.0122544 ]\n","Logit 1576: [0.89984045 0.09039479 0.00976476]\n","Logit 1577: [0.0325673 0.9522208 0.0152119]\n","Logit 1578: [0.03846403 0.9479611  0.01357486]\n","Logit 1579: [0.05950062 0.9277969  0.01270248]\n","Logit 1580: [0.3231126  0.66330782 0.01357958]\n","Logit 1581: [0.3822532  0.02483357 0.59291323]\n","Logit 1582: [0.09572242 0.89214641 0.01213118]\n","Logit 1583: [0.18345314 0.80195854 0.01458832]\n","Logit 1584: [0.23240579 0.75536107 0.01223314]\n","Logit 1585: [0.02728281 0.96128277 0.01143442]\n","Logit 1586: [0.95867125 0.02811354 0.01321521]\n","Logit 1587: [0.77828703 0.21370892 0.00800404]\n","Logit 1588: [0.59960494 0.38600153 0.01439353]\n","Logit 1589: [0.03939176 0.95111845 0.00948979]\n","Logit 1590: [0.07194291 0.91559491 0.01246219]\n","Logit 1591: [0.11571928 0.02188681 0.86239392]\n","Logit 1592: [0.59035655 0.39192038 0.01772307]\n","Logit 1593: [0.0277106  0.96121834 0.01107106]\n","Logit 1594: [0.94801219 0.03891953 0.01306828]\n","Logit 1595: [0.9114664  0.08060153 0.00793207]\n","Logit 1596: [0.94302727 0.03963298 0.01733975]\n","Logit 1597: [0.07790101 0.01498062 0.90711837]\n","Logit 1598: [0.65874783 0.32930403 0.01194813]\n","Logit 1599: [0.0325673 0.9522208 0.0152119]\n","Logit 1600: [0.73842293 0.24843004 0.01314704]\n","Logit 1601: [0.87658129 0.07581344 0.04760527]\n","Logit 1602: [0.8016537  0.18700032 0.01134599]\n","Logit 1603: [0.56407732 0.4249571  0.01096558]\n","Logit 1604: [0.73420762 0.2363342  0.02945818]\n","Logit 1605: [0.13183064 0.8544129  0.01375646]\n","Logit 1606: [0.13240058 0.85894518 0.00865424]\n","Logit 1607: [0.7272552  0.03277507 0.23996973]\n","Logit 1608: [0.07819117 0.90838774 0.01342109]\n","Logit 1609: [0.06087167 0.9283207  0.01080763]\n","Logit 1610: [0.04040162 0.02146762 0.93813077]\n","Logit 1611: [0.08568101 0.9063231  0.00799589]\n","Logit 1612: [0.8986095  0.0909899  0.01040061]\n","Logit 1613: [0.90844709 0.08124181 0.0103111 ]\n","Logit 1614: [0.38302559 0.60671717 0.01025725]\n","Logit 1615: [0.93297516 0.02073323 0.0462916 ]\n","Logit 1616: [0.09047494 0.89981759 0.00970747]\n","Logit 1617: [0.9407894  0.04670212 0.01250848]\n","Logit 1618: [0.09520548 0.89167176 0.01312276]\n","Logit 1619: [0.96081247 0.02365744 0.01553009]\n","Logit 1620: [0.82378212 0.16683115 0.00938673]\n","Logit 1621: [0.88676899 0.09640811 0.0168229 ]\n","Logit 1622: [0.65385653 0.28314918 0.0629943 ]\n","Logit 1623: [0.47377333 0.51077252 0.01545415]\n","Logit 1624: [0.84012436 0.14945382 0.01042182]\n","Logit 1625: [0.80879164 0.02816611 0.16304225]\n","Logit 1626: [0.96703464 0.02191398 0.01105137]\n","Logit 1627: [0.96592548 0.02316443 0.01091009]\n","Logit 1628: [0.68988814 0.30153701 0.00857486]\n","Logit 1629: [0.81736338 0.03390175 0.14873487]\n","Logit 1630: [0.94160813 0.04649238 0.01189949]\n","Logit 1631: [0.69252375 0.28532354 0.02215271]\n","Logit 1632: [0.72595334 0.12365139 0.15039527]\n","Logit 1633: [0.72261926 0.26583677 0.01154398]\n","Logit 1634: [0.04393626 0.01804021 0.93802353]\n","Logit 1635: [0.7136186  0.27231779 0.01406361]\n","Logit 1636: [0.88955359 0.10167429 0.00877213]\n","Logit 1637: [0.90584782 0.082456   0.01169618]\n","Logit 1638: [0.76834057 0.22024077 0.01141866]\n","Logit 1639: [0.08464299 0.02296003 0.89239698]\n","Logit 1640: [0.87460317 0.02912004 0.09627679]\n","Logit 1641: [0.91205617 0.03048589 0.05745794]\n","Logit 1642: [0.07613235 0.91491564 0.00895202]\n","Logit 1643: [0.75697784 0.23415754 0.00886462]\n","Logit 1644: [0.76102688 0.23051887 0.00845424]\n","Logit 1645: [0.86377702 0.12892296 0.00730002]\n","Logit 1646: [0.85273592 0.13900155 0.00826253]\n","Logit 1647: [0.10807825 0.87249465 0.0194271 ]\n","Logit 1648: [0.0588023 0.8971998 0.0439979]\n","Logit 1649: [0.29080683 0.69421133 0.01498184]\n","Logit 1650: [0.09389378 0.87809942 0.0280068 ]\n","Logit 1651: [0.20374675 0.78524746 0.01100578]\n","Logit 1652: [0.17667284 0.80308442 0.02024273]\n","Logit 1653: [0.06658532 0.91996773 0.01344695]\n","Logit 1654: [0.96296783 0.02494566 0.01208651]\n","Logit 1655: [0.02986279 0.95896195 0.01117527]\n","Logit 1656: [0.05997864 0.92751955 0.01250181]\n","Logit 1657: [0.08715958 0.8945797  0.01826072]\n","Logit 1658: [0.07717377 0.90907645 0.01374978]\n","Logit 1659: [0.03727001 0.94630419 0.0164258 ]\n","Logit 1660: [0.01877559 0.97103123 0.01019317]\n","Logit 1661: [0.03089281 0.95870367 0.01040352]\n","Logit 1662: [0.02449166 0.9603308  0.01517754]\n","Logit 1663: [0.69632639 0.29398061 0.009693  ]\n","Logit 1664: [0.93940119 0.04629291 0.0143059 ]\n","Logit 1665: [0.04603731 0.94154163 0.01242106]\n","Logit 1666: [0.94916955 0.04444747 0.00638298]\n","Logit 1667: [0.03894185 0.95017819 0.01087996]\n","Logit 1668: [0.03304189 0.95624145 0.01071666]\n","Logit 1669: [0.11106377 0.87273155 0.01620468]\n","Logit 1670: [0.01969605 0.96725384 0.01305011]\n","Logit 1671: [0.77809178 0.20809544 0.01381278]\n","Logit 1672: [0.78333621 0.20919137 0.00747242]\n","Logit 1673: [0.03687575 0.94918237 0.01394189]\n","Logit 1674: [0.44346462 0.54600375 0.01053164]\n","Logit 1675: [0.02685686 0.95509467 0.01804846]\n","Logit 1676: [0.78029684 0.21001405 0.00968911]\n","Logit 1677: [0.10828632 0.88264526 0.00906842]\n","Logit 1678: [0.0325673 0.9522208 0.0152119]\n","Logit 1679: [0.07208192 0.91334152 0.01457656]\n","Logit 1680: [0.08886865 0.893906   0.01722535]\n","Logit 1681: [0.22378853 0.76412309 0.01208838]\n","Logit 1682: [0.55974788 0.42833618 0.01191594]\n","Logit 1683: [0.17872032 0.80882919 0.01245049]\n","Logit 1684: [0.96715498 0.02115959 0.01168543]\n","Logit 1685: [0.95345227 0.02494154 0.02160619]\n","Logit 1686: [0.92928907 0.0436975  0.02701342]\n","Logit 1687: [0.69089714 0.27241507 0.03668779]\n","Logit 1688: [0.50370957 0.47783499 0.01845544]\n","Logit 1689: [0.06076797 0.66881026 0.27042176]\n","Logit 1690: [0.87649176 0.11100576 0.01250248]\n","Logit 1691: [0.85761565 0.13200214 0.01038221]\n","Logit 1692: [0.0778456  0.02317461 0.89897979]\n","Logit 1693: [0.19048596 0.78320685 0.02630719]\n","Logit 1694: [0.58196494 0.40272186 0.0153132 ]\n","Logit 1695: [0.80219109 0.18749151 0.01031739]\n","Logit 1696: [0.05749755 0.03824556 0.90425688]\n","Logit 1697: [0.07942095 0.90845323 0.01212582]\n","Logit 1698: [0.10496854 0.87186501 0.02316645]\n","Logit 1699: [0.60359523 0.3747808  0.02162397]\n","Logit 1700: [0.96376712 0.01927368 0.0169592 ]\n","Logit 1701: [0.06682682 0.92094056 0.01223262]\n","Logit 1702: [0.06617749 0.92161672 0.01220579]\n","Logit 1703: [0.25502428 0.71036067 0.03461505]\n","Logit 1704: [0.08083466 0.89850388 0.02066146]\n","Logit 1705: [0.04269821 0.93412116 0.02318063]\n","Logit 1706: [0.06003869 0.92867487 0.01128644]\n","Logit 1707: [0.53062717 0.46056688 0.00880594]\n","Logit 1708: [0.47725882 0.49869977 0.02404141]\n","Logit 1709: [0.89306546 0.09444204 0.0124925 ]\n","Logit 1710: [0.93975836 0.0400002  0.02024144]\n","Logit 1711: [0.06362016 0.9260321  0.01034774]\n","Logit 1712: [0.62826297 0.35036464 0.02137238]\n","Logit 1713: [0.06561594 0.92208825 0.0122958 ]\n","Logit 1714: [0.53634474 0.44900891 0.01464635]\n","Logit 1715: [0.70136131 0.2895296  0.00910909]\n","Logit 1716: [0.96133186 0.02425523 0.01441291]\n","Logit 1717: [0.03816951 0.94808046 0.01375003]\n","Logit 1718: [0.1582887  0.82674535 0.01496596]\n","Logit 1719: [0.93012586 0.06082647 0.00904767]\n","Logit 1720: [0.5628089  0.42649318 0.01069792]\n","Logit 1721: [0.69937953 0.29012467 0.0104958 ]\n","Logit 1722: [0.63862317 0.35096333 0.01041351]\n","Logit 1723: [0.04289309 0.94021927 0.01688764]\n","Logit 1724: [0.96486827 0.02741133 0.0077204 ]\n","Logit 1725: [0.9041572  0.08239258 0.01345022]\n","Logit 1726: [0.12845069 0.01745142 0.85409789]\n","Logit 1727: [0.5310469  0.45378512 0.01516797]\n","Logit 1728: [0.75752173 0.22277789 0.01970038]\n","Logit 1729: [0.89569545 0.09204157 0.01226298]\n","Logit 1730: [0.02294379 0.9637579  0.01329831]\n","Logit 1731: [0.96097713 0.02479703 0.01422584]\n","Logit 1732: [0.47912305 0.02315411 0.49772284]\n","Logit 1733: [0.03713779 0.94848828 0.01437393]\n","Logit 1734: [0.03791906 0.94277986 0.01930108]\n","Logit 1735: [0.45804466 0.52157314 0.0203822 ]\n","Logit 1736: [0.91190785 0.07538581 0.01270634]\n","Logit 1737: [0.03712472 0.94630447 0.01657081]\n","Logit 1738: [0.86262156 0.12737485 0.0100036 ]\n","Logit 1739: [0.06663968 0.90721922 0.02614111]\n","Logit 1740: [0.96136154 0.02666601 0.01197245]\n","Logit 1741: [0.10078896 0.87676312 0.02244792]\n","Logit 1742: [0.03428957 0.9515415  0.01416894]\n","Logit 1743: [0.06021811 0.02008209 0.9196998 ]\n","Logit 1744: [0.04458951 0.93812711 0.01728337]\n","Logit 1745: [0.11061299 0.37916488 0.51022212]\n","Logit 1746: [0.56176697 0.3919851  0.04624793]\n","Logit 1747: [0.41854448 0.56596995 0.01548556]\n","Logit 1748: [0.04786754 0.02661629 0.92551617]\n","Logit 1749: [0.37165431 0.61066357 0.01768212]\n","Logit 1750: [0.14763094 0.83475427 0.01761479]\n","Logit 1751: [0.07144871 0.91576623 0.01278505]\n","Logit 1752: [0.09314499 0.89633789 0.01051713]\n","Logit 1753: [0.12027358 0.01741399 0.86231243]\n","Logit 1754: [0.84048685 0.12984034 0.02967281]\n","Logit 1755: [0.09678954 0.89125596 0.01195451]\n","Logit 1756: [0.19273446 0.79187166 0.01539387]\n","Logit 1757: [0.21061275 0.77416029 0.01522697]\n","Logit 1758: [0.06268928 0.02237439 0.91493633]\n","Logit 1759: [0.92084519 0.05225582 0.02689899]\n","Logit 1760: [0.04321369 0.94586018 0.01092613]\n","Logit 1761: [0.86153339 0.1232024  0.01526421]\n","Logit 1762: [0.96224175 0.02720356 0.01055469]\n","Logit 1763: [0.81650252 0.17499314 0.00850434]\n","Logit 1764: [0.05282793 0.9291103  0.01806177]\n","Logit 1765: [0.65972828 0.33011638 0.01015534]\n","Logit 1766: [0.25858072 0.72875727 0.01266201]\n","Logit 1767: [0.77047706 0.21032838 0.01919456]\n","Logit 1768: [0.91902875 0.06094297 0.02002829]\n","Logit 1769: [0.82538334 0.16524835 0.00936831]\n","Logit 1770: [0.96136154 0.02666601 0.01197245]\n","Logit 1771: [0.17281122 0.79994935 0.02723943]\n","Logit 1772: [0.30690062 0.68422157 0.00887781]\n","Logit 1773: [0.33919549 0.64085747 0.01994704]\n","Logit 1774: [0.71499828 0.27687044 0.00813127]\n","Logit 1775: [0.07942371 0.90133238 0.01924391]\n","Logit 1776: [0.94842093 0.04140703 0.01017203]\n","Logit 1777: [0.71434214 0.27729252 0.00836534]\n","Logit 1778: [0.49133501 0.48089157 0.02777342]\n","Logit 1779: [0.93539817 0.02148886 0.04311297]\n","Logit 1780: [0.15491038 0.82525988 0.01982974]\n","Logit 1781: [0.07621404 0.91196472 0.01182124]\n","Logit 1782: [0.03585625 0.94991589 0.01422785]\n","Logit 1783: [0.59977377 0.38875868 0.01146755]\n","Logit 1784: [0.08441582 0.90479016 0.01079402]\n","Logit 1785: [0.49550027 0.49501662 0.00948312]\n","Logit 1786: [0.50686101 0.03366042 0.45947857]\n","Logit 1787: [0.17267094 0.81337511 0.01395395]\n","Logit 1788: [0.47709625 0.51134995 0.01155381]\n","Logit 1789: [0.06080359 0.92456978 0.01462663]\n","Logit 1790: [0.87181461 0.12090328 0.00728211]\n","Logit 1791: [0.2712145  0.03367408 0.69511142]\n","Logit 1792: [0.93206751 0.05276369 0.01516881]\n","Logit 1793: [0.15451861 0.83654424 0.00893715]\n","Logit 1794: [0.07103473 0.91380044 0.01516484]\n","Logit 1795: [0.07864328 0.90741592 0.0139408 ]\n","Logit 1796: [0.81999029 0.1642087  0.01580101]\n","Logit 1797: [0.83039664 0.16192574 0.00767762]\n","Logit 1798: [0.37762582 0.60344434 0.01892984]\n","Logit 1799: [0.72097055 0.02803708 0.25099236]\n","Logit 1800: [0.5434583  0.44791058 0.00863112]\n","Logit 1801: [0.25890415 0.73288239 0.00821346]\n","Logit 1802: [0.08409554 0.90207279 0.01383167]\n","Logit 1803: [0.50980811 0.4812646  0.00892729]\n","Logit 1804: [0.41764144 0.57196419 0.01039437]\n","Logit 1805: [0.06008427 0.93199291 0.00792282]\n","Logit 1806: [0.92294688 0.06861922 0.0084339 ]\n","Logit 1807: [0.73472183 0.25391307 0.01136509]\n","Logit 1808: [0.06671935 0.92125705 0.01202361]\n","Logit 1809: [0.7038421  0.23810023 0.05805767]\n","Logit 1810: [0.52669775 0.44700258 0.02629967]\n","Logit 1811: [0.52303069 0.04187704 0.43509227]\n","Logit 1812: [0.02960337 0.01930094 0.95109569]\n","Logit 1813: [0.03276567 0.01991224 0.94732209]\n","Logit 1814: [0.82254077 0.01776868 0.15969055]\n","Logit 1815: [0.4307896  0.55966509 0.00954531]\n","Logit 1816: [0.35521402 0.63261935 0.01216663]\n","Logit 1817: [0.95419828 0.02532939 0.02047234]\n","Logit 1818: [0.89501916 0.09162749 0.01335335]\n","Logit 1819: [0.9615273  0.02646305 0.01200965]\n","Logit 1820: [0.05730138 0.90691029 0.03578832]\n","Logit 1821: [0.86856094 0.11731758 0.01412148]\n","Logit 1822: [0.94943649 0.04272544 0.00783808]\n","Logit 1823: [0.7038421  0.23810023 0.05805767]\n","Logit 1824: [0.03436651 0.9518158  0.01381769]\n","Logit 1825: [0.08353579 0.89568573 0.02077848]\n","Logit 1826: [0.04651076 0.937392   0.01609725]\n","Logit 1827: [0.84364233 0.14678212 0.00957555]\n","Logit 1828: [0.37762582 0.60344434 0.01892984]\n","Logit 1829: [0.19980545 0.78486225 0.01533229]\n","Logit 1830: [0.75979999 0.22154669 0.01865332]\n","Logit 1831: [0.34124761 0.65010872 0.00864367]\n","Logit 1832: [0.91976098 0.0604283  0.01981072]\n","Logit 1833: [0.88964831 0.09715602 0.01319567]\n","Logit 1834: [0.96239411 0.02478514 0.01282075]\n","Logit 1835: [0.29596677 0.68546054 0.01857269]\n","Logit 1836: [0.31868647 0.66647373 0.01483981]\n","Logit 1837: [0.03504595 0.95325519 0.01169887]\n","Logit 1838: [0.10189295 0.88010664 0.01800041]\n","Logit 1839: [0.09314499 0.89633789 0.01051713]\n","Logit 1840: [0.37762582 0.60344434 0.01892984]\n","Logit 1841: [0.11636924 0.86935762 0.01427314]\n","Logit 1842: [0.45484137 0.02189498 0.52326365]\n","Logit 1843: [0.11288428 0.87349798 0.01361774]\n","Logit 1844: [0.28777676 0.02574825 0.68647499]\n","Logit 1845: [0.41702046 0.02943712 0.55354243]\n","Logit 1846: [0.06594161 0.91609247 0.01796592]\n","Logit 1847: [0.28225315 0.70337509 0.01437176]\n","Logit 1848: [0.76653945 0.22460585 0.0088547 ]\n","Logit 1849: [0.91687339 0.06738171 0.0157449 ]\n","Logit 1850: [0.40863696 0.17558102 0.41578202]\n","Logit 1851: [0.18213594 0.80597925 0.01188481]\n","Logit 1852: [0.19528208 0.02981252 0.7749054 ]\n","Logit 1853: [0.92100588 0.03491811 0.04407601]\n","Logit 1854: [0.43245729 0.55042709 0.01711562]\n","Logit 1855: [0.94716774 0.04155473 0.01127753]\n","Logit 1856: [0.76177726 0.22496113 0.0132616 ]\n","Logit 1857: [0.08583009 0.90281623 0.01135369]\n","Logit 1858: [0.40306396 0.58132281 0.01561323]\n","Logit 1859: [0.62270425 0.36127451 0.01602124]\n","Logit 1860: [0.36869385 0.61865313 0.01265302]\n","Logit 1861: [0.36664594 0.62064784 0.01270622]\n","Logit 1862: [0.32640658 0.66258682 0.0110066 ]\n","Logit 1863: [0.5841527  0.39448626 0.02136104]\n","Logit 1864: [0.96849405 0.02079936 0.01070659]\n","Logit 1865: [0.94650493 0.04076226 0.01273281]\n","Logit 1866: [0.74856732 0.23878964 0.01264303]\n","Logit 1867: [0.93322909 0.05850542 0.00826548]\n","Logit 1868: [0.83261604 0.15298789 0.01439607]\n","Logit 1869: [0.15542403 0.83184599 0.01272998]\n","Logit 1870: [0.86519638 0.03827483 0.09652879]\n","Logit 1871: [0.08248968 0.90422873 0.01328159]\n","Logit 1872: [0.09314499 0.89633789 0.01051713]\n","Logit 1873: [0.03565052 0.95233788 0.0120116 ]\n","Logit 1874: [0.2698347  0.70573183 0.02443347]\n","Logit 1875: [0.7038421  0.23810023 0.05805767]\n","Logit 1876: [0.79462642 0.04851988 0.1568537 ]\n","Logit 1877: [0.89617939 0.06930869 0.03451192]\n","Logit 1878: [0.45087059 0.04343809 0.50569132]\n","Logit 1879: [0.80864679 0.0422335  0.14911971]\n","Logit 1880: [0.95061676 0.03921749 0.01016576]\n","Logit 1881: [0.07046543 0.9195655  0.00996907]\n","Logit 1882: [0.13367864 0.65542406 0.21089731]\n","Logit 1883: [0.85929934 0.12298797 0.01771269]\n","Logit 1884: [0.94236962 0.04767994 0.00995044]\n","Logit 1885: [0.92859427 0.05167634 0.01972939]\n","Logit 1886: [0.29817214 0.68252282 0.01930504]\n","Logit 1887: [0.19619896 0.76825328 0.03554776]\n","Logit 1888: [0.92014438 0.06926798 0.01058764]\n","Logit 1889: [0.96082843 0.01934671 0.01982486]\n","Logit 1890: [0.71249795 0.27779474 0.0097073 ]\n","Logit 1891: [0.61908408 0.03919211 0.3417238 ]\n","Logit 1892: [0.76133517 0.22461112 0.01405371]\n","Logit 1893: [0.27104696 0.71971448 0.00923856]\n","Logit 1894: [0.02950843 0.95594789 0.01454368]\n","Logit 1895: [0.05166843 0.93117625 0.01715531]\n","Logit 1896: [0.03462484 0.95150787 0.01386728]\n","Logit 1897: [0.07218497 0.9135314  0.01428364]\n","Logit 1898: [0.84458798 0.1352167  0.02019531]\n","Logit 1899: [0.21232644 0.02174884 0.76592472]\n","Logit 1900: [0.14355762 0.02007697 0.83636541]\n","Logit 1901: [0.2833306  0.70227795 0.01439145]\n","Logit 1902: [0.11525701 0.87130459 0.0134384 ]\n","Logit 1903: [0.56228319 0.42113065 0.01658615]\n","Logit 1904: [0.42011609 0.55061668 0.02926723]\n","Logit 1905: [0.03983999 0.94934256 0.01081745]\n","Logit 1906: [0.42277135 0.55247579 0.02475286]\n","Logit 1907: [0.6521301  0.32583673 0.02203316]\n","Logit 1908: [0.25389943 0.7343237  0.01177687]\n","Logit 1909: [0.11731303 0.87171371 0.01097326]\n","Logit 1910: [0.91274202 0.07928372 0.00797426]\n","Logit 1911: [0.50770467 0.46114315 0.03115218]\n","Logit 1912: [0.91839779 0.07109641 0.0105058 ]\n","Logit 1913: [0.6521301  0.32583673 0.02203316]\n","Logit 1914: [0.32497937 0.6661635  0.00885713]\n","Logit 1915: [0.0923783  0.89477093 0.01285077]\n","Logit 1916: [0.07446587 0.91095303 0.01458109]\n","Logit 1917: [0.91749719 0.07260461 0.0098982 ]\n","Logit 1918: [0.6521301  0.32583673 0.02203316]\n","Logit 1919: [0.09017248 0.02756827 0.88225925]\n","Logit 1920: [0.08607003 0.01398894 0.89994103]\n","Logit 1921: [0.76811973 0.2152872  0.01659306]\n","Logit 1922: [0.7868051  0.19122099 0.0219739 ]\n","Logit 1923: [0.02442469 0.96145319 0.01412213]\n","Logit 1924: [0.17184347 0.02488061 0.80327592]\n","Logit 1925: [0.09886588 0.01715098 0.88398314]\n","Logit 1926: [0.93652614 0.05166169 0.01181217]\n","Logit 1927: [0.60490858 0.02944772 0.3656437 ]\n","Logit 1928: [0.06047819 0.92785326 0.01166855]\n","Logit 1929: [0.2088015  0.77503444 0.01616406]\n","Logit 1930: [0.90517054 0.08267646 0.01215301]\n","Logit 1931: [0.94805601 0.03963089 0.0123131 ]\n","Logit 1932: [0.95228904 0.02572699 0.02198398]\n","Logit 1933: [0.93932761 0.0323625  0.02830989]\n","Logit 1934: [0.91041531 0.07190362 0.01768107]\n","Logit 1935: [0.95944797 0.03205433 0.0084977 ]\n","Logit 1936: [0.95059804 0.03331531 0.01608665]\n","Logit 1937: [0.94336048 0.04005561 0.01658391]\n","Logit 1938: [0.87613411 0.11495185 0.00891405]\n","Logit 1939: [0.20805546 0.75511282 0.03683172]\n","Logit 1940: [0.79002901 0.20072835 0.00924264]\n","Logit 1941: [0.80513702 0.18544359 0.00941939]\n","Logit 1942: [0.77823808 0.21213605 0.00962587]\n","Logit 1943: [0.79965565 0.19049213 0.00985222]\n","Logit 1944: [0.85824834 0.1330706  0.00868106]\n","Logit 1945: [0.85538943 0.13723885 0.00737172]\n","Logit 1946: [0.81206622 0.18004993 0.00788385]\n","Logit 1947: [0.09706966 0.80015373 0.10277661]\n","Logit 1948: [0.9262809  0.02105205 0.05266705]\n","Logit 1949: [0.51215376 0.03254961 0.45529662]\n","Logit 1950: [0.87067627 0.01619812 0.11312561]\n","Logit 1951: [0.94353415 0.03416744 0.02229841]\n","Logit 1952: [0.71881459 0.23536756 0.04581785]\n","Logit 1953: [0.06545184 0.92169292 0.01285524]\n","Logit 1954: [0.77928829 0.21138763 0.00932408]\n","Logit 1955: [0.54643009 0.03908392 0.41448599]\n","Logit 1956: [0.47907924 0.50207073 0.01885003]\n","Logit 1957: [0.07046543 0.9195655  0.00996907]\n","Logit 1958: [0.11678933 0.87047471 0.01273596]\n","Logit 1959: [0.70003943 0.28743592 0.01252466]\n","Logit 1960: [0.36709641 0.62141037 0.01149322]\n","Logit 1961: [0.93823531 0.04292816 0.01883652]\n","Logit 1962: [0.70590707 0.28563034 0.00846259]\n","Logit 1963: [0.42502108 0.03665153 0.53832739]\n","Logit 1964: [0.90162049 0.07152264 0.02685687]\n","Logit 1965: [0.67977343 0.29798779 0.02223878]\n","Logit 1966: [0.7199002  0.2711169  0.00898291]\n","Logit 1967: [0.68076353 0.30789474 0.01134173]\n","Logit 1968: [0.26096692 0.72160904 0.01742404]\n","Logit 1969: [0.7858965  0.20662129 0.00748221]\n","Logit 1970: [0.71032305 0.24291748 0.04675947]\n","Logit 1971: [0.78973463 0.03534835 0.17491703]\n","Logit 1972: [0.96072409 0.01257549 0.02670042]\n","Logit 1973: [0.9368936  0.05351815 0.00958825]\n","Logit 1974: [0.75012945 0.24175422 0.00811633]\n","Logit 1975: [0.42962096 0.03046014 0.5399189 ]\n","Logit 1976: [0.57683661 0.41305249 0.0101109 ]\n","Logit 1977: [0.59242265 0.39851133 0.00906602]\n","Logit 1978: [0.78372704 0.04438791 0.17188506]\n","Logit 1979: [0.17582258 0.02075183 0.80342559]\n","Logit 1980: [0.94723696 0.04353091 0.00923213]\n","Logit 1981: [0.53469523 0.45512136 0.0101834 ]\n","Logit 1982: [0.89900916 0.09088602 0.01010481]\n","Logit 1983: [0.91190253 0.07459811 0.01349936]\n","Logit 1984: [0.09732256 0.01943724 0.8832402 ]\n","Logit 1985: [0.04982949 0.93654789 0.01362262]\n","Logit 1986: [0.40597407 0.5770055  0.01702043]\n","Logit 1987: [0.53509308 0.44665061 0.01825631]\n","Logit 1988: [0.94506441 0.04176725 0.01316834]\n","Logit 1989: [0.87301436 0.10113279 0.02585285]\n","Logit 1990: [0.17259993 0.11738725 0.71001282]\n","Logit 1991: [0.95736394 0.03446571 0.00817035]\n","Logit 1992: [0.81078865 0.17880372 0.01040762]\n","Logit 1993: [0.94938179 0.03645379 0.01416442]\n","Logit 1994: [0.90892023 0.08249364 0.00858613]\n","Logit 1995: [0.87682866 0.11268021 0.01049113]\n","Logit 1996: [0.97162296 0.01883296 0.00954408]\n","Logit 1997: [0.71032305 0.24291748 0.04675947]\n","Logit 1998: [0.75921589 0.23019501 0.0105891 ]\n","Logit 1999: [0.91871699 0.07480832 0.00647469]\n","Logit 2000: [0.18624277 0.80035776 0.01339947]\n","Logit 2001: [0.14765853 0.83572603 0.01661544]\n","Logit 2002: [0.15332941 0.02380335 0.82286725]\n","Logit 2003: [0.9073712  0.08102681 0.01160199]\n","Logit 2004: [0.86289995 0.12854076 0.00855929]\n","Logit 2005: [0.76590581 0.203988   0.0301062 ]\n","Logit 2006: [0.36882351 0.61766315 0.01351334]\n","Logit 2007: [0.78558487 0.20613636 0.00827877]\n","Logit 2008: [0.03497934 0.95470096 0.0103197 ]\n","Logit 2009: [0.25909807 0.71889437 0.02200757]\n","Logit 2010: [0.21167701 0.76750692 0.02081608]\n","Logit 2011: [0.92706033 0.04757427 0.02536539]\n","Logit 2012: [0.90354679 0.0352058  0.06124742]\n","Logit 2013: [0.71285924 0.2093555  0.07778526]\n","Logit 2014: [0.42841123 0.5522439  0.01934487]\n","Logit 2015: [0.88271543 0.09109334 0.02619123]\n","Logit 2016: [0.71331118 0.02657255 0.26011627]\n","Logit 2017: [0.03259875 0.95081634 0.0165849 ]\n","Logit 2018: [0.17780984 0.78895153 0.03323863]\n","Logit 2019: [0.72444766 0.2663793  0.00917304]\n","Logit 2020: [0.17706407 0.81236453 0.0105714 ]\n","Logit 2021: [0.16361571 0.82726571 0.00911857]\n","Logit 2022: [0.87743946 0.11098513 0.01157541]\n","Logit 2023: [0.77884827 0.20617281 0.01497893]\n","Logit 2024: [0.103147   0.88422784 0.01262516]\n","Logit 2025: [0.87750969 0.112723   0.00976731]\n","Logit 2026: [0.17313967 0.79766323 0.0291971 ]\n","Logit 2027: [0.05603753 0.9299248  0.01403767]\n","Logit 2028: [0.61687877 0.3592955  0.02382573]\n","Logit 2029: [0.74332991 0.24752971 0.00914038]\n","Logit 2030: [0.02069473 0.96550614 0.01379913]\n","Logit 2031: [0.02025093 0.96531873 0.01443035]\n","Logit 2032: [0.11207239 0.87380388 0.01412372]\n","Logit 2033: [0.91232148 0.07671999 0.01095852]\n","Logit 2034: [0.5123656  0.03657592 0.45105849]\n","Logit 2035: [0.78301883 0.2009977  0.01598347]\n","Logit 2036: [0.22293488 0.76083672 0.0162284 ]\n","Logit 2037: [0.21703361 0.77170852 0.01125787]\n","Logit 2038: [0.03939398 0.01683593 0.94377009]\n","Logit 2039: [0.03747262 0.0189439  0.94358348]\n","Logit 2040: [0.06205823 0.81506543 0.12287635]\n","Logit 2041: [0.72893262 0.25475741 0.01630997]\n","Logit 2042: [0.02983625 0.95251224 0.01765152]\n","Logit 2043: [0.03411559 0.9448664  0.02101801]\n","Logit 2044: [0.7213871  0.04121784 0.23739506]\n","Logit 2045: [0.73151086 0.25640859 0.01208055]\n","Logit 2046: [0.38697444 0.59614716 0.0168784 ]\n","Logit 2047: [0.11402844 0.87591254 0.01005902]\n","Logit 2048: [0.90793939 0.0839402  0.00812041]\n","Logit 2049: [0.96580441 0.01791533 0.01628026]\n","Logit 2050: [0.20046056 0.77693278 0.02260666]\n","Logit 2051: [0.82228953 0.16808101 0.00962946]\n","Logit 2052: [0.44219181 0.04410477 0.51370342]\n","Logit 2053: [0.45343703 0.53584696 0.01071601]\n","Logit 2054: [0.77293025 0.218228   0.00884176]\n","Logit 2055: [0.89577287 0.09655249 0.00767464]\n","Logit 2056: [0.02182579 0.96644026 0.01173395]\n","Logit 2057: [0.71032305 0.24291748 0.04675947]\n","Logit 2058: [0.36180295 0.62391965 0.0142774 ]\n","Logit 2059: [0.93898768 0.02534288 0.03566945]\n","Logit 2060: [0.11989659 0.85437888 0.02572453]\n","Logit 2061: [0.91617036 0.07318428 0.01064536]\n","Logit 2062: [0.86165844 0.12846554 0.00987601]\n","Logit 2063: [0.86434671 0.12580434 0.00984895]\n","Logit 2064: [0.89360102 0.09886164 0.00753734]\n","Logit 2065: [0.71032305 0.24291748 0.04675947]\n","Logit 2066: [0.0257409  0.95653308 0.01772603]\n","Logit 2067: [0.92544416 0.06535822 0.00919763]\n","Logit 2068: [0.93427892 0.0586571  0.00706399]\n","Logit 2069: [0.91066587 0.07764444 0.01168969]\n","Logit 2070: [0.92255648 0.0656012  0.01184232]\n","Logit 2071: [0.95377864 0.0374545  0.00876686]\n","Logit 2072: [0.92702653 0.06407839 0.00889508]\n","Logit 2073: [0.04782513 0.94178387 0.010391  ]\n","Logit 2074: [0.29974354 0.68344488 0.01681158]\n","Logit 2075: [0.28654342 0.69616408 0.0172925 ]\n","Logit 2076: [0.83104795 0.15725876 0.0116933 ]\n","Logit 2077: [0.56439496 0.42147569 0.01412935]\n","Logit 2078: [0.02848901 0.96015669 0.0113543 ]\n","Logit 2079: [0.29349468 0.02210761 0.68439771]\n","Logit 2080: [0.63300432 0.01979904 0.34719664]\n","Logit 2081: [0.30910607 0.02523706 0.66565687]\n","Logit 2082: [0.40411742 0.58455221 0.01133037]\n","Logit 2083: [0.05411483 0.01800271 0.92788246]\n","Logit 2084: [0.8209519  0.16920632 0.00984178]\n","Logit 2085: [0.53761533 0.44613423 0.01625044]\n","Logit 2086: [0.07091851 0.91587612 0.01320537]\n","Logit 2087: [0.94808845 0.04323097 0.00868058]\n","Logit 2088: [0.0394919 0.9479656 0.0125425]\n","Logit 2089: [0.95616878 0.03336361 0.01046761]\n","Logit 2090: [0.49339751 0.03094688 0.4756556 ]\n","Logit 2091: [0.96125003 0.02487685 0.01387312]\n","Logit 2092: [0.94346816 0.04855833 0.00797351]\n","Logit 2093: [0.85770302 0.07151326 0.07078372]\n","Logit 2094: [0.94291741 0.05026646 0.00681613]\n","Logit 2095: [0.10739992 0.01750048 0.8750996 ]\n","Logit 2096: [0.91349843 0.07783842 0.00866315]\n","Logit 2097: [0.93917687 0.04104351 0.01977962]\n","Logit 2098: [0.93917687 0.04104351 0.01977962]\n","Logit 2099: [0.57955837 0.40737222 0.01306941]\n","Logit 2100: [0.48525192 0.49675934 0.01798875]\n","Logit 2101: [0.06783498 0.92332277 0.00884225]\n","Logit 2102: [0.93909885 0.04531648 0.01558467]\n","Logit 2103: [0.35145137 0.08788991 0.56065872]\n","Logit 2104: [0.12365566 0.01988783 0.8564565 ]\n","Logit 2105: [0.92848447 0.05702642 0.01448911]\n","Logit 2106: [0.93699536 0.02408395 0.03892069]\n","Logit 2107: [0.58378533 0.10735853 0.30885614]\n","Logit 2108: [0.49339751 0.03094688 0.4756556 ]\n","Logit 2109: [0.06783498 0.92332277 0.00884225]\n","Logit 2110: [0.52217659 0.46036942 0.01745399]\n","Logit 2111: [0.96814763 0.02085292 0.01099945]\n","Logit 2112: [0.20634512 0.02979222 0.76386266]\n","Logit 2113: [0.38016901 0.60988607 0.00994491]\n","Logit 2114: [0.90159847 0.03988519 0.05851634]\n","Logit 2115: [0.45666745 0.51873692 0.02459563]\n","Logit 2116: [0.72365646 0.26660859 0.00973496]\n","Logit 2117: [0.10238932 0.01998523 0.87762545]\n","Logit 2118: [0.10061145 0.8823526  0.01703596]\n","Logit 2119: [0.63559575 0.35480024 0.009604  ]\n","Logit 2120: [0.83378564 0.10392827 0.06228609]\n","Logit 2121: [0.51526055 0.43900748 0.04573197]\n","Logit 2122: [0.90572411 0.08590181 0.00837408]\n","Logit 2123: [0.35691664 0.62610241 0.01698095]\n","Logit 2124: [0.3274263 0.6562721 0.0163016]\n","Logit 2125: [0.90625959 0.08617059 0.00756982]\n","Logit 2126: [0.0373888  0.01883703 0.94377417]\n","Logit 2127: [0.1299686  0.01848529 0.8515461 ]\n","Logit 2128: [0.16754089 0.01909125 0.81336786]\n","Logit 2129: [0.80191198 0.18788442 0.0102036 ]\n","Logit 2130: [0.35650066 0.63243553 0.01106381]\n","Logit 2131: [0.9441938  0.03544935 0.02035685]\n","Logit 2132: [0.50038286 0.48783776 0.01177937]\n","Logit 2133: [0.95130855 0.03594391 0.01274754]\n","Logit 2134: [0.07571004 0.02111651 0.90317345]\n","Logit 2135: [0.12700652 0.01945569 0.85353778]\n","Logit 2136: [0.74538191 0.05253885 0.20207924]\n","Logit 2137: [0.74289993 0.06303969 0.19406039]\n","Logit 2138: [0.36107102 0.62448432 0.01444466]\n","Logit 2139: [0.06102263 0.92642914 0.01254823]\n","Logit 2140: [0.90364702 0.08744188 0.0089111 ]\n","Logit 2141: [0.05404447 0.91856713 0.0273884 ]\n","Logit 2142: [0.82657215 0.16292073 0.01050712]\n","Logit 2143: [0.54849711 0.43815496 0.01334794]\n","Logit 2144: [0.23367891 0.75616862 0.01015247]\n","Logit 2145: [0.96790878 0.02088853 0.01120268]\n","Logit 2146: [0.03531108 0.9534578  0.01123112]\n","Logit 2147: [0.8282451  0.16340998 0.00834492]\n","Logit 2148: [0.19451514 0.79180773 0.01367713]\n","Logit 2149: [0.06038955 0.91670595 0.02290451]\n","Logit 2150: [0.07367366 0.90082185 0.02550449]\n","Logit 2151: [0.20987686 0.77636785 0.01375529]\n","Logit 2152: [0.60177392 0.38701969 0.01120638]\n","Logit 2153: [0.08180249 0.90524697 0.01295054]\n","Logit 2154: [0.87288383 0.11671295 0.01040322]\n","Logit 2155: [0.13071454 0.85611988 0.01316558]\n","Logit 2156: [0.90320225 0.08347931 0.01331844]\n","Logit 2157: [0.08180249 0.90524697 0.01295054]\n","Logit 2158: [0.01957982 0.96672454 0.01369564]\n","Logit 2159: [0.05100216 0.93867148 0.01032636]\n","Logit 2160: [0.0289886  0.95809744 0.01291396]\n","Logit 2161: [0.89244926 0.08904662 0.01850411]\n","Logit 2162: [0.13890977 0.0191538  0.84193642]\n","Logit 2163: [0.93179663 0.05828741 0.00991596]\n","Logit 2164: [0.55487545 0.42793788 0.01718667]\n","Logit 2165: [0.29475661 0.69510306 0.01014034]\n","Logit 2166: [0.46194901 0.52859373 0.00945726]\n","Logit 2167: [0.76464317 0.22459789 0.01075894]\n","Logit 2168: [0.16492495 0.82457286 0.01050219]\n","Logit 2169: [0.05788028 0.9302713  0.01184843]\n","Logit 2170: [0.86991942 0.12248044 0.00760014]\n","Logit 2171: [0.90419103 0.08757867 0.0082303 ]\n","Logit 2172: [0.09537055 0.89354494 0.01108451]\n","Logit 2173: [0.22652148 0.76257964 0.01089888]\n","Logit 2174: [0.85710698 0.13544857 0.00744445]\n","Logit 2175: [0.04614016 0.94134399 0.01251585]\n","Logit 2176: [0.10146805 0.01855103 0.87998092]\n","Logit 2177: [0.07567685 0.02044288 0.90388027]\n","Logit 2178: [0.64626383 0.34055711 0.01317907]\n","Logit 2179: [0.21777793 0.10002295 0.68219912]\n","Logit 2180: [0.62664861 0.35985402 0.01349738]\n","Logit 2181: [0.82672426 0.16307011 0.01020563]\n","Logit 2182: [0.71345426 0.27119366 0.01535208]\n","Logit 2183: [0.03384303 0.95254311 0.01361386]\n","Logit 2184: [0.9596036  0.02605149 0.01434491]\n","Logit 2185: [0.91368016 0.04069692 0.04562292]\n","Logit 2186: [0.91935773 0.06867856 0.01196372]\n","Logit 2187: [0.76353308 0.06146238 0.17500454]\n","Logit 2188: [0.73998136 0.25041734 0.0096013 ]\n","Logit 2189: [0.81541555 0.17587303 0.00871142]\n","Logit 2190: [0.92311296 0.06624448 0.01064256]\n","Logit 2191: [0.78214585 0.03000305 0.1878511 ]\n","Logit 2192: [0.6918794  0.29976112 0.00835948]\n","Logit 2193: [0.95034214 0.0380742  0.01158366]\n","Logit 2194: [0.63302473 0.35613653 0.01083874]\n","Logit 2195: [0.94736263 0.04581581 0.00682156]\n","Logit 2196: [0.92098903 0.06809855 0.01091242]\n","Logit 2197: [0.37441843 0.61550574 0.01007583]\n","Logit 2198: [0.39237296 0.59888187 0.00874517]\n","Logit 2199: [0.17122152 0.81804961 0.01072887]\n","Logit 2200: [0.04561694 0.93911448 0.01526857]\n","Logit 2201: [0.53625367 0.45289577 0.01085056]\n","Logit 2202: [0.0780156  0.01972062 0.90226378]\n","Logit 2203: [0.91358517 0.07926015 0.00715468]\n","Logit 2204: [0.30002347 0.6897852  0.01019132]\n","Logit 2205: [0.71957886 0.16695572 0.11346542]\n","Logit 2206: [0.46557904 0.52500043 0.00942053]\n","Logit 2207: [0.89196    0.09951477 0.00852524]\n","Logit 2208: [0.37122155 0.61925542 0.00952303]\n","Logit 2209: [0.77391617 0.21680593 0.0092779 ]\n","Logit 2210: [0.56501831 0.42525052 0.00973118]\n","Logit 2211: [0.89869154 0.09393265 0.00737581]\n","Logit 2212: [0.39951472 0.59044448 0.0100408 ]\n","Logit 2213: [0.96993805 0.02269965 0.0073623 ]\n","Logit 2214: [0.69635983 0.2945695  0.00907067]\n","Logit 2215: [0.85024873 0.14164056 0.00811071]\n","Logit 2216: [0.57933191 0.39314568 0.02752241]\n","Logit 2217: [0.04782302 0.93190765 0.02026933]\n","Logit 2218: [0.67196862 0.31881346 0.00921792]\n","Logit 2219: [0.70314275 0.28772502 0.00913223]\n","Logit 2220: [0.02110257 0.96454883 0.0143486 ]\n","Logit 2221: [0.27199807 0.71907305 0.00892888]\n","Logit 2222: [0.82757342 0.1363485  0.03607807]\n","Logit 2223: [0.87226631 0.11658056 0.01115313]\n","Logit 2224: [0.07465275 0.91309975 0.0122475 ]\n","Logit 2225: [0.14402794 0.84352151 0.01245055]\n","Logit 2226: [0.3794269  0.61048144 0.01009165]\n","Logit 2227: [0.82527525 0.15045211 0.02427264]\n","Logit 2228: [0.08666704 0.89729481 0.01603815]\n","Logit 2229: [0.58141382 0.40708314 0.01150304]\n","Logit 2230: [0.83713703 0.15053076 0.01233221]\n","Logit 2231: [0.12073667 0.8677485  0.01151484]\n","Logit 2232: [0.89094362 0.09890437 0.01015201]\n","Logit 2233: [0.57933191 0.39314568 0.02752241]\n","Logit 2234: [0.08625134 0.89981619 0.01393247]\n","Logit 2235: [0.08487999 0.90417417 0.01094584]\n","Logit 2236: [0.956691   0.02463827 0.01867073]\n","Logit 2237: [0.71189096 0.27634102 0.01176802]\n","Logit 2238: [0.43364637 0.55086358 0.01549005]\n","Logit 2239: [0.12391482 0.8637167  0.01236848]\n","Logit 2240: [0.85525951 0.13588451 0.00885598]\n","Logit 2241: [0.33899835 0.65152485 0.0094768 ]\n","Logit 2242: [0.14366639 0.84572896 0.01060465]\n","Logit 2243: [0.87790024 0.11519763 0.00690213]\n","Logit 2244: [0.84654247 0.14610484 0.00735269]\n","Logit 2245: [0.86351076 0.1184975  0.01799174]\n","Logit 2246: [0.91179508 0.06195761 0.02624732]\n","Logit 2247: [0.81780406 0.1468212  0.03537473]\n","Logit 2248: [0.36539213 0.06354215 0.57106572]\n","Logit 2249: [0.57601919 0.0428834  0.38109742]\n","Logit 2250: [0.51269415 0.04662881 0.44067704]\n","Logit 2251: [0.73681648 0.25007789 0.01310563]\n","Logit 2252: [0.84711001 0.03137251 0.12151748]\n","Logit 2253: [0.83644186 0.15472619 0.00883195]\n","Logit 2254: [0.79586303 0.1927624  0.01137457]\n","Logit 2255: [0.91288234 0.07851093 0.00860673]\n","Logit 2256: [0.80104553 0.18896879 0.00998568]\n","Logit 2257: [0.9106615 0.0472836 0.0420549]\n","Logit 2258: [0.82007223 0.14462771 0.03530006]\n","Logit 2259: [0.03722228 0.02091041 0.94186731]\n","Logit 2260: [0.64394616 0.3373533  0.01870054]\n","Logit 2261: [0.06941359 0.74408331 0.1865031 ]\n","Logit 2262: [0.03858281 0.01884972 0.94256747]\n","Logit 2263: [0.90769203 0.08321602 0.00909195]\n","Logit 2264: [0.1851011  0.80129901 0.0135999 ]\n","Logit 2265: [0.5765441  0.40604577 0.01741012]\n","Logit 2266: [0.9091763  0.08068413 0.01013957]\n","Logit 2267: [0.46638479 0.50823733 0.02537789]\n","Logit 2268: [0.96847789 0.02137558 0.01014653]\n","Logit 2269: [0.9082606  0.03276188 0.05897752]\n","Logit 2270: [0.96038357 0.02524583 0.0143706 ]\n","Logit 2271: [0.03714079 0.94902807 0.01383114]\n","Logit 2272: [0.86822741 0.02192753 0.10984506]\n","Logit 2273: [0.96597187 0.02480451 0.00922362]\n","Logit 2274: [0.86378163 0.12132712 0.01489124]\n","Logit 2275: [0.8485375  0.14229036 0.00917214]\n","Logit 2276: [0.05540329 0.92910246 0.01549425]\n","Logit 2277: [0.81177157 0.17814849 0.01007994]\n","Logit 2278: [0.19602396 0.78673017 0.01724587]\n","Logit 2279: [0.04188053 0.94554773 0.01257175]\n","Logit 2280: [0.33271133 0.64666347 0.0206252 ]\n","Logit 2281: [0.10257609 0.88709623 0.01032769]\n","Logit 2282: [0.10661051 0.88396415 0.00942533]\n","Logit 2283: [0.57933191 0.39314568 0.02752241]\n","Logit 2284: [0.96673734 0.02247061 0.01079205]\n","Logit 2285: [0.94624923 0.04306292 0.01068785]\n","Logit 2286: [0.89735082 0.03291021 0.06973898]\n","Logit 2287: [0.40293202 0.03874381 0.55832417]\n","Logit 2288: [0.02878297 0.93975911 0.03145792]\n","Logit 2289: [0.21416106 0.77310877 0.01273017]\n","Logit 2290: [0.83766728 0.02759225 0.13474047]\n","Logit 2291: [0.86448721 0.12534955 0.01016324]\n","Logit 2292: [0.0597374  0.92729134 0.01297127]\n","Logit 2293: [0.04141214 0.93932001 0.01926784]\n","Logit 2294: [0.77609984 0.02242858 0.20147159]\n","Logit 2295: [0.02013597 0.96689497 0.01296906]\n","Logit 2296: [0.02272117 0.95207962 0.02519921]\n","Logit 2297: [0.94927329 0.04203528 0.00869143]\n","Logit 2298: [0.9682074  0.02157932 0.01021328]\n","Logit 2299: [0.89051524 0.10112164 0.00836312]\n","Logit 2300: [0.06226063 0.92423012 0.01350926]\n","Logit 2301: [0.48288843 0.50655717 0.01055441]\n","Logit 2302: [0.92728644 0.04753943 0.02517414]\n","Logit 2303: [0.90365098 0.08632995 0.01001907]\n","Logit 2304: [0.0537921  0.93641641 0.00979149]\n","Logit 2305: [0.46440921 0.51909879 0.016492  ]\n","Logit 2306: [0.93107588 0.04559232 0.0233318 ]\n","Logit 2307: [0.77449221 0.20957438 0.01593342]\n","Logit 2308: [0.92929846 0.04808663 0.02261491]\n","Logit 2309: [0.42739961 0.56180619 0.0107942 ]\n","Logit 2310: [0.89243853 0.09926173 0.00829973]\n","Logit 2311: [0.09566954 0.89536201 0.00896845]\n","Logit 2312: [0.88738966 0.10215745 0.01045288]\n","Logit 2313: [0.07893833 0.90009908 0.02096259]\n","Logit 2314: [0.16541687 0.82501564 0.00956749]\n","Logit 2315: [0.96038882 0.0269662  0.01264498]\n","Logit 2316: [0.40374083 0.57889709 0.01736208]\n","Logit 2317: [0.94443288 0.04783102 0.0077361 ]\n","Logit 2318: [0.90842286 0.08296212 0.00861502]\n","Logit 2319: [0.02058087 0.96207136 0.01734776]\n","Logit 2320: [0.92254057 0.05196999 0.02548944]\n","Logit 2321: [0.0256541  0.96313299 0.01121292]\n","Logit 2322: [0.72416407 0.26240294 0.01343299]\n","Logit 2323: [0.81852572 0.16617183 0.01530245]\n","Logit 2324: [0.17385007 0.79645123 0.0296987 ]\n","Logit 2325: [0.02707845 0.95408225 0.0188393 ]\n","Logit 2326: [0.86211109 0.12314213 0.01474678]\n","Logit 2327: [0.70050292 0.28316699 0.01633009]\n","Logit 2328: [0.62072217 0.36118117 0.01809666]\n","Logit 2329: [0.79447812 0.18714518 0.01837671]\n","Logit 2330: [0.83245994 0.15331438 0.01422569]\n","Logit 2331: [0.9261892 0.0484197 0.0253911]\n","Logit 2332: [0.02586174 0.95727681 0.01686145]\n","Logit 2333: [0.14609105 0.03997321 0.81393574]\n","Logit 2334: [0.94381262 0.04671514 0.00947224]\n","Logit 2335: [0.0537921  0.93641641 0.00979149]\n","Logit 2336: [0.03977671 0.93268422 0.02753907]\n","Logit 2337: [0.10207632 0.06833032 0.82959335]\n","Logit 2338: [0.12078361 0.85275168 0.02646471]\n","Logit 2339: [0.12738023 0.85521368 0.01740609]\n","Logit 2340: [0.92319945 0.06596824 0.0108323 ]\n","Logit 2341: [0.46979111 0.5109504  0.01925849]\n","Logit 2342: [0.7596294  0.03800484 0.20236576]\n","Logit 2343: [0.13646791 0.02180492 0.84172717]\n","Logit 2344: [0.07404497 0.91112702 0.01482801]\n","Logit 2345: [0.96553523 0.02120683 0.01325794]\n","Logit 2346: [0.08945848 0.8943362  0.01620532]\n","Logit 2347: [0.0574981  0.93171809 0.01078381]\n","Logit 2348: [0.9445783  0.04584875 0.00957295]\n","Logit 2349: [0.03218699 0.95732253 0.01049048]\n","Logit 2350: [0.25242745 0.07274224 0.67483031]\n","Logit 2351: [0.59794957 0.39176214 0.01028829]\n","Logit 2352: [0.3626678  0.62480061 0.01253159]\n","Logit 2353: [0.37425202 0.61343238 0.0123156 ]\n","Logit 2354: [0.03706208 0.95142012 0.0115178 ]\n","Logit 2355: [0.32571898 0.65893495 0.01534607]\n","Logit 2356: [0.91883678 0.07297738 0.00818584]\n","Logit 2357: [0.08945848 0.8943362  0.01620532]\n","Logit 2358: [0.89303991 0.06933621 0.03762388]\n","Logit 2359: [0.88683602 0.07045296 0.04271102]\n","Logit 2360: [0.96489176 0.02018275 0.01492549]\n","Logit 2361: [0.95922791 0.0278429  0.0129292 ]\n","Logit 2362: [0.96394562 0.02327552 0.01277887]\n","Logit 2363: [0.89877689 0.09464948 0.00657363]\n","Logit 2364: [0.94040203 0.04513628 0.01446169]\n","Logit 2365: [0.93378105 0.04919946 0.01701949]\n","Logit 2366: [0.46169645 0.50955635 0.0287472 ]\n","Logit 2367: [0.91741734 0.07421565 0.00836701]\n","Logit 2368: [0.93289099 0.05729706 0.00981195]\n","Logit 2369: [0.03305959 0.01670477 0.95023564]\n","Logit 2370: [0.96815657 0.01624042 0.01560301]\n","Logit 2371: [0.05718805 0.87598498 0.06682697]\n","Logit 2372: [0.91039882 0.08103462 0.00856657]\n","Logit 2373: [0.05102046 0.90834219 0.04063735]\n","Logit 2374: [0.07477879 0.91010297 0.01511823]\n","Logit 2375: [0.85344589 0.1365598  0.00999432]\n","Logit 2376: [0.69589387 0.29538027 0.00872586]\n","Logit 2377: [0.47889541 0.51177673 0.00932786]\n","Logit 2378: [0.55754222 0.43252148 0.0099363 ]\n","Logit 2379: [0.94985277 0.02566117 0.02448606]\n","Logit 2380: [0.82890854 0.01744027 0.15365119]\n","Logit 2381: [0.27955899 0.02666521 0.6937758 ]\n","Logit 2382: [0.95365879 0.03076027 0.01558094]\n","Logit 2383: [0.92901841 0.04861505 0.02236654]\n","Logit 2384: [0.41324622 0.02356555 0.56318823]\n","Logit 2385: [0.92169594 0.06968186 0.0086222 ]\n","Logit 2386: [0.32872977 0.65727806 0.01399217]\n","Logit 2387: [0.66672928 0.02351545 0.30975527]\n","Logit 2388: [0.88213831 0.03341187 0.08444981]\n","Logit 2389: [0.89379368 0.09330675 0.01289957]\n","Logit 2390: [0.91437209 0.07641522 0.00921269]\n","Logit 2391: [0.67950016 0.30567711 0.01482274]\n","Logit 2392: [0.06074305 0.92094719 0.01830975]\n","Logit 2393: [0.94169544 0.05018906 0.0081155 ]\n","Logit 2394: [0.77655933 0.2110592  0.01238147]\n","Logit 2395: [0.56295073 0.39550553 0.04154374]\n","Logit 2396: [0.68475758 0.30282253 0.01241989]\n","Logit 2397: [0.63969694 0.35189692 0.00840615]\n","Logit 2398: [0.06809633 0.90469088 0.02721279]\n","Logit 2399: [0.21314442 0.77661033 0.01024525]\n","Logit 2400: [0.80560084 0.18209487 0.01230429]\n","Logit 2401: [0.84795527 0.13329265 0.01875208]\n","Logit 2402: [0.35800481 0.63139348 0.01060171]\n","Logit 2403: [0.96250886 0.02381521 0.01367593]\n","Logit 2404: [0.86599723 0.12690916 0.00709361]\n","Logit 2405: [0.93591472 0.05682695 0.00725833]\n","Logit 2406: [0.91437997 0.02284639 0.06277364]\n","Logit 2407: [0.33399391 0.65520678 0.01079931]\n","Logit 2408: [0.26131381 0.02348924 0.71519695]\n","Logit 2409: [0.06570944 0.92396591 0.01032465]\n","Logit 2410: [0.11010768 0.87916575 0.01072657]\n","Logit 2411: [0.73745862 0.25398927 0.00855212]\n","Logit 2412: [0.18368372 0.80610923 0.01020705]\n","Logit 2413: [0.84476471 0.14669062 0.00854467]\n","Logit 2414: [0.26345708 0.02248735 0.71405557]\n","Logit 2415: [0.49532582 0.4895551  0.01511909]\n","Logit 2416: [0.24247819 0.74871108 0.00881073]\n","Logit 2417: [0.03691306 0.01479816 0.94828878]\n","Logit 2418: [0.73562502 0.25546958 0.0089054 ]\n","Logit 2419: [0.90946802 0.08277038 0.0077616 ]\n","Logit 2420: [0.03345662 0.93799619 0.02854719]\n","Logit 2421: [0.46877816 0.47989491 0.05132693]\n","Logit 2422: [0.04478604 0.89080982 0.06440413]\n","Logit 2423: [0.54027557 0.44790418 0.01182024]\n","Logit 2424: [0.03917245 0.9338916  0.02693595]\n","Logit 2425: [0.05808474 0.83826686 0.10364841]\n","Logit 2426: [0.82689779 0.16652443 0.00657778]\n","Logit 2427: [0.51046629 0.47860302 0.01093069]\n","Logit 2428: [0.38610867 0.60329598 0.01059536]\n","Logit 2429: [0.35207547 0.63814921 0.00977532]\n","Logit 2430: [0.04409463 0.94209459 0.01381078]\n","Logit 2431: [0.73118093 0.25767322 0.01114585]\n","Logit 2432: [0.92067136 0.06150069 0.01782795]\n","Logit 2433: [0.0513762  0.93271597 0.01590783]\n","Logit 2434: [0.02012276 0.96720455 0.01267269]\n","Logit 2435: [0.87112961 0.11710569 0.0117647 ]\n","Logit 2436: [0.81365908 0.1762671  0.01007382]\n","Logit 2437: [0.03381331 0.9419978  0.02418889]\n","Logit 2438: [0.97220736 0.01563773 0.01215491]\n","Logit 2439: [0.72086097 0.26962925 0.00950979]\n","Logit 2440: [0.45948825 0.53145641 0.00905535]\n","Logit 2441: [0.45119427 0.54000884 0.00879689]\n","Logit 2442: [0.86703163 0.01941891 0.11354946]\n","Logit 2443: [0.16733115 0.82082243 0.01184642]\n","Logit 2444: [0.74128275 0.24878405 0.00993319]\n","Logit 2445: [0.96262041 0.02155981 0.01581977]\n","Logit 2446: [0.96697827 0.02099109 0.01203065]\n","Logit 2447: [0.0199515  0.96885673 0.01119178]\n","Logit 2448: [0.86264074 0.12547943 0.01187983]\n","Logit 2449: [0.82365896 0.16717313 0.00916791]\n","Logit 2450: [0.05613701 0.9359069  0.00795609]\n","Logit 2451: [0.68491486 0.29790511 0.01718003]\n","Logit 2452: [0.9736071  0.01673548 0.00965742]\n","Logit 2453: [0.82754358 0.15770827 0.01474816]\n","Logit 2454: [0.93488553 0.04760249 0.01751198]\n","Logit 2455: [0.03938775 0.0257053  0.93490696]\n","Logit 2456: [0.96028591 0.02935427 0.01035981]\n","Logit 2457: [0.54265346 0.44463431 0.01271223]\n","Logit 2458: [0.73076824 0.25815728 0.01107448]\n","Logit 2459: [0.57422024 0.40997613 0.01580363]\n","Logit 2460: [0.93965801 0.04572141 0.01462058]\n","Logit 2461: [0.53906171 0.03053081 0.43040748]\n","Logit 2462: [0.07292887 0.91464615 0.01242498]\n","Logit 2463: [0.93285447 0.02498141 0.04216412]\n","Logit 2464: [0.97099016 0.01334587 0.01566397]\n","Logit 2465: [0.02368772 0.96345235 0.01285994]\n","Logit 2466: [0.02744308 0.96081129 0.01174563]\n","Logit 2467: [0.02288933 0.96617641 0.01093426]\n","Logit 2468: [0.70685428 0.02202272 0.271123  ]\n","Logit 2469: [0.04949465 0.94030152 0.01020383]\n","Logit 2470: [0.11650348 0.19347017 0.69002635]\n","Logit 2471: [0.82063613 0.16790694 0.01145694]\n","Logit 2472: [0.83370335 0.15649532 0.00980133]\n","Logit 2473: [0.03481062 0.01741011 0.94777927]\n","Logit 2474: [0.1958018  0.78143823 0.02275997]\n","Logit 2475: [0.59716149 0.39010125 0.01273726]\n","Logit 2476: [0.11181298 0.01842534 0.86976168]\n","Logit 2477: [0.44685333 0.0271387  0.52600797]\n","Logit 2478: [0.08196272 0.90757387 0.01046341]\n","Logit 2479: [0.60659891 0.38220371 0.01119739]\n","Logit 2480: [0.74330451 0.24667667 0.01001882]\n","Logit 2481: [0.24362865 0.73807361 0.01829775]\n","Logit 2482: [0.851857   0.13683905 0.01130396]\n","Logit 2483: [0.48701992 0.50297053 0.01000955]\n","Logit 2484: [0.24104655 0.7377776  0.02117585]\n","Logit 2485: [0.43898659 0.53393262 0.02708078]\n","Logit 2486: [0.24104655 0.7377776  0.02117585]\n","Logit 2487: [0.29568713 0.68280954 0.02150334]\n","Logit 2488: [0.74330451 0.24667667 0.01001882]\n","Logit 2489: [0.61210821 0.37563872 0.01225307]\n","Logit 2490: [0.96379206 0.0215019  0.01470604]\n","Logit 2491: [0.91191931 0.07973127 0.00834942]\n","Logit 2492: [0.89777183 0.09077132 0.01145685]\n","Logit 2493: [0.83661398 0.03781414 0.12557189]\n","Logit 2494: [0.82343579 0.02636611 0.1501981 ]\n","Logit 2495: [0.97311988 0.01648387 0.01039625]\n","Logit 2496: [0.59723508 0.38092549 0.02183943]\n","Logit 2497: [0.84595703 0.14359991 0.01044306]\n","Logit 2498: [0.89694611 0.030993   0.07206089]\n","Logit 2499: [0.86154925 0.04202331 0.09642744]\n","Logit 2500: [0.18048647 0.03769028 0.78182326]\n","Logit 2501: [0.65602166 0.29764771 0.04633062]\n","Logit 2502: [0.80189633 0.17918955 0.01891413]\n","Logit 2503: [0.44912274 0.53543391 0.01544336]\n","Logit 2504: [0.05120602 0.03625736 0.91253662]\n","Logit 2505: [0.79079819 0.19897118 0.01023063]\n","Logit 2506: [0.76249633 0.02815626 0.20934742]\n","Logit 2507: [0.94426701 0.04556587 0.01016712]\n","Logit 2508: [0.8342546  0.03107798 0.13466742]\n","Logit 2509: [0.76949615 0.01941505 0.2110888 ]\n","Logit 2510: [0.46569934 0.02007544 0.51422522]\n","Logit 2511: [0.83421484 0.01692903 0.14885612]\n","Logit 2512: [0.21643536 0.02508175 0.75848289]\n","Logit 2513: [0.90133783 0.07793554 0.02072663]\n","Logit 2514: [0.86537181 0.11367154 0.02095665]\n","Logit 2515: [0.24662629 0.73449471 0.018879  ]\n","Logit 2516: [0.81123555 0.17907707 0.00968738]\n","Logit 2517: [0.53604681 0.44396465 0.01998854]\n","Logit 2518: [0.13331447 0.85354669 0.01313883]\n","Logit 2519: [0.57942208 0.40331848 0.01725944]\n","Logit 2520: [0.81123555 0.17907707 0.00968738]\n","Logit 2521: [0.31500471 0.66751653 0.01747875]\n","Logit 2522: [0.94088829 0.03793548 0.02117623]\n","Logit 2523: [0.05822342 0.01576271 0.92601387]\n","Logit 2524: [0.36723419 0.03900978 0.59375603]\n","Logit 2525: [0.14136805 0.01772726 0.8409047 ]\n","Logit 2526: [0.95990774 0.02679523 0.01329702]\n","Logit 2527: [0.7946423  0.04861571 0.15674199]\n","Logit 2528: [0.88578246 0.09186849 0.02234905]\n","Logit 2529: [0.08196272 0.90757387 0.01046341]\n","Logit 2530: [0.84606012 0.14435563 0.00958426]\n","Logit 2531: [0.90805028 0.08299519 0.00895453]\n","Logit 2532: [0.09827887 0.02212797 0.87959316]\n","Logit 2533: [0.08931399 0.90071894 0.00996707]\n","Logit 2534: [0.07750734 0.90485087 0.01764178]\n","Logit 2535: [0.19326276 0.02261617 0.78412107]\n","Logit 2536: [0.06582947 0.01784801 0.91632252]\n","Logit 2537: [0.73527188 0.25224885 0.01247926]\n","Logit 2538: [0.1583357  0.06230865 0.77935564]\n","Logit 2539: [0.04283981 0.01765023 0.93950996]\n","Logit 2540: [0.38645633 0.58956334 0.02398033]\n","Logit 2541: [0.04136224 0.944854   0.01378376]\n","Logit 2542: [0.94278222 0.04838108 0.0088367 ]\n","Logit 2543: [0.20868433 0.0282596  0.76305607]\n","Logit 2544: [0.86453169 0.12073778 0.01473053]\n","Logit 2545: [0.97131065 0.01682696 0.01186239]\n","Logit 2546: [0.82150893 0.16890826 0.00958281]\n","Logit 2547: [0.09888772 0.01736546 0.88374682]\n","Logit 2548: [0.95759047 0.03233779 0.01007173]\n","Logit 2549: [0.95161146 0.03868764 0.00970091]\n","Logit 2550: [0.60501813 0.38344789 0.01153398]\n","Logit 2551: [0.67949495 0.31124739 0.00925766]\n","Logit 2552: [0.90728208 0.08198889 0.01072903]\n","Logit 2553: [0.85892225 0.13428515 0.00679261]\n","Logit 2554: [0.7091377  0.27851709 0.01234521]\n","Logit 2555: [0.87219591 0.11803888 0.00976521]\n","Logit 2556: [0.95725354 0.03090639 0.01184008]\n","Logit 2557: [0.84934473 0.06104752 0.08960774]\n","Logit 2558: [0.96313616 0.02579234 0.0110715 ]\n","Logit 2559: [0.65178102 0.33616404 0.01205494]\n","Logit 2560: [0.02097899 0.96171354 0.01730748]\n","Logit 2561: [0.75471567 0.05613212 0.18915221]\n","Logit 2562: [0.71715047 0.03789639 0.24495314]\n","Logit 2563: [0.07629664 0.01597313 0.90773023]\n","Logit 2564: [0.95491498 0.02773125 0.01735377]\n","Logit 2565: [0.94628307 0.0416174  0.01209953]\n","Logit 2566: [0.93080329 0.05672394 0.01247277]\n","Logit 2567: [0.76751058 0.22379498 0.00869444]\n","Logit 2568: [0.92819548 0.06223078 0.00957374]\n","Logit 2569: [0.88926075 0.101585   0.00915425]\n","Logit 2570: [0.12206557 0.86909203 0.0088424 ]\n","Logit 2571: [0.93245823 0.02927948 0.0382623 ]\n","Logit 2572: [0.63857183 0.02476012 0.33666806]\n","Logit 2573: [0.7393045  0.24630948 0.01438602]\n","Logit 2574: [0.80709942 0.02874563 0.16415494]\n","Logit 2575: [0.91586386 0.07283011 0.01130603]\n","Logit 2576: [0.87136644 0.12087063 0.00776293]\n","Logit 2577: [0.83907006 0.15336663 0.00756332]\n","Logit 2578: [0.78648496 0.03375525 0.17975979]\n","Logit 2579: [0.54049227 0.44983758 0.00967014]\n","Logit 2580: [0.77166917 0.21958074 0.00875009]\n","Logit 2581: [0.94833429 0.04406277 0.00760294]\n","Logit 2582: [0.37127459 0.03774334 0.59098207]\n","Logit 2583: [0.04278067 0.94280539 0.01441394]\n","Logit 2584: [0.9567374  0.02124054 0.02202206]\n","Logit 2585: [0.94284379 0.04569757 0.01145864]\n","Logit 2586: [0.18389366 0.80506267 0.01104367]\n","Logit 2587: [0.95141543 0.03990749 0.00867708]\n","Logit 2588: [0.94439634 0.04262331 0.01298035]\n","Logit 2589: [0.89016201 0.10288034 0.00695764]\n","Logit 2590: [0.52673937 0.46484587 0.00841476]\n","Logit 2591: [0.88236274 0.10779673 0.00984053]\n","Logit 2592: [0.14034696 0.82689457 0.03275847]\n","Logit 2593: [0.53099679 0.0746888  0.3943144 ]\n","Logit 2594: [0.03329858 0.95290834 0.01379308]\n","Logit 2595: [0.05288096 0.93368319 0.01343585]\n","Logit 2596: [0.96296783 0.02494566 0.01208651]\n","Logit 2597: [0.02124615 0.96685329 0.01190056]\n","Logit 2598: [0.20459229 0.7777992  0.01760851]\n","Logit 2599: [0.0445604  0.94279381 0.01264579]\n","Logit 2600: [0.04408366 0.94071116 0.01520518]\n","Logit 2601: [0.23048809 0.76018253 0.00932937]\n","Logit 2602: [0.1209171  0.86809103 0.01099187]\n","Logit 2603: [0.0468284  0.94103298 0.01213862]\n","Logit 2604: [0.96398973 0.02311801 0.01289226]\n","Logit 2605: [0.37252871 0.61030964 0.01716165]\n","Logit 2606: [0.96453636 0.02029388 0.01516976]\n","Logit 2607: [0.06285746 0.92684621 0.01029633]\n","Logit 2608: [0.43273637 0.40099816 0.16626547]\n","Logit 2609: [0.15319322 0.04049812 0.80630865]\n","Logit 2610: [0.5241097  0.46706417 0.00882613]\n","Logit 2611: [0.04510903 0.94236159 0.01252938]\n","Logit 2612: [0.35860515 0.04464426 0.59675059]\n","Logit 2613: [0.95876138 0.0277615  0.01347712]\n","Logit 2614: [0.87211553 0.08547022 0.04241425]\n","Logit 2615: [0.06179783 0.02073003 0.91747214]\n","Logit 2616: [0.74839272 0.2389672  0.01264009]\n","Logit 2617: [0.86827801 0.01561071 0.11611128]\n","Logit 2618: [0.27709137 0.69321451 0.02969411]\n","Logit 2619: [0.96239411 0.02478514 0.01282075]\n","Logit 2620: [0.15637089 0.82959023 0.01403888]\n","Logit 2621: [0.01959207 0.96780151 0.01260643]\n","Logit 2622: [0.95563127 0.03621317 0.00815556]\n","Logit 2623: [0.66576478 0.32478486 0.00945037]\n","Logit 2624: [0.83665991 0.15210692 0.01123317]\n","Logit 2625: [0.87177807 0.11998666 0.00823526]\n","Logit 2626: [0.83470998 0.15603532 0.0092547 ]\n","Logit 2627: [0.10085655 0.89051486 0.00862859]\n","Logit 2628: [0.64197192 0.3493751  0.00865298]\n","Logit 2629: [0.73617024 0.25329805 0.01053171]\n","Logit 2630: [0.80621237 0.18285703 0.0109306 ]\n","Logit 2631: [0.3306736  0.65666036 0.01266604]\n","Logit 2632: [0.08235068 0.90028412 0.0173652 ]\n","Logit 2633: [0.91741262 0.0741383  0.00844908]\n","Logit 2634: [0.79770701 0.19067855 0.01161444]\n","Logit 2635: [0.73310904 0.25621705 0.01067391]\n","Logit 2636: [0.74937667 0.24198377 0.00863956]\n","Logit 2637: [0.55990029 0.42719941 0.0129003 ]\n","Logit 2638: [0.94669687 0.04414807 0.00915506]\n","Logit 2639: [0.91937418 0.07138081 0.00924501]\n","Logit 2640: [0.95337752 0.03743875 0.00918372]\n","Logit 2641: [0.76103783 0.22895173 0.01001044]\n","Logit 2642: [0.95668765 0.03037956 0.01293279]\n","Logit 2643: [0.88105652 0.07151386 0.04742961]\n","Logit 2644: [0.09122584 0.89289243 0.01588173]\n","Logit 2645: [0.16855818 0.81322795 0.01821387]\n","Logit 2646: [0.0358506  0.95069401 0.01345539]\n","Logit 2647: [0.03352102 0.95413524 0.01234374]\n","Logit 2648: [0.87542461 0.08442317 0.04015222]\n","Logit 2649: [0.07023887 0.92054048 0.00922065]\n","Logit 2650: [0.76543482 0.22537999 0.00918519]\n","Logit 2651: [0.14909539 0.84056853 0.01033608]\n","Logit 2652: [0.66036697 0.32995228 0.00968075]\n","Logit 2653: [0.23838094 0.02842887 0.73319019]\n","Logit 2654: [0.93252327 0.05819062 0.0092861 ]\n","Logit 2655: [0.17092171 0.80395676 0.02512153]\n","Logit 2656: [0.64152203 0.34776913 0.01070885]\n","Logit 2657: [0.0360043  0.95197692 0.01201878]\n","Logit 2658: [0.23930838 0.03782713 0.72286449]\n","Logit 2659: [0.11021098 0.02492078 0.86486824]\n","Logit 2660: [0.14805146 0.8357026  0.01624594]\n","Logit 2661: [0.77904122 0.21235497 0.00860381]\n","Logit 2662: [0.2639502  0.71329927 0.02275053]\n","Logit 2663: [0.14368085 0.83463453 0.02168462]\n","Logit 2664: [0.26970946 0.716163   0.01412755]\n","Logit 2665: [0.02596457 0.95827173 0.01576369]\n","Logit 2666: [0.02947902 0.95313182 0.01738916]\n","Logit 2667: [0.89424833 0.09333997 0.0124117 ]\n","Logit 2668: [0.9104616  0.07266614 0.01687225]\n","Logit 2669: [0.16332221 0.82236149 0.0143163 ]\n","Logit 2670: [0.77672131 0.21421832 0.00906037]\n","Logit 2671: [0.95945795 0.02795858 0.01258347]\n","Logit 2672: [0.02354751 0.96384842 0.01260407]\n","Logit 2673: [0.02327873 0.96549186 0.01122941]\n","Logit 2674: [0.02491248 0.96403806 0.01104945]\n","Logit 2675: [0.88309532 0.10574464 0.01116004]\n","Logit 2676: [0.09735521 0.87869642 0.02394837]\n","Logit 2677: [0.55977278 0.43129347 0.00893374]\n","Logit 2678: [0.32263083 0.65396361 0.02340556]\n","Logit 2679: [0.0648009  0.91397561 0.02122349]\n","Logit 2680: [0.64243168 0.34877278 0.00879554]\n","Logit 2681: [0.58683956 0.39321669 0.01994374]\n","Logit 2682: [0.67420631 0.31185554 0.01393815]\n","Logit 2683: [0.03915863 0.94180351 0.01903786]\n","Logit 2684: [0.05643869 0.93270267 0.01085864]\n","Logit 2685: [0.68505993 0.30133339 0.01360668]\n","Logit 2686: [0.6228634  0.15086094 0.22627566]\n","Logit 2687: [0.24214511 0.74005512 0.01779977]\n","Logit 2688: [0.91340816 0.06724187 0.01934997]\n","Logit 2689: [0.82940714 0.16236579 0.00822707]\n","Logit 2690: [0.10784844 0.01906429 0.87308727]\n","Logit 2691: [0.96553968 0.02290777 0.01155255]\n","Logit 2692: [0.19144925 0.7916001  0.01695065]\n","Logit 2693: [0.92742956 0.06460904 0.0079614 ]\n","Logit 2694: [0.03574655 0.94700969 0.01724375]\n","Logit 2695: [0.62012669 0.36957203 0.01030127]\n","Logit 2696: [0.90966025 0.08225148 0.00808827]\n","Logit 2697: [0.66557884 0.32485273 0.00956843]\n","Logit 2698: [0.17092171 0.80395676 0.02512153]\n","Logit 2699: [0.02692762 0.96229836 0.01077401]\n","Logit 2700: [0.03826039 0.95266084 0.00907877]\n","Logit 2701: [0.65348934 0.29354568 0.05296498]\n","Logit 2702: [0.63818275 0.34731295 0.0145043 ]\n","Logit 2703: [0.92087335 0.06986656 0.00926008]\n","Logit 2704: [0.62987334 0.34379653 0.02633013]\n","Logit 2705: [0.92980454 0.06194421 0.00825125]\n","Logit 2706: [0.92344926 0.06906004 0.0074907 ]\n","Logit 2707: [0.13609957 0.85223251 0.01166793]\n","Logit 2708: [0.96013285 0.02655402 0.01331314]\n","Logit 2709: [0.94971792 0.04187056 0.00841152]\n","Logit 2710: [0.60302185 0.33172197 0.06525617]\n","Logit 2711: [0.06692924 0.820958   0.11211276]\n","Logit 2712: [0.49142189 0.49867344 0.00990467]\n","Logit 2713: [0.79430737 0.19427358 0.01141905]\n","Logit 2714: [0.7077868  0.02192294 0.27029026]\n","Logit 2715: [0.22189718 0.03862119 0.73948164]\n","Logit 2716: [0.72991602 0.12796011 0.14212387]\n","Logit 2717: [0.88054851 0.11063143 0.00882007]\n","Logit 2718: [0.7316633  0.25822119 0.01011552]\n","Logit 2719: [0.31756186 0.6726068  0.00983134]\n","Logit 2720: [0.34668052 0.64264416 0.01067532]\n","Logit 2721: [0.23493021 0.75577636 0.00929343]\n","Logit 2722: [0.67589624 0.31386085 0.01024292]\n","Logit 2723: [0.13365548 0.85114608 0.01519845]\n","Logit 2724: [0.78039824 0.03821403 0.18138773]\n","Logit 2725: [0.90129652 0.08777114 0.01093234]\n","Logit 2726: [0.88564216 0.10281033 0.0115475 ]\n","Logit 2727: [0.76740278 0.19487246 0.03772475]\n","Logit 2728: [0.17092171 0.80395676 0.02512153]\n","Logit 2729: [0.48323167 0.493244   0.02352433]\n","Logit 2730: [0.94005253 0.04902461 0.01092286]\n","Logit 2731: [0.9672034  0.02421346 0.00858314]\n","Logit 2732: [0.17672402 0.81477306 0.00850292]\n","Logit 2733: [0.06093054 0.92944532 0.00962415]\n","Logit 2734: [0.03228421 0.95181179 0.015904  ]\n","Logit 2735: [0.90457305 0.08648928 0.00893768]\n","Logit 2736: [0.9248875  0.06803709 0.00707541]\n","Logit 2737: [0.88233664 0.10452492 0.01313844]\n","Logit 2738: [0.90512673 0.08341245 0.01146082]\n","Logit 2739: [0.95888513 0.01759693 0.02351794]\n","Logit 2740: [0.96394389 0.02480082 0.01125528]\n","Logit 2741: [0.93597474 0.05638834 0.00763692]\n","Logit 2742: [0.96185592 0.02921622 0.00892786]\n","Logit 2743: [0.57212674 0.41694499 0.01092827]\n","Logit 2744: [0.87792847 0.11376195 0.00830958]\n","Logit 2745: [0.79088448 0.19514402 0.01397149]\n","Logit 2746: [0.94671925 0.04537297 0.00790777]\n","Logit 2747: [0.86088027 0.13010047 0.00901925]\n","Logit 2748: [0.92701445 0.06396814 0.00901741]\n","Logit 2749: [0.53884966 0.45110595 0.01004439]\n","Logit 2750: [0.88113626 0.10886263 0.01000112]\n","Logit 2751: [0.16245806 0.82947335 0.00806859]\n","Logit 2752: [0.53150544 0.4555095  0.01298506]\n","Logit 2753: [0.09098489 0.8917307  0.01728441]\n","Logit 2754: [0.62484895 0.35689847 0.01825259]\n","Logit 2755: [0.80600184 0.18636937 0.00762879]\n","Logit 2756: [0.84561798 0.14396351 0.0104185 ]\n","Logit 2757: [0.25266464 0.58897076 0.1583646 ]\n","Logit 2758: [0.88545218 0.10684213 0.00770568]\n","Logit 2759: [0.94907467 0.04204701 0.00887832]\n","Logit 2760: [0.94877114 0.04230122 0.00892764]\n","Logit 2761: [0.93748413 0.05236247 0.01015341]\n","Logit 2762: [0.83490573 0.15641522 0.00867905]\n","Logit 2763: [0.10619103 0.01729083 0.87651814]\n","Logit 2764: [0.13314748 0.01787987 0.84897265]\n","Logit 2765: [0.8521034  0.13951018 0.00838642]\n","Logit 2766: [0.30403823 0.05739971 0.63856207]\n","Logit 2767: [0.17092171 0.80395676 0.02512153]\n","Logit 2768: [0.86661736 0.12579655 0.00758609]\n","Logit 2769: [0.02896616 0.95969606 0.01133777]\n","Logit 2770: [0.0664754  0.92080653 0.01271806]\n","Logit 2771: [0.03369236 0.95434109 0.01196655]\n","Logit 2772: [0.28163401 0.70871959 0.0096464 ]\n","Logit 2773: [0.49512701 0.48840376 0.01646923]\n","Logit 2774: [0.5107644  0.03826746 0.45096815]\n","Logit 2775: [0.91913982 0.07022195 0.01063823]\n","Logit 2776: [0.94211712 0.02567687 0.03220601]\n","Logit 2777: [0.93085645 0.02745827 0.04168528]\n","Logit 2778: [0.50633937 0.33261314 0.16104749]\n","Logit 2779: [0.39858668 0.03018541 0.57122791]\n","Logit 2780: [0.96401738 0.0197166  0.01626602]\n","Logit 2781: [0.38500086 0.60628315 0.00871599]\n","Logit 2782: [0.03369236 0.95434109 0.01196655]\n","Logit 2783: [0.81220232 0.18008011 0.00771757]\n","Logit 2784: [0.8976743  0.09491494 0.00741075]\n","Logit 2785: [0.06635882 0.92589225 0.00774893]\n","Logit 2786: [0.03107167 0.96002615 0.00890218]\n","Logit 2787: [0.31441216 0.67279921 0.01278863]\n","Logit 2788: [0.23783485 0.74922359 0.01294156]\n","Logit 2789: [0.91746101 0.07166837 0.01087061]\n","Logit 2790: [0.48360015 0.50631392 0.01008593]\n","Logit 2791: [0.32373124 0.66620228 0.01006647]\n","Logit 2792: [0.84056641 0.13832868 0.02110491]\n","Logit 2793: [0.94798825 0.01671416 0.03529759]\n","Logit 2794: [0.27248913 0.71826388 0.00924699]\n","Logit 2795: [0.77568218 0.20699569 0.01732213]\n","Logit 2796: [0.91155288 0.07460592 0.0138412 ]\n","Logit 2797: [0.9306552 0.0589454 0.0103994]\n","Logit 2798: [0.05568833 0.93024176 0.01406991]\n","Logit 2799: [0.96732476 0.02272709 0.00994814]\n","Logit 2800: [0.1822608  0.02767559 0.7900636 ]\n","Logit 2801: [0.06361987 0.01553752 0.92084261]\n","Logit 2802: [0.95609296 0.01296267 0.03094437]\n","Logit 2803: [0.10509725 0.88061325 0.0142895 ]\n","Logit 2804: [0.04418716 0.93741087 0.01840197]\n","Logit 2805: [0.04519488 0.93863916 0.01616596]\n","Logit 2806: [0.08543432 0.02061702 0.89394865]\n","Logit 2807: [0.17639836 0.11719149 0.70641015]\n","Logit 2808: [0.56732061 0.42366049 0.00901891]\n","Logit 2809: [0.06689253 0.92072151 0.01238596]\n","Logit 2810: [0.92009178 0.070656   0.00925222]\n","Logit 2811: [0.68935847 0.29018743 0.0204541 ]\n","Logit 2812: [0.06606859 0.91450144 0.01942997]\n","Logit 2813: [0.16410738 0.82188867 0.01400395]\n","Logit 2814: [0.86916569 0.12053592 0.01029838]\n","Logit 2815: [0.02446443 0.9620777  0.01345787]\n","Logit 2816: [0.06749175 0.02594427 0.90656398]\n","Logit 2817: [0.03091406 0.95422421 0.01486173]\n","Logit 2818: [0.0789675  0.90605483 0.01497767]\n","Logit 2819: [0.91781536 0.07366555 0.00851908]\n","Logit 2820: [0.03370787 0.95431423 0.0119779 ]\n","Logit 2821: [0.04641965 0.94312364 0.01045671]\n","Logit 2822: [0.11802408 0.87400484 0.00797108]\n","Logit 2823: [0.35614603 0.63427922 0.00957475]\n","Logit 2824: [0.69437817 0.29574604 0.0098758 ]\n","Logit 2825: [0.08258046 0.89883749 0.01858205]\n","Logit 2826: [0.14576887 0.84562747 0.00860367]\n","Logit 2827: [0.46334835 0.03442799 0.50222366]\n","Logit 2828: [0.05700775 0.92886067 0.01413158]\n","Logit 2829: [0.95737719 0.02821301 0.0144098 ]\n","Logit 2830: [0.86963158 0.12252963 0.0078388 ]\n","Logit 2831: [0.5957796  0.39571376 0.00850664]\n","Logit 2832: [0.9099969  0.07870347 0.01129962]\n","Logit 2833: [0.04999818 0.9365121  0.01348972]\n","Logit 2834: [0.10165501 0.86349586 0.03484913]\n","Logit 2835: [0.32920688 0.0379321  0.63286102]\n","Logit 2836: [0.24188865 0.74144032 0.01667104]\n","Logit 2837: [0.25172014 0.73732218 0.01095768]\n","Logit 2838: [0.70643108 0.28320277 0.01036615]\n","Logit 2839: [0.5768195  0.41304024 0.01014026]\n","Logit 2840: [0.62526446 0.3636466  0.01108894]\n","Logit 2841: [0.12634456 0.83033172 0.04332372]\n","Logit 2842: [0.03114489 0.95386779 0.01498732]\n","Logit 2843: [0.93508998 0.05068367 0.01422635]\n","Logit 2844: [0.54107296 0.43222433 0.02670271]\n","Logit 2845: [0.09756812 0.88936856 0.01306332]\n","Logit 2846: [0.14591343 0.82898009 0.02510648]\n","Logit 2847: [0.40077138 0.43503322 0.1641954 ]\n","Logit 2848: [0.08987093 0.8854485  0.02468057]\n","Logit 2849: [0.0681478  0.91907103 0.01278117]\n","Logit 2850: [0.95436961 0.0362007  0.00942969]\n","Logit 2851: [0.94241196 0.04852766 0.00906038]\n","Logit 2852: [0.01734467 0.97228824 0.01036709]\n","Logit 2853: [0.95436961 0.0362007  0.00942969]\n","Logit 2854: [0.03112797 0.95895204 0.00991999]\n","Logit 2855: [0.33760869 0.64411906 0.01827225]\n","Logit 2856: [0.74032798 0.25077943 0.00889259]\n","Logit 2857: [0.66871796 0.32273991 0.00854213]\n","Logit 2858: [0.92017325 0.07062776 0.00919899]\n","Logit 2859: [0.05907336 0.91687165 0.02405499]\n","Logit 2860: [0.87055266 0.11122009 0.01822724]\n","Logit 2861: [0.16349026 0.82200269 0.01450704]\n","Logit 2862: [0.80429806 0.17009907 0.02560287]\n","Logit 2863: [0.39270496 0.59763516 0.00965987]\n","Logit 2864: [0.94042011 0.05172473 0.00785516]\n","Logit 2865: [0.30118278 0.6884051  0.01041213]\n","Logit 2866: [0.06788659 0.91969286 0.01242055]\n","Logit 2867: [0.09505002 0.88506893 0.01988105]\n","Logit 2868: [0.32612336 0.65334147 0.02053517]\n","Logit 2869: [0.94965593 0.01921534 0.03112872]\n","Logit 2870: [0.38579657 0.60339729 0.01080614]\n","Logit 2871: [0.93093549 0.05512095 0.01394356]\n","Logit 2872: [0.91440968 0.0687439  0.01684642]\n","Logit 2873: [0.79036454 0.02825978 0.18137568]\n","Logit 2874: [0.96223321 0.02581854 0.01194825]\n","Logit 2875: [0.95826729 0.03217159 0.00956112]\n","Logit 2876: [0.34993574 0.02866843 0.62139583]\n","Logit 2877: [0.91369268 0.07752305 0.00878427]\n","Logit 2878: [0.94351953 0.04620165 0.01027882]\n","Logit 2879: [0.92971417 0.05868487 0.01160096]\n","Logit 2880: [0.2475131  0.73929854 0.01318836]\n","Logit 2881: [0.77222614 0.21569962 0.01207424]\n","Logit 2882: [0.7802694  0.02961127 0.19011933]\n","Logit 2883: [0.93734925 0.04612417 0.01652658]\n","Logit 2884: [0.63404236 0.34004191 0.02591573]\n","Logit 2885: [0.75634834 0.22010991 0.02354175]\n","Logit 2886: [0.04513403 0.91320527 0.0416607 ]\n","Logit 2887: [0.774855   0.21318249 0.01196251]\n","Logit 2888: [0.04626128 0.88966791 0.06407081]\n","Logit 2889: [0.80191076 0.18834339 0.00974585]\n","Logit 2890: [0.93916408 0.05286772 0.0079682 ]\n","Logit 2891: [0.3630471  0.61156237 0.02539053]\n","Logit 2892: [0.27413968 0.70587525 0.01998507]\n","Logit 2893: [0.79673849 0.19354063 0.00972088]\n","Logit 2894: [0.92917995 0.06104713 0.00977292]\n","Logit 2895: [0.96538858 0.02469288 0.00991854]\n","Logit 2896: [0.13622216 0.855294   0.00848385]\n","Logit 2897: [0.56128037 0.35590155 0.08281808]\n","Logit 2898: [0.83609189 0.15451048 0.00939763]\n","Logit 2899: [0.42753024 0.0447981  0.52767166]\n","Logit 2900: [0.71891891 0.19065739 0.0904237 ]\n","Logit 2901: [0.69623553 0.28924383 0.01452064]\n","Logit 2902: [0.96624765 0.02252512 0.01122723]\n","Logit 2903: [0.52895688 0.46136439 0.00967873]\n","Logit 2904: [0.28982571 0.69322184 0.01695245]\n","Logit 2905: [0.23880262 0.74714728 0.0140501 ]\n","Logit 2906: [0.22206947 0.76308183 0.0148487 ]\n","Logit 2907: [0.08558502 0.02477324 0.88964174]\n","Logit 2908: [0.30164608 0.68343087 0.01492305]\n","Logit 2909: [0.18824426 0.78944813 0.02230761]\n","Logit 2910: [0.70428978 0.2834494  0.01226082]\n","Logit 2911: [0.9582035  0.03202839 0.00976811]\n","Logit 2912: [0.34645686 0.15325213 0.50029101]\n","Logit 2913: [0.96093993 0.0266283  0.01243177]\n","Logit 2914: [0.92849511 0.0273085  0.0441964 ]\n","Logit 2915: [0.55446782 0.03388888 0.4116433 ]\n","Logit 2916: [0.51591874 0.47112759 0.01295367]\n","Logit 2917: [0.19815927 0.78949645 0.01234427]\n","Logit 2918: [0.94393805 0.04721441 0.00884753]\n","Logit 2919: [0.81658036 0.17356312 0.00985652]\n","Logit 2920: [0.09483254 0.88991531 0.01525214]\n","Logit 2921: [0.91110581 0.07987436 0.00901982]\n","Logit 2922: [0.28299679 0.68855536 0.02844784]\n","Logit 2923: [0.49634404 0.48437299 0.01928297]\n","Logit 2924: [0.04054269 0.94462963 0.01482769]\n","Logit 2925: [0.03403703 0.95333708 0.01262589]\n","Logit 2926: [0.87260287 0.11264478 0.01475235]\n","Logit 2927: [0.23022449 0.01912541 0.7506501 ]\n","Logit 2928: [0.93049859 0.05887795 0.01062346]\n","Logit 2929: [0.12670459 0.85902565 0.01426976]\n","Logit 2930: [0.0694224  0.91860037 0.01197723]\n","Logit 2931: [0.30932788 0.02599966 0.66467246]\n","Logit 2932: [0.70893475 0.27198738 0.01907787]\n","Logit 2933: [0.872267   0.11406789 0.01366511]\n","Logit 2934: [0.94664531 0.04456801 0.00878668]\n","Logit 2935: [0.83201387 0.05251789 0.11546825]\n","Logit 2936: [0.6248502 0.3644719 0.0106779]\n","Logit 2937: [0.88606098 0.09842315 0.01551588]\n","Logit 2938: [0.83396512 0.15581998 0.0102149 ]\n","Logit 2939: [0.96478921 0.02293472 0.01227607]\n","Logit 2940: [0.12935574 0.806452   0.06419226]\n","Logit 2941: [0.10810405 0.8753702  0.01652576]\n","Logit 2942: [0.64875393 0.3378864  0.01335968]\n","Logit 2943: [0.93600809 0.04394902 0.02004288]\n","Logit 2944: [0.71952558 0.26886707 0.01160736]\n","Logit 2945: [0.12670459 0.85902565 0.01426976]\n","Logit 2946: [0.0626876  0.92513164 0.01218076]\n","Logit 2947: [0.08249372 0.90501848 0.0124878 ]\n","Logit 2948: [0.75788271 0.23012764 0.01198966]\n","Logit 2949: [0.39778441 0.59076481 0.01145079]\n","Logit 2950: [0.55640856 0.43164202 0.01194942]\n","Logit 2951: [0.59882225 0.39195094 0.00922681]\n","Logit 2952: [0.89447418 0.09665334 0.00887248]\n","Logit 2953: [0.78368495 0.20513639 0.01117867]\n","Logit 2954: [0.05699364 0.93488635 0.00812001]\n","Logit 2955: [0.04651363 0.92336681 0.03011957]\n","Logit 2956: [0.96411335 0.02144614 0.0144405 ]\n","Logit 2957: [0.90249377 0.08888032 0.00862592]\n","Logit 2958: [0.26808769 0.72024783 0.01166448]\n","Logit 2959: [0.50715407 0.47318144 0.0196645 ]\n","Logit 2960: [0.33243281 0.64801785 0.01954934]\n","Logit 2961: [0.1530671  0.78496729 0.06196561]\n","Logit 2962: [0.1296865  0.85793151 0.01238199]\n","Logit 2963: [0.09401546 0.8933287  0.01265584]\n","Logit 2964: [0.79819986 0.13940229 0.06239785]\n","Logit 2965: [0.5300215  0.45869538 0.01128312]\n","Logit 2966: [0.95726985 0.0332718  0.00945835]\n","Logit 2967: [0.05328465 0.01686529 0.92985006]\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# 1. 기존 데이터프레임(df)와 감성 분석 결과(preds1, logit1)를 준비했다고 가정합니다.\n","# df['news_title']: 기사 제목이 담긴 데이터프레임의 칼럼\n","# preds1: 감성 분석 결과 (라벨, 예: 'Positive', 'Neutral', 'Negative')\n","# logit1: 각 감성에 대한 확률 (2D 리스트 또는 배열 형태)\n","\n","# 예제 데이터 (실제 데이터로 대체하세요)\n","# df = pd.read_csv(\"input.csv\")\n","# preds1 = ['Positive', 'Negative', 'Neutral']  # 감성 라벨\n","# logit1 = [[0.8, 0.1, 0.1], [0.2, 0.3, 0.5], [0.4, 0.4, 0.2]]  # 확률\n","\n","# 2. 새로운 데이터프레임 생성\n","output_df = pd.DataFrame({\n","    'Article Title': df['kor_sentence'],  # 기사 제목\n","    'Date' : df[\"Date\"],\n","    'Sentiment Label': preds1,  # 감성 라벨\n","    'Neutral Probability': [x[0] for x in new_logit1],  # 긍정 확률\n","    'Positive Probability': [x[1] for x in new_logit1],   # 중립 확률\n","    'Negative Probability': [x[2] for x in new_logit1]  # 부정 확률\n","})\n","\n","# 3. 새로운 파일로 저장\n","output_file = \"삼전_sentiment_results.csv\"  # 출력 파일 이름\n","output_df.to_csv(output_file, index=False, encoding='utf-8-sig')  # UTF-8로 저장 (한글 호환)\n","\n","print(f\"결과가 {output_file}에 저장되었습니다!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"glUaNzXbkaq6","executionInfo":{"status":"ok","timestamp":1731905613715,"user_tz":-540,"elapsed":529,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"3481ecc4-2ecc-415e-be9c-ed00b123838d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["결과가 삼전_sentiment_results.csv에 저장되었습니다!\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P2XgRsx9dExI","executionInfo":{"status":"ok","timestamp":1731904522459,"user_tz":-540,"elapsed":5,"user":{"displayName":"‍임주현[학생](공과대학 산업경영공학과)","userId":"14719868373122672648"}},"outputId":"db0c85d9-d639-4e1a-db14-41051f27bdc6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":[],"metadata":{"id":"U6hfPXfGd1qd"},"execution_count":null,"outputs":[]}]}