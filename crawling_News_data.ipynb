{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14078340,
     "status": "ok",
     "timestamp": 1731694418512,
     "user": {
      "displayName": "‍임주현[학생](공과대학 산업경영공학과)",
      "userId": "14719868373122672648"
     },
     "user_tz": -540
    },
    "id": "nQcCIIZU7ENH",
    "outputId": "d91fa81a-f65d-4bf5-9872-b5b2f4e820cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현대차 뉴스 크롤링 중...\n",
      "현대차 뉴스 저장 완료!\n",
      "기아 뉴스 크롤링 중...\n",
      "기아 뉴스 저장 완료!\n",
      "현대모비스 뉴스 크롤링 중...\n",
      "현대모비스 뉴스 저장 완료!\n",
      "한온시스템 뉴스 크롤링 중...\n",
      "한온시스템 뉴스 저장 완료!\n",
      "한국타이어앤테크놀로지 뉴스 크롤링 중...\n",
      "한국타이어앤테크놀로지 뉴스 저장 완료!\n",
      "HL만도 뉴스 크롤링 중...\n",
      "HL만도 뉴스 저장 완료!\n",
      "현대위아 뉴스 크롤링 중...\n",
      "현대위아 뉴스 저장 완료!\n",
      "에스엘 뉴스 크롤링 중...\n",
      "에스엘 뉴스 저장 완료!\n",
      "금호타이어 뉴스 크롤링 중...\n",
      "금호타이어 뉴스 저장 완료!\n",
      "명신산업 뉴스 크롤링 중...\n",
      "명신산업 뉴스 저장 완료!\n",
      "삼성전자 뉴스 크롤링 중...\n",
      "삼성전자 뉴스 저장 완료!\n",
      "SK하이닉스 뉴스 크롤링 중...\n",
      "SK하이닉스 뉴스 저장 완료!\n",
      "삼성SDI 뉴스 크롤링 중...\n",
      "삼성SDI 뉴스 저장 완료!\n",
      "삼성전기 뉴스 크롤링 중...\n",
      "삼성전기 뉴스 저장 완료!\n",
      "삼성에스디에스 뉴스 크롤링 중...\n",
      "삼성에스디에스 뉴스 저장 완료!\n",
      "엘앤에프 뉴스 크롤링 중...\n",
      "엘앤에프 뉴스 저장 완료!\n",
      "LG이노텍 뉴스 크롤링 중...\n",
      "LG이노텍 뉴스 저장 완료!\n",
      "모든 크롤링 작업 완료!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Excel 파일에서 기업명 읽기\n",
    "def get_company_list(file_path):\n",
    "    df = pd.read_excel(file_path, header=None)  # 첫 번째 열에서 회사명 읽기\n",
    "    company_list = df[0].tolist()\n",
    "    return company_list\n",
    "\n",
    "# 뉴스 크롤링 함수\n",
    "def crawl_news_for_company(company_name, start_date, end_date):\n",
    "    news_data = []\n",
    "    current_date = start_date\n",
    "    while current_date <= end_date:\n",
    "        page_num = 1\n",
    "        while True:\n",
    "            url = f\"https://search.hankyung.com/search/news?query={company_name}&sort=DATE%2FDESC%2CRANK%2FDESC&period=DATE&area=title&sdate={current_date.strftime('%Y.%m.%d')}&edate={current_date.strftime('%Y.%m.%d')}&page={page_num}\"\n",
    "            response = requests.get(url)\n",
    "            response.encoding = 'utf-8'  # 인코딩을 utf-8로 설정\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # 뉴스 제목과 날짜 찾기\n",
    "            articles = soup.select(\"ul.article > li\")\n",
    "            if not articles:\n",
    "                break  # 다음 페이지에 데이터가 없으면 종료\n",
    "\n",
    "            for article in articles:\n",
    "                title_tag = article.select_one(\"em.tit\")\n",
    "                date_tag = article.select_one(\"span.date_time\")\n",
    "                if title_tag and date_tag:\n",
    "                    title = title_tag.text.strip()\n",
    "                    date = date_tag.text.strip()\n",
    "                    news_data.append({\"Title\": title, \"Date\": date})\n",
    "\n",
    "            # 다음 페이지로 이동\n",
    "            page_num += 1\n",
    "            time.sleep(3)  # 요청 간격 조절\n",
    "\n",
    "        current_date += datetime.timedelta(days=1)  # 날짜 간격 이동\n",
    "\n",
    "    return news_data\n",
    "\n",
    "# 결과 저장 및 실행 함수\n",
    "def main(file_path, start_date, end_date):\n",
    "    company_list = get_company_list(file_path)\n",
    "    for company in company_list:\n",
    "        print(f\"{company} 뉴스 크롤링 중...\")\n",
    "        news_data = crawl_news_for_company(company, start_date, end_date)\n",
    "        news_df = pd.DataFrame(news_data)\n",
    "        if not news_df.empty:\n",
    "            news_df.to_csv(f\"{company}_daily_news_titles.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "            print(f\"{company} 뉴스 저장 완료!\")\n",
    "        else:\n",
    "            print(f\"{company} 관련 뉴스 없음.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 검색 기간 설정\n",
    "    start_date = datetime.date(2023, 7, 1)\n",
    "    end_date = datetime.date(2024, 6, 30)\n",
    "\n",
    "    # 기업명 목록이 포함된 엑셀 파일 경로\n",
    "    file_path = \"/content/drive/MyDrive/남은거.xlsx\"  # 해당 파일 경로에 맞게 수정\n",
    "\n",
    "    main(file_path, start_date, end_date)\n",
    "    print(\"모든 크롤링 작업 완료!\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMv59ASfCTIEoM7TsjBHiQy",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
